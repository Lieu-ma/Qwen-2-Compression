{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f024c95f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T03:31:19.690749Z",
     "iopub.status.busy": "2025-06-22T03:31:19.690224Z",
     "iopub.status.idle": "2025-06-22T03:32:26.473856Z",
     "shell.execute_reply": "2025-06-22T03:32:26.473171Z"
    },
    "papermill": {
     "duration": 66.791894,
     "end_time": "2025-06-22T03:32:26.475073",
     "exception": false,
     "start_time": "2025-06-22T03:31:19.683179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q torch transformers peft bitsandbytes accelerate pandas scikit-learn numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d0337c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T03:32:26.514192Z",
     "iopub.status.busy": "2025-06-22T03:32:26.513958Z",
     "iopub.status.idle": "2025-06-22T03:32:29.704605Z",
     "shell.execute_reply": "2025-06-22T03:32:29.704006Z"
    },
    "papermill": {
     "duration": 3.211183,
     "end_time": "2025-06-22T03:32:29.705686",
     "exception": false,
     "start_time": "2025-06-22T03:32:26.494503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 'final_train_set.csv' and 'final_validation_set.csv'\n",
      "Final training set size: 9170 rows\n",
      "Final validation set size: 1019 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This path should be correct inside your Kaggle notebook\n",
    "train_csv_path = '/kaggle/input/classification-of-math-problems-by-kasut-academy/train.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    # It's safer to rename columns in case the header is inconsistent\n",
    "    df.columns = ['problem', 'target']\n",
    "\n",
    "    # Split the data (90% for training, 10% for validation)\n",
    "    # stratify=df['target'] ensures both sets have a similar distribution of categories\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['target'])\n",
    "\n",
    "    # Save the new files to your /kaggle/working/ directory\n",
    "    train_df.to_csv('final_train_set.csv', index=False)\n",
    "    val_df.to_csv('final_validation_set.csv', index=False)\n",
    "\n",
    "    print(\"Successfully created 'final_train_set.csv' and 'final_validation_set.csv'\")\n",
    "    print(f\"Final training set size: {len(train_df)} rows\")\n",
    "    print(f\"Final validation set size: {len(val_df)} rows\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find the file at '{train_csv_path}'. Please double-check the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8703f",
   "metadata": {
    "papermill": {
     "duration": 0.018682,
     "end_time": "2025-06-22T03:32:29.743778",
     "exception": false,
     "start_time": "2025-06-22T03:32:29.725096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 25, 50, 75% Qwen 2-0.5b (4-bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59887c96",
   "metadata": {
    "papermill": {
     "duration": 0.018454,
     "end_time": "2025-06-22T03:32:29.780989",
     "exception": false,
     "start_time": "2025-06-22T03:32:29.762535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5a441f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T03:32:29.819217Z",
     "iopub.status.busy": "2025-06-22T03:32:29.818980Z",
     "iopub.status.idle": "2025-06-22T03:32:29.823701Z",
     "shell.execute_reply": "2025-06-22T03:32:29.823162Z"
    },
    "papermill": {
     "duration": 0.024967,
     "end_time": "2025-06-22T03:32:29.824651",
     "exception": false,
     "start_time": "2025-06-22T03:32:29.799684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prune_model_25pct.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prune_model_25pct.py\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\\n\",\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "SPARSITY_AMOUNT = 0.25 \n",
    "PRUNED_MODEL_SAVE_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "def run_pruning():\n",
    "    print(f\"--- STAGE 1: Pruning Full-Precision 0.5B Model ({SPARSITY_AMOUNT*100}%) ---\")\n",
    "    \n",
    "    if os.path.exists(PRUNED_MODEL_SAVE_DIR):\n",
    "        print(f\"Removing existing directory: {PRUNED_MODEL_SAVE_DIR}\")\n",
    "        os.system(f\"rm -rf {PRUNED_MODEL_SAVE_DIR}\")\n",
    "\n",
    "    print(f\"Loading full-precision model: {BASE_MODEL_PATH}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "    print(f\"\\nApplying {SPARSITY_AMOUNT*100}% unstructured pruning...\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=SPARSITY_AMOUNT)\n",
    "            prune.remove(module, 'weight')\n",
    "\n",
    "    print(\"\\nPruning complete.\")\n",
    "    print(f\"\\nSaving {SPARSITY_AMOUNT*100}% pruned model to {PRUNED_MODEL_SAVE_DIR}...\")\n",
    "    model.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    print(\"Pruned model saved successfully.\")\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pruning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fdee53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T03:32:29.863237Z",
     "iopub.status.busy": "2025-06-22T03:32:29.862751Z",
     "iopub.status.idle": "2025-06-22T03:33:08.511838Z",
     "shell.execute_reply": "2025-06-22T03:33:08.511095Z"
    },
    "papermill": {
     "duration": 38.66985,
     "end_time": "2025-06-22T03:33:08.513223",
     "exception": false,
     "start_time": "2025-06-22T03:32:29.843373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: Pruning Full-Precision 0.5B Model (25.0%) ---\r\n",
      "Loading full-precision model: Qwen/Qwen2-0.5B-Instruct\r\n",
      "config.json: 100%|█████████████████████████████| 659/659 [00:00<00:00, 4.80MB/s]\r\n",
      "2025-06-22 03:32:44.922234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750563165.145810     113 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750563165.214078     113 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\r\n",
      "model.safetensors: 100%|██████████████████████| 988M/988M [00:03<00:00, 281MB/s]\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "generation_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 2.04MB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████| 1.29k/1.29k [00:00<00:00, 12.1MB/s]\r\n",
      "vocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 25.3MB/s]\r\n",
      "merges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 33.4MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 15.7MB/s]\r\n",
      "\r\n",
      "Applying 25.0% unstructured pruning...\r\n",
      "\r\n",
      "Pruning complete.\r\n",
      "\r\n",
      "Saving 25.0% pruned model to /kaggle/working/pruned_25pct_0.5B_model...\r\n",
      "Pruned model saved successfully.\r\n"
     ]
    }
   ],
   "source": [
    "!python prune_model_25pct.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86ca28fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T03:33:08.601192Z",
     "iopub.status.busy": "2025-06-22T03:33:08.600919Z",
     "iopub.status.idle": "2025-06-22T03:33:08.608019Z",
     "shell.execute_reply": "2025-06-22T03:33:08.607473Z"
    },
    "papermill": {
     "duration": 0.030157,
     "end_time": "2025-06-22T03:33:08.608871",
     "exception": false,
     "start_time": "2025-06-22T03:33:08.578714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_pruned_quantized_25pct_4bit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_pruned_quantized_25pct_4bit.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig, get_cosine_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "SPARSITY_AMOUNT = 0.25\n",
    "PRUNED_MODEL_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model/'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = f'/kaggle/working/pruned_q4_{int(SPARSITY_AMOUNT*100)}pct_0.5B_backbone/'\n",
    "FINAL_HEAD_PATH = f'/kaggle/working/pruned_q4_{int(SPARSITY_AMOUNT*100)}pct_0.5B_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRUNED_MODEL_DIR)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts, self.targets = prompts, targets\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None: item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "    def __len__(self): return len(self.prompts)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pruned_model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(pruned_model_path)\n",
    "        bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(pruned_model_path, quantization_config=bnb_config, device_map=rank, torch_dtype=torch.bfloat16, use_cache=False)\n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def train_and_validate_process(rank, world_size, train_prompts, train_targets, val_prompts, val_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    model = Net(PRUNED_MODEL_DIR, rank).to(rank)\n",
    "\n",
    "    # --- ADDED: Print model size ---\n",
    "    if rank == 0:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nLoaded pruned, quantized model with {total_params/1_000_000:.2f}M total parameters.\")\n",
    "        print(f\"Trainable LoRA parameters: {trainable_params/1_000_000:.2f}M\\n\")\n",
    "    # --------------------------------\n",
    "\n",
    "    model = DDP(model, device_ids=[rank], find_unused_parameters=False)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train(); train_sampler.set_epoch(epoch)\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            val_dataset = MathDataset(val_prompts, val_targets)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "            all_preds = []\n",
    "            with torch.no_grad():\n",
    "                for v_batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                    encodings = tokenizer(v_batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        logits = model.module(encodings)\n",
    "                        preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "                    all_preds.extend(preds)\n",
    "            f1_micro = f1_score(val_targets, all_preds, average='micro')\n",
    "            print(f'  Epoch {epoch+1} | Validation F1-micro: {f1_micro:.4f}')\n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "    destroy_process_group()\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(PRUNED_MODEL_DIR):\n",
    "        print(f\"Error: Pruned model directory not found. Please run the pruning script first.\")\n",
    "        return\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    df_train = pd.read_csv(TRAIN_DATA_PATH); df_train.columns = ['problem', 'target']\n",
    "    df_val = pd.read_csv(VAL_DATA_PATH); df_val.columns = ['problem', 'target']\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for training on {SPARSITY_AMOUNT*100}% sparse 4-bit model.\")\n",
    "        args = (world_size, train_prompts, train_targets, val_prompts, val_targets)\n",
    "        mp.spawn(train_and_validate_process, args=args, nprocs=world_size)\n",
    "    else: print(\"This script requires GPUs.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd09c180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T03:33:08.651474Z",
     "iopub.status.busy": "2025-06-22T03:33:08.651270Z",
     "iopub.status.idle": "2025-06-22T03:49:00.912631Z",
     "shell.execute_reply": "2025-06-22T03:49:00.911892Z"
    },
    "papermill": {
     "duration": 952.284169,
     "end_time": "2025-06-22T03:49:00.914102",
     "exception": false,
     "start_time": "2025-06-22T03:33:08.629933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:33:14.033563: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750563194.055867     186 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750563194.062690     186 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for training on 25.0% sparse 4-bit model.\r\n",
      "2025-06-22 03:33:22.501907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750563202.523541     254 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750563202.530171     254 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-22 03:33:30.679934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750563210.701821     321 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750563210.708551     321 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 03:33:38.930427: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750563218.952213     390 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750563218.959022     390 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 03:33:47.114489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750563227.136001     482 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750563227.142603     482 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "\r\n",
      "Loaded pruned, quantized model with 323.92M total parameters.\r\n",
      "Trainable LoRA parameters: 8.81M\r\n",
      "\r\n",
      "  Epoch 1/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:35<00:00,  3.68it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.02it/s]\r\n",
      "  Epoch 1 | Validation F1-micro: 0.8410\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:34<00:00,  3.70it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.01it/s]\r\n",
      "  Epoch 2 | Validation F1-micro: 0.8656\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:35<00:00,  3.69it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  4.99it/s]\r\n",
      "  Epoch 3 | Validation F1-micro: 0.8675\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:35<00:00,  3.68it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  4.99it/s]\r\n",
      "  Epoch 4 | Validation F1-micro: 0.8754\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:34<00:00,  3.70it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.00it/s]\r\n",
      "  Epoch 5 | Validation F1-micro: 0.8695\r\n",
      "\r\n",
      "Training complete. Saving final model...\r\n"
     ]
    }
   ],
   "source": [
    "!python train_pruned_quantized_25pct_4bit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa53f62",
   "metadata": {
    "papermill": {
     "duration": 0.164679,
     "end_time": "2025-06-22T03:49:01.296504",
     "exception": false,
     "start_time": "2025-06-22T03:49:01.131825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a0d700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T03:49:01.624216Z",
     "iopub.status.busy": "2025-06-22T03:49:01.623472Z",
     "iopub.status.idle": "2025-06-22T03:49:01.628834Z",
     "shell.execute_reply": "2025-06-22T03:49:01.628322Z"
    },
    "papermill": {
     "duration": 0.170923,
     "end_time": "2025-06-22T03:49:01.629754",
     "exception": false,
     "start_time": "2025-06-22T03:49:01.458831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prune_model_50pct.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prune_model_50pct.py\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\\n\",\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "SPARSITY_AMOUNT = 0.50  # Set to 50%\n",
    "PRUNED_MODEL_SAVE_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "def run_pruning():\n",
    "    print(f\"--- STAGE 1: Pruning Full-Precision 0.5B Model (50%) ---\")\n",
    "    if os.path.exists(PRUNED_MODEL_SAVE_DIR):\n",
    "        print(f\"Removing existing directory: {PRUNED_MODEL_SAVE_DIR}\")\n",
    "        os.system(f\"rm -rf {PRUNED_MODEL_SAVE_DIR}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "    print(f\"\\nApplying {SPARSITY_AMOUNT*100}% unstructured pruning...\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=SPARSITY_AMOUNT)\n",
    "            prune.remove(module, 'weight')\n",
    "    print(\"\\nPruning complete.\")\n",
    "    print(f\"\\nSaving 50% pruned model to {PRUNED_MODEL_SAVE_DIR}...\")\n",
    "    model.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    print(\"Pruned 50% model saved successfully.\")\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pruning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a989130a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T03:49:01.956487Z",
     "iopub.status.busy": "2025-06-22T03:49:01.955960Z",
     "iopub.status.idle": "2025-06-22T03:49:16.025984Z",
     "shell.execute_reply": "2025-06-22T03:49:16.025247Z"
    },
    "papermill": {
     "duration": 14.234317,
     "end_time": "2025-06-22T03:49:16.027257",
     "exception": false,
     "start_time": "2025-06-22T03:49:01.792940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: Pruning Full-Precision 0.5B Model (50%) ---\r\n",
      "2025-06-22 03:49:07.041628: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750564147.063657     661 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750564147.070335     661 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "\r\n",
      "Applying 50.0% unstructured pruning...\r\n",
      "\r\n",
      "Pruning complete.\r\n",
      "\r\n",
      "Saving 50% pruned model to /kaggle/working/pruned_50pct_0.5B_model...\r\n",
      "Pruned 50% model saved successfully.\r\n"
     ]
    }
   ],
   "source": [
    "!python prune_model_50pct.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e34495f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T03:49:16.357732Z",
     "iopub.status.busy": "2025-06-22T03:49:16.357469Z",
     "iopub.status.idle": "2025-06-22T03:49:16.364606Z",
     "shell.execute_reply": "2025-06-22T03:49:16.364061Z"
    },
    "papermill": {
     "duration": 0.172092,
     "end_time": "2025-06-22T03:49:16.365513",
     "exception": false,
     "start_time": "2025-06-22T03:49:16.193421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_pruned_quantized_50pct_4bit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_pruned_quantized_50pct_4bit.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig, get_cosine_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "SPARSITY_AMOUNT = 0.50\n",
    "PRUNED_MODEL_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model/'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = f'/kaggle/working/pruned_q4_{int(SPARSITY_AMOUNT*100)}pct_0.5B_backbone/'\n",
    "FINAL_HEAD_PATH = f'/kaggle/working/pruned_q4_{int(SPARSITY_AMOUNT*100)}pct_0.5B_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRUNED_MODEL_DIR)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts, self.targets = prompts, targets\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None: item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "    def __len__(self): return len(self.prompts)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pruned_model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(pruned_model_path)\n",
    "        bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(pruned_model_path, quantization_config=bnb_config, device_map=rank, torch_dtype=torch.bfloat16, use_cache=False)\n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def train_and_validate_process(rank, world_size, train_prompts, train_targets, val_prompts, val_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    model = Net(PRUNED_MODEL_DIR, rank).to(rank)\n",
    "\n",
    "    # --- ADDED: Print model size ---\n",
    "    if rank == 0:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nLoaded pruned, quantized model with {total_params/1_000_000:.2f}M total parameters.\")\n",
    "        print(f\"Trainable LoRA parameters: {trainable_params/1_000_000:.2f}M\\n\")\n",
    "    # --------------------------------\n",
    "\n",
    "    model = DDP(model, device_ids=[rank], find_unused_parameters=False)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train(); train_sampler.set_epoch(epoch)\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            val_dataset = MathDataset(val_prompts, val_targets)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "            all_preds = []\n",
    "            with torch.no_grad():\n",
    "                for v_batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                    encodings = tokenizer(v_batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        logits = model.module(encodings)\n",
    "                        preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "                    all_preds.extend(preds)\n",
    "            f1_micro = f1_score(val_targets, all_preds, average='micro')\n",
    "            print(f'  Epoch {epoch+1} | Validation F1-micro: {f1_micro:.4f}')\n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "    destroy_process_group()\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(PRUNED_MODEL_DIR):\n",
    "        print(f\"Error: Pruned model directory not found. Please run the pruning script first.\")\n",
    "        return\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    df_train = pd.read_csv(TRAIN_DATA_PATH); df_train.columns = ['problem', 'target']\n",
    "    df_val = pd.read_csv(VAL_DATA_PATH); df_val.columns = ['problem', 'target']\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for training on {SPARSITY_AMOUNT*100}% sparse 4-bit model.\")\n",
    "        args = (world_size, train_prompts, train_targets, val_prompts, val_targets)\n",
    "        mp.spawn(train_and_validate_process, args=args, nprocs=world_size)\n",
    "    else: print(\"This script requires GPUs.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01f3e3be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T03:49:16.739834Z",
     "iopub.status.busy": "2025-06-22T03:49:16.739230Z",
     "iopub.status.idle": "2025-06-22T04:05:08.979787Z",
     "shell.execute_reply": "2025-06-22T04:05:08.979031Z"
    },
    "papermill": {
     "duration": 952.453472,
     "end_time": "2025-06-22T04:05:08.981121",
     "exception": false,
     "start_time": "2025-06-22T03:49:16.527649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 03:49:22.068192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750564162.090496     733 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750564162.097333     733 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for training on 50.0% sparse 4-bit model.\r\n",
      "2025-06-22 03:49:30.239769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750564170.261215     801 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750564170.267818     801 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-22 03:49:38.391756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750564178.413413     868 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750564178.420013     868 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 03:49:46.650283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750564186.672014     937 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750564186.678597     937 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 03:49:54.814675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750564194.836252    1029 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750564194.842826    1029 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "\r\n",
      "Loaded pruned, quantized model with 323.92M total parameters.\r\n",
      "Trainable LoRA parameters: 8.81M\r\n",
      "\r\n",
      "  Epoch 1/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:35<00:00,  3.68it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.02it/s]\r\n",
      "  Epoch 1 | Validation F1-micro: 0.8253\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:35<00:00,  3.69it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.02it/s]\r\n",
      "  Epoch 2 | Validation F1-micro: 0.8410\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:35<00:00,  3.68it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.02it/s]\r\n",
      "  Epoch 3 | Validation F1-micro: 0.8489\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:35<00:00,  3.69it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.02it/s]\r\n",
      "  Epoch 4 | Validation F1-micro: 0.8410\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:35<00:00,  3.69it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.01it/s]\r\n",
      "  Epoch 5 | Validation F1-micro: 0.8449\r\n",
      "\r\n",
      "Training complete. Saving final model...\r\n"
     ]
    }
   ],
   "source": [
    "!python train_pruned_quantized_50pct_4bit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfdbf1b",
   "metadata": {
    "papermill": {
     "duration": 0.305188,
     "end_time": "2025-06-22T04:05:09.649124",
     "exception": false,
     "start_time": "2025-06-22T04:05:09.343936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a00d2fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:05:10.261105Z",
     "iopub.status.busy": "2025-06-22T04:05:10.260396Z",
     "iopub.status.idle": "2025-06-22T04:05:10.265606Z",
     "shell.execute_reply": "2025-06-22T04:05:10.265095Z"
    },
    "papermill": {
     "duration": 0.310142,
     "end_time": "2025-06-22T04:05:10.266672",
     "exception": false,
     "start_time": "2025-06-22T04:05:09.956530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prune_model_75pct.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prune_model_75pct.py\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\\n\",\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "SPARSITY_AMOUNT = 0.75\n",
    "PRUNED_MODEL_SAVE_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "def run_pruning():\n",
    "    print(f\"--- STAGE 1: Pruning Full-Precision 0.5B Model ({SPARSITY_AMOUNT*100}%) ---\")\n",
    "    if os.path.exists(PRUNED_MODEL_SAVE_DIR):\n",
    "        os.system(f\"rm -rf {PRUNED_MODEL_SAVE_DIR}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "    print(f\"\\nApplying {SPARSITY_AMOUNT*100}% unstructured pruning...\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=SPARSITY_AMOUNT)\n",
    "            prune.remove(module, 'weight')\n",
    "    print(\"\\nPruning complete.\")\n",
    "    print(f\"\\nSaving {SPARSITY_AMOUNT*100}% pruned model to {PRUNED_MODEL_SAVE_DIR}...\")\n",
    "    model.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pruning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cea8691c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:05:10.932988Z",
     "iopub.status.busy": "2025-06-22T04:05:10.932341Z",
     "iopub.status.idle": "2025-06-22T04:05:25.111426Z",
     "shell.execute_reply": "2025-06-22T04:05:25.110713Z"
    },
    "papermill": {
     "duration": 14.537559,
     "end_time": "2025-06-22T04:05:25.112713",
     "exception": false,
     "start_time": "2025-06-22T04:05:10.575154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: Pruning Full-Precision 0.5B Model (75.0%) ---\r\n",
      "2025-06-22 04:05:16.043239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750565116.065619    1208 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750565116.072510    1208 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "\r\n",
      "Applying 75.0% unstructured pruning...\r\n",
      "\r\n",
      "Pruning complete.\r\n",
      "\r\n",
      "Saving 75.0% pruned model to /kaggle/working/pruned_75pct_0.5B_model...\r\n"
     ]
    }
   ],
   "source": [
    "!python prune_model_75pct.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a7faebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:05:25.732483Z",
     "iopub.status.busy": "2025-06-22T04:05:25.732212Z",
     "iopub.status.idle": "2025-06-22T04:05:25.739346Z",
     "shell.execute_reply": "2025-06-22T04:05:25.738866Z"
    },
    "papermill": {
     "duration": 0.314289,
     "end_time": "2025-06-22T04:05:25.740431",
     "exception": false,
     "start_time": "2025-06-22T04:05:25.426142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_pruned_quantized_75pct_4bit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_pruned_quantized_75pct_4bit.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig, get_cosine_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "SPARSITY_AMOUNT = 0.75\n",
    "PRUNED_MODEL_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model/'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = f'/kaggle/working/pruned_q4_{int(SPARSITY_AMOUNT*100)}pct_0.5B_backbone/'\n",
    "FINAL_HEAD_PATH = f'/kaggle/working/pruned_q4_{int(SPARSITY_AMOUNT*100)}pct_0.5B_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRUNED_MODEL_DIR)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts, self.targets = prompts, targets\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None: item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "    def __len__(self): return len(self.prompts)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pruned_model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(pruned_model_path)\n",
    "        bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(pruned_model_path, quantization_config=bnb_config, device_map=rank, torch_dtype=torch.bfloat16, use_cache=False)\n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def train_and_validate_process(rank, world_size, train_prompts, train_targets, val_prompts, val_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    model = Net(PRUNED_MODEL_DIR, rank).to(rank)\n",
    "\n",
    "    # --- ADDED: Print model size ---\n",
    "    if rank == 0:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nLoaded pruned, quantized model with {total_params/1_000_000:.2f}M total parameters.\")\n",
    "        print(f\"Trainable LoRA parameters: {trainable_params/1_000_000:.2f}M\\n\")\n",
    "    # --------------------------------\n",
    "\n",
    "    model = DDP(model, device_ids=[rank], find_unused_parameters=False)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train(); train_sampler.set_epoch(epoch)\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            val_dataset = MathDataset(val_prompts, val_targets)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "            all_preds = []\n",
    "            with torch.no_grad():\n",
    "                for v_batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                    encodings = tokenizer(v_batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        logits = model.module(encodings)\n",
    "                        preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "                    all_preds.extend(preds)\n",
    "            f1_micro = f1_score(val_targets, all_preds, average='micro')\n",
    "            print(f'  Epoch {epoch+1} | Validation F1-micro: {f1_micro:.4f}')\n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "    destroy_process_group()\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(PRUNED_MODEL_DIR):\n",
    "        print(f\"Error: Pruned model directory not found. Please run the pruning script first.\")\n",
    "        return\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    df_train = pd.read_csv(TRAIN_DATA_PATH); df_train.columns = ['problem', 'target']\n",
    "    df_val = pd.read_csv(VAL_DATA_PATH); df_val.columns = ['problem', 'target']\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for training on {SPARSITY_AMOUNT*100}% sparse 4-bit model.\")\n",
    "        args = (world_size, train_prompts, train_targets, val_prompts, val_targets)\n",
    "        mp.spawn(train_and_validate_process, args=args, nprocs=world_size)\n",
    "    else: print(\"This script requires GPUs.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f1eff82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:05:26.407671Z",
     "iopub.status.busy": "2025-06-22T04:05:26.407023Z",
     "iopub.status.idle": "2025-06-22T04:21:19.474817Z",
     "shell.execute_reply": "2025-06-22T04:21:19.474064Z"
    },
    "papermill": {
     "duration": 953.377762,
     "end_time": "2025-06-22T04:21:19.476196",
     "exception": false,
     "start_time": "2025-06-22T04:05:26.098434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 04:05:31.708691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750565131.731014    1280 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750565131.737826    1280 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for training on 75.0% sparse 4-bit model.\r\n",
      "2025-06-22 04:05:39.852467: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750565139.873978    1348 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750565139.880635    1348 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-22 04:05:47.973314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750565147.995046    1415 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750565148.001732    1415 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 04:05:56.204076: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750565156.225574    1484 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750565156.232188    1484 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 04:06:04.339776: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750565164.361367    1576 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750565164.368002    1576 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "\r\n",
      "Loaded pruned, quantized model with 323.92M total parameters.\r\n",
      "Trainable LoRA parameters: 8.81M\r\n",
      "\r\n",
      "  Epoch 1/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:36<00:00,  3.67it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.06it/s]\r\n",
      "  Epoch 1 | Validation F1-micro: 0.7046\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:36<00:00,  3.67it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.08it/s]\r\n",
      "  Epoch 2 | Validation F1-micro: 0.7498\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:35<00:00,  3.68it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.07it/s]\r\n",
      "  Epoch 3 | Validation F1-micro: 0.7920\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:35<00:00,  3.68it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.07it/s]\r\n",
      "  Epoch 4 | Validation F1-micro: 0.7880\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:35<00:00,  3.68it/s]\r\n",
      "Validating:   2%|▍                              | 2/128 [00:00<00:24,  5.10it/s][E622 04:20:50.473040142 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=11466, OpType=ALLREDUCE, NumelIn=1937408, NumelOut=1937408, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\r\n",
      "ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. \r\n",
      "Last error:\r\n",
      "\r\n",
      "Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):\r\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x78fca496c446 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\r\n",
      "frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x78fc5a818f80 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x78fc5a8191cc in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x78fc5a8193e0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x78fc5a820b5a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x78fc5a82261d in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #6: <unknown function> + 0x145c0 (0x78fca4e855c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\r\n",
      "frame #7: <unknown function> + 0x94ac3 (0x78fcca42aac3 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "frame #8: clone + 0x44 (0x78fcca4bba04 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "\r\n",
      "[E622 04:20:50.473039971 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=11466, OpType=ALLREDUCE, NumelIn=1937408, NumelOut=1937408, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\r\n",
      "ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. \r\n",
      "Last error:\r\n",
      "\r\n",
      "Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):\r\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7856cfab9446 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\r\n",
      "frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x785685a18f80 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x785685a191cc in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x785685a194a0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x785685a21378 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x785685a2261d in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #6: <unknown function> + 0x145c0 (0x7856cff3e5c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\r\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7856f54efac3 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "frame #8: clone + 0x44 (0x7856f5580a04 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:25<00:00,  5.04it/s]\r\n",
      "  Epoch 5 | Validation F1-micro: 0.7900\r\n",
      "\r\n",
      "Training complete. Saving final model...\r\n"
     ]
    }
   ],
   "source": [
    "!python train_pruned_quantized_75pct_4bit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2bdd8",
   "metadata": {
    "papermill": {
     "duration": 0.451067,
     "end_time": "2025-06-22T04:21:20.390354",
     "exception": false,
     "start_time": "2025-06-22T04:21:19.939287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 25, 50, 75% Qwen 2-0.5b (8-bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a25bb",
   "metadata": {
    "papermill": {
     "duration": 0.453629,
     "end_time": "2025-06-22T04:21:21.347798",
     "exception": false,
     "start_time": "2025-06-22T04:21:20.894169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62a5609d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:21:22.303593Z",
     "iopub.status.busy": "2025-06-22T04:21:22.302866Z",
     "iopub.status.idle": "2025-06-22T04:21:22.308325Z",
     "shell.execute_reply": "2025-06-22T04:21:22.307799Z"
    },
    "papermill": {
     "duration": 0.455898,
     "end_time": "2025-06-22T04:21:22.309247",
     "exception": false,
     "start_time": "2025-06-22T04:21:21.853349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prune_model_25pct.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prune_model_25pct.py\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "SPARSITY_AMOUNT = 0.25\n",
    "PRUNED_MODEL_SAVE_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "def run_pruning():\n",
    "    print(f\"--- STAGE 1: Pruning Full-Precision 0.5B Model ({SPARSITY_AMOUNT*100}%) ---\")\n",
    "    \n",
    "    if os.path.exists(PRUNED_MODEL_SAVE_DIR):\n",
    "        print(f\"Removing existing directory: {PRUNED_MODEL_SAVE_DIR}\")\n",
    "        os.system(f\"rm -rf {PRUNED_MODEL_SAVE_DIR}\")\n",
    "\n",
    "    print(f\"Loading full-precision model: {BASE_MODEL_PATH}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "    print(f\"\\nApplying {SPARSITY_AMOUNT*100}% unstructured pruning...\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=SPARSITY_AMOUNT)\n",
    "            prune.remove(module, 'weight')\n",
    "\n",
    "    print(\"\\nPruning complete.\")\n",
    "    print(f\"\\nSaving {SPARSITY_AMOUNT*100}% pruned model to {PRUNED_MODEL_SAVE_DIR}...\")\n",
    "    model.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    print(\"Pruned model saved successfully.\")\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pruning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b72704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:21:23.261273Z",
     "iopub.status.busy": "2025-06-22T04:21:23.260603Z",
     "iopub.status.idle": "2025-06-22T04:21:37.199446Z",
     "shell.execute_reply": "2025-06-22T04:21:37.198664Z"
    },
    "papermill": {
     "duration": 14.442263,
     "end_time": "2025-06-22T04:21:37.200791",
     "exception": false,
     "start_time": "2025-06-22T04:21:22.758528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: Pruning Full-Precision 0.5B Model (25.0%) ---\r\n",
      "Removing existing directory: /kaggle/working/pruned_25pct_0.5B_model\r\n",
      "Loading full-precision model: Qwen/Qwen2-0.5B-Instruct\r\n",
      "2025-06-22 04:21:28.563960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750566088.586182    1755 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750566088.592876    1755 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "\r\n",
      "Applying 25.0% unstructured pruning...\r\n",
      "\r\n",
      "Pruning complete.\r\n",
      "\r\n",
      "Saving 25.0% pruned model to /kaggle/working/pruned_25pct_0.5B_model...\r\n",
      "Pruned model saved successfully.\r\n"
     ]
    }
   ],
   "source": [
    "# Execute the script\n",
    "!python prune_model_25pct.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b7c6c88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:21:38.177366Z",
     "iopub.status.busy": "2025-06-22T04:21:38.177069Z",
     "iopub.status.idle": "2025-06-22T04:21:38.184197Z",
     "shell.execute_reply": "2025-06-22T04:21:38.183627Z"
    },
    "papermill": {
     "duration": 0.525488,
     "end_time": "2025-06-22T04:21:38.185155",
     "exception": false,
     "start_time": "2025-06-22T04:21:37.659667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_pruned_quantized_8bit_25pct.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_pruned_quantized_8bit_25pct.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig, get_cosine_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "SPARSITY_AMOUNT = 0.25\n",
    "PRUNED_MODEL_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model/'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = f'/kaggle/working/pruned_q8_{int(SPARSITY_AMOUNT*100)}pct_0.5B_backbone/'\n",
    "FINAL_HEAD_PATH = f'/kaggle/working/pruned_q8_{int(SPARSITY_AMOUNT*100)}pct_0.5B_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRUNED_MODEL_DIR)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts, self.targets = prompts, targets\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None: item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "    def __len__(self): return len(self.prompts)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pruned_model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(pruned_model_path)\n",
    "        bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(pruned_model_path, quantization_config=bnb_config, device_map=rank, torch_dtype=torch.bfloat16, use_cache=False)\n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def train_and_validate_process(rank, world_size, train_prompts, train_targets, val_prompts, val_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    model = Net(PRUNED_MODEL_DIR, rank).to(rank)\n",
    "\n",
    "    if rank == 0:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nLoaded pruned, quantized model with {total_params/1_000_000:.2f}M total parameters.\")\n",
    "        print(f\"Trainable LoRA parameters: {trainable_params/1_000_000:.2f}M\\n\")\n",
    "\n",
    "    model = DDP(model, device_ids=[rank], find_unused_parameters=False)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train(); train_sampler.set_epoch(epoch)\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            val_dataset = MathDataset(val_prompts, val_targets)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "            all_preds = []\n",
    "            with torch.no_grad():\n",
    "                for v_batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                    encodings = tokenizer(v_batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        logits = model.module(encodings)\n",
    "                        preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "                    all_preds.extend(preds)\n",
    "            f1_micro = f1_score(val_targets, all_preds, average='micro')\n",
    "            print(f'  Epoch {epoch+1} | Validation F1-micro: {f1_micro:.4f}')\n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "    destroy_process_group()\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(PRUNED_MODEL_DIR):\n",
    "        print(f\"Error: Pruned model directory not found. Please run the pruning script first.\")\n",
    "        return\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    df_train = pd.read_csv(TRAIN_DATA_PATH); df_train.columns = ['problem', 'target']\n",
    "    df_val = pd.read_csv(VAL_DATA_PATH); df_val.columns = ['problem', 'target']\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for training on {SPARSITY_AMOUNT*100}% sparse 8-bit model.\")\n",
    "        args = (world_size, train_prompts, train_targets, val_prompts, val_targets)\n",
    "        mp.spawn(train_and_validate_process, args=args, nprocs=world_size)\n",
    "    else: print(\"This script requires GPUs.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3be387a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:21:39.116281Z",
     "iopub.status.busy": "2025-06-22T04:21:39.115745Z",
     "iopub.status.idle": "2025-06-22T04:45:30.639553Z",
     "shell.execute_reply": "2025-06-22T04:45:30.638814Z"
    },
    "papermill": {
     "duration": 1431.997449,
     "end_time": "2025-06-22T04:45:30.640858",
     "exception": false,
     "start_time": "2025-06-22T04:21:38.643409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 04:21:44.441766: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750566104.463837    1829 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750566104.470582    1829 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for training on 25.0% sparse 8-bit model.\r\n",
      "2025-06-22 04:21:52.671080: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750566112.692579    1897 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750566112.699200    1897 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-22 04:22:00.809416: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750566120.831290    1964 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750566120.838017    1964 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 04:22:08.996382: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750566129.018400    2033 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750566129.025307    2033 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 04:22:17.141087: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750566137.162866    2125 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750566137.169531    2125 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "\r\n",
      "Loaded pruned, quantized model with 502.84M total parameters.\r\n",
      "Trainable LoRA parameters: 8.81M\r\n",
      "\r\n",
      "  Epoch 1/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:46<00:00,  2.53it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.57it/s]\r\n",
      "  Epoch 1 | Validation F1-micro: 0.7115\r\n",
      "  Epoch 2/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:46<00:00,  2.53it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.57it/s]\r\n",
      "  Epoch 2 | Validation F1-micro: 0.8391\r\n",
      "  Epoch 3/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:46<00:00,  2.53it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.56it/s]\r\n",
      "  Epoch 3 | Validation F1-micro: 0.8597\r\n",
      "  Epoch 4/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:46<00:00,  2.53it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:50<00:00,  2.56it/s]\r\n",
      "  Epoch 4 | Validation F1-micro: 0.8675\r\n",
      "  Epoch 5/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:47<00:00,  2.52it/s]\r\n",
      "Validating:   1%|▏                              | 1/128 [00:00<00:49,  2.55it/s][E622 04:44:37.948055112 ProcessGroupNCCL.cpp:542] [Rank 2] Collective WorkNCCL(SeqNum=11467, OpType=ALLREDUCE, NumelIn=1937408, NumelOut=1937408, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\r\n",
      "ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. \r\n",
      "Last error:\r\n",
      "\r\n",
      "Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):\r\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d89bcb6c446 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\r\n",
      "frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7d8972a18f80 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7d8972a191cc in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7d8972a193e0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7d8972a20b5a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d8972a2261d in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #6: <unknown function> + 0x145c0 (0x7d89bcffb5c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\r\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7d89e25bdac3 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "frame #8: clone + 0x44 (0x7d89e264ea04 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:50<00:00,  2.55it/s]\r\n",
      "  Epoch 5 | Validation F1-micro: 0.8685\r\n",
      "\r\n",
      "Training complete. Saving final model...\r\n"
     ]
    }
   ],
   "source": [
    "# Execute the script\n",
    "!python train_pruned_quantized_8bit_25pct.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39adba",
   "metadata": {
    "papermill": {
     "duration": 0.596264,
     "end_time": "2025-06-22T04:45:31.843744",
     "exception": false,
     "start_time": "2025-06-22T04:45:31.247480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4085f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:45:33.151061Z",
     "iopub.status.busy": "2025-06-22T04:45:33.150361Z",
     "iopub.status.idle": "2025-06-22T04:45:33.155448Z",
     "shell.execute_reply": "2025-06-22T04:45:33.154917Z"
    },
    "papermill": {
     "duration": 0.662085,
     "end_time": "2025-06-22T04:45:33.156360",
     "exception": false,
     "start_time": "2025-06-22T04:45:32.494275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prune_model_50pct.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prune_model_50pct.py\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "SPARSITY_AMOUNT = 0.50\n",
    "PRUNED_MODEL_SAVE_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "def run_pruning():\n",
    "    print(f\"--- STAGE 1: Pruning Full-Precision 0.5B Model ({SPARSITY_AMOUNT*100}%) ---\")\n",
    "    if os.path.exists(PRUNED_MODEL_SAVE_DIR):\n",
    "        print(f\"Removing existing directory: {PRUNED_MODEL_SAVE_DIR}\")\n",
    "        os.system(f\"rm -rf {PRUNED_MODEL_SAVE_DIR}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "    print(f\"\\nApplying {SPARSITY_AMOUNT*100}% unstructured pruning...\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=SPARSITY_AMOUNT)\n",
    "            prune.remove(module, 'weight')\n",
    "    print(\"\\nPruning complete.\")\n",
    "    print(f\"\\nSaving {SPARSITY_AMOUNT*100}% pruned model to {PRUNED_MODEL_SAVE_DIR}...\")\n",
    "    model.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pruning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f7d424b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:45:34.349405Z",
     "iopub.status.busy": "2025-06-22T04:45:34.348771Z",
     "iopub.status.idle": "2025-06-22T04:45:48.156068Z",
     "shell.execute_reply": "2025-06-22T04:45:48.155353Z"
    },
    "papermill": {
     "duration": 14.40424,
     "end_time": "2025-06-22T04:45:48.157460",
     "exception": false,
     "start_time": "2025-06-22T04:45:33.753220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: Pruning Full-Precision 0.5B Model (50.0%) ---\r\n",
      "Removing existing directory: /kaggle/working/pruned_50pct_0.5B_model\r\n",
      "2025-06-22 04:45:39.633853: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750567539.656583    2304 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750567539.663334    2304 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "\r\n",
      "Applying 50.0% unstructured pruning...\r\n",
      "\r\n",
      "Pruning complete.\r\n",
      "\r\n",
      "Saving 50.0% pruned model to /kaggle/working/pruned_50pct_0.5B_model...\r\n"
     ]
    }
   ],
   "source": [
    "!python prune_model_50pct.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87a39ad1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:45:49.481026Z",
     "iopub.status.busy": "2025-06-22T04:45:49.480751Z",
     "iopub.status.idle": "2025-06-22T04:45:49.487800Z",
     "shell.execute_reply": "2025-06-22T04:45:49.487306Z"
    },
    "papermill": {
     "duration": 0.671696,
     "end_time": "2025-06-22T04:45:49.488858",
     "exception": false,
     "start_time": "2025-06-22T04:45:48.817162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_pruned_quantized_8bit_50pct.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_pruned_quantized_8bit_50pct.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig, get_cosine_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "SPARSITY_AMOUNT = 0.50\n",
    "PRUNED_MODEL_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model/'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = f'/kaggle/working/pruned_q8_{int(SPARSITY_AMOUNT*100)}pct_0.5B_backbone/'\n",
    "FINAL_HEAD_PATH = f'/kaggle/working/pruned_q8_{int(SPARSITY_AMOUNT*100)}pct_0.5B_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRUNED_MODEL_DIR)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts, self.targets = prompts, targets\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None: item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "    def __len__(self): return len(self.prompts)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pruned_model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(pruned_model_path)\n",
    "        bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(pruned_model_path, quantization_config=bnb_config, device_map=rank, torch_dtype=torch.bfloat16, use_cache=False)\n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def train_and_validate_process(rank, world_size, train_prompts, train_targets, val_prompts, val_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    model = Net(PRUNED_MODEL_DIR, rank).to(rank)\n",
    "\n",
    "    if rank == 0:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nLoaded pruned, quantized model with {total_params/1_000_000:.2f}M total parameters.\")\n",
    "        print(f\"Trainable LoRA parameters: {trainable_params/1_000_000:.2f}M\\n\")\n",
    "\n",
    "    model = DDP(model, device_ids=[rank], find_unused_parameters=False)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train(); train_sampler.set_epoch(epoch)\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            val_dataset = MathDataset(val_prompts, val_targets)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "            all_preds = []\n",
    "            with torch.no_grad():\n",
    "                for v_batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                    encodings = tokenizer(v_batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        logits = model.module(encodings)\n",
    "                        preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "                    all_preds.extend(preds)\n",
    "            f1_micro = f1_score(val_targets, all_preds, average='micro')\n",
    "            print(f'  Epoch {epoch+1} | Validation F1-micro: {f1_micro:.4f}')\n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "    destroy_process_group()\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(PRUNED_MODEL_DIR):\n",
    "        print(f\"Error: Pruned model directory not found. Please run the pruning script first.\")\n",
    "        return\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    df_train = pd.read_csv(TRAIN_DATA_PATH); df_train.columns = ['problem', 'target']\n",
    "    df_val = pd.read_csv(VAL_DATA_PATH); df_val.columns = ['problem', 'target']\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for training on {SPARSITY_AMOUNT*100}% sparse 8-bit model.\")\n",
    "        args = (world_size, train_prompts, train_targets, val_prompts, val_targets)\n",
    "        mp.spawn(train_and_validate_process, args=args, nprocs=world_size)\n",
    "    else: print(\"This script requires GPUs.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab5a9142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:45:50.685585Z",
     "iopub.status.busy": "2025-06-22T04:45:50.685315Z",
     "iopub.status.idle": "2025-06-22T05:09:36.251226Z",
     "shell.execute_reply": "2025-06-22T05:09:36.250481Z"
    },
    "papermill": {
     "duration": 1426.161849,
     "end_time": "2025-06-22T05:09:36.252541",
     "exception": false,
     "start_time": "2025-06-22T04:45:50.090692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 04:45:56.012061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750567556.034089    2378 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750567556.040825    2378 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for training on 50.0% sparse 8-bit model.\r\n",
      "2025-06-22 04:46:04.148889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750567564.170340    2446 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750567564.176912    2446 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-22 04:46:12.270737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750567572.292275    2513 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750567572.298852    2513 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 04:46:20.472187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750567580.493616    2582 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750567580.500168    2582 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 04:46:28.671400: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750567588.693796    2674 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750567588.700632    2674 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "\r\n",
      "Loaded pruned, quantized model with 502.84M total parameters.\r\n",
      "Trainable LoRA parameters: 8.81M\r\n",
      "\r\n",
      "  Epoch 1/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:46<00:00,  2.53it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.58it/s]\r\n",
      "  Epoch 1 | Validation F1-micro: 0.7831\r\n",
      "  Epoch 2/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:46<00:00,  2.52it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.58it/s]\r\n",
      "  Epoch 2 | Validation F1-micro: 0.8027\r\n",
      "  Epoch 3/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:45<00:00,  2.54it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.58it/s]\r\n",
      "  Epoch 3 | Validation F1-micro: 0.8508\r\n",
      "  Epoch 4/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:45<00:00,  2.54it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.59it/s]\r\n",
      "  Epoch 4 | Validation F1-micro: 0.8430\r\n",
      "  Epoch 5/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:45<00:00,  2.54it/s]\r\n",
      "Validating:   1%|▏                              | 1/128 [00:00<00:49,  2.58it/s][E622 05:08:43.147513033 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=11467, OpType=ALLREDUCE, NumelIn=1937408, NumelOut=1937408, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\r\n",
      "ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. \r\n",
      "Last error:\r\n",
      "\r\n",
      "Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):\r\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ca53aeb9446 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\r\n",
      "frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ca4f0e18f80 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ca4f0e191cc in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7ca4f0e193e0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7ca4f0e20b5a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ca4f0e2261d in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #6: <unknown function> + 0x145c0 (0x7ca53b32d5c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\r\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7ca5608ebac3 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "frame #8: clone + 0x44 (0x7ca56097ca04 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "\r\n",
      "[E622 05:08:43.149969187 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=11467, OpType=ALLREDUCE, NumelIn=1937408, NumelOut=1937408, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\r\n",
      "ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. \r\n",
      "Last error:\r\n",
      "\r\n",
      "Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):\r\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6eac96c446 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\r\n",
      "frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f6e62818f80 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f6e628191cc in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7f6e628193e0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f6e62820b5a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f6e6282261d in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #6: <unknown function> + 0x145c0 (0x7f6eace405c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\r\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7f6ed23e8ac3 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "frame #8: clone + 0x44 (0x7f6ed2479a04 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.58it/s]\r\n",
      "  Epoch 5 | Validation F1-micro: 0.8430\r\n",
      "\r\n",
      "Training complete. Saving final model...\r\n"
     ]
    }
   ],
   "source": [
    "!python train_pruned_quantized_8bit_50pct.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed61e6",
   "metadata": {
    "papermill": {
     "duration": 0.807824,
     "end_time": "2025-06-22T05:09:37.877266",
     "exception": false,
     "start_time": "2025-06-22T05:09:37.069442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d28d301",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T05:09:39.445794Z",
     "iopub.status.busy": "2025-06-22T05:09:39.445063Z",
     "iopub.status.idle": "2025-06-22T05:09:39.450340Z",
     "shell.execute_reply": "2025-06-22T05:09:39.449808Z"
    },
    "papermill": {
     "duration": 0.822389,
     "end_time": "2025-06-22T05:09:39.451262",
     "exception": false,
     "start_time": "2025-06-22T05:09:38.628873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prune_model_75pct.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prune_model_75pct.py\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "SPARSITY_AMOUNT = 0.75\n",
    "PRUNED_MODEL_SAVE_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "def run_pruning():\n",
    "    print(f\"--- STAGE 1: Pruning Full-Precision 0.5B Model ({SPARSITY_AMOUNT*100}%) ---\")\n",
    "    if os.path.exists(PRUNED_MODEL_SAVE_DIR):\n",
    "        print(f\"Removing existing directory: {PRUNED_MODEL_SAVE_DIR}\")\n",
    "        os.system(f\"rm -rf {PRUNED_MODEL_SAVE_DIR}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "    print(f\"\\nApplying {SPARSITY_AMOUNT*100}% unstructured pruning...\")\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=SPARSITY_AMOUNT)\n",
    "            prune.remove(module, 'weight')\n",
    "    print(\"\\nPruning complete.\")\n",
    "    print(f\"\\nSaving {SPARSITY_AMOUNT*100}% pruned model to {PRUNED_MODEL_SAVE_DIR}...\")\n",
    "    model.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(PRUNED_MODEL_SAVE_DIR)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pruning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfdad436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T05:09:41.028613Z",
     "iopub.status.busy": "2025-06-22T05:09:41.028078Z",
     "iopub.status.idle": "2025-06-22T05:09:55.306834Z",
     "shell.execute_reply": "2025-06-22T05:09:55.306091Z"
    },
    "papermill": {
     "duration": 15.107729,
     "end_time": "2025-06-22T05:09:55.308169",
     "exception": false,
     "start_time": "2025-06-22T05:09:40.200440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: Pruning Full-Precision 0.5B Model (75.0%) ---\r\n",
      "Removing existing directory: /kaggle/working/pruned_75pct_0.5B_model\r\n",
      "2025-06-22 05:09:46.290604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750568986.312689    2853 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750568986.319480    2853 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "\r\n",
      "Applying 75.0% unstructured pruning...\r\n",
      "\r\n",
      "Pruning complete.\r\n",
      "\r\n",
      "Saving 75.0% pruned model to /kaggle/working/pruned_75pct_0.5B_model...\r\n"
     ]
    }
   ],
   "source": [
    "!python prune_model_75pct.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d4ef2e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T05:09:56.869233Z",
     "iopub.status.busy": "2025-06-22T05:09:56.868947Z",
     "iopub.status.idle": "2025-06-22T05:09:56.875979Z",
     "shell.execute_reply": "2025-06-22T05:09:56.875469Z"
    },
    "papermill": {
     "duration": 0.754592,
     "end_time": "2025-06-22T05:09:56.876910",
     "exception": false,
     "start_time": "2025-06-22T05:09:56.122318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_pruned_quantized_8bit_75pct.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_pruned_quantized_8bit_75pct.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig, get_cosine_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "SPARSITY_AMOUNT = 0.75\n",
    "PRUNED_MODEL_DIR = f'/kaggle/working/pruned_{int(SPARSITY_AMOUNT*100)}pct_0.5B_model/'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = f'/kaggle/working/pruned_q8_{int(SPARSITY_AMOUNT*100)}pct_0.5B_backbone/'\n",
    "FINAL_HEAD_PATH = f'/kaggle/working/pruned_q8_{int(SPARSITY_AMOUNT*100)}pct_0.5B_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRUNED_MODEL_DIR)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts, self.targets = prompts, targets\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None: item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "    def __len__(self): return len(self.prompts)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pruned_model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(pruned_model_path)\n",
    "        bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(pruned_model_path, quantization_config=bnb_config, device_map=rank, torch_dtype=torch.bfloat16, use_cache=False)\n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def train_and_validate_process(rank, world_size, train_prompts, train_targets, val_prompts, val_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    model = Net(PRUNED_MODEL_DIR, rank).to(rank)\n",
    "    \n",
    "    if rank == 0:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nLoaded pruned, quantized model with {total_params/1_000_000:.2f}M total parameters.\")\n",
    "        print(f\"Trainable LoRA parameters: {trainable_params/1_000_000:.2f}M\\n\")\n",
    "\n",
    "    model = DDP(model, device_ids=[rank], find_unused_parameters=False)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train(); train_sampler.set_epoch(epoch)\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            val_dataset = MathDataset(val_prompts, val_targets)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "            all_preds = []\n",
    "            with torch.no_grad():\n",
    "                for v_batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                    encodings = tokenizer(v_batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        logits = model.module(encodings)\n",
    "                        preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "                    all_preds.extend(preds)\n",
    "            f1_micro = f1_score(val_targets, all_preds, average='micro')\n",
    "            print(f'  Epoch {epoch+1} | Validation F1-micro: {f1_micro:.4f}')\n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "    destroy_process_group()\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(PRUNED_MODEL_DIR):\n",
    "        print(f\"Error: Pruned model directory not found. Please run the pruning script first.\")\n",
    "        return\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    df_train = pd.read_csv(TRAIN_DATA_PATH); df_train.columns = ['problem', 'target']\n",
    "    df_val = pd.read_csv(VAL_DATA_PATH); df_val.columns = ['problem', 'target']\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for training on {SPARSITY_AMOUNT*100}% sparse 8-bit model.\")\n",
    "        args = (world_size, train_prompts, train_targets, val_prompts, val_targets)\n",
    "        mp.spawn(train_and_validate_process, args=args, nprocs=world_size)\n",
    "    else: print(\"This script requires GPUs.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5cc9c13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T05:09:58.500891Z",
     "iopub.status.busy": "2025-06-22T05:09:58.500233Z",
     "iopub.status.idle": "2025-06-22T05:33:38.371003Z",
     "shell.execute_reply": "2025-06-22T05:33:38.370262Z"
    },
    "papermill": {
     "duration": 1420.683224,
     "end_time": "2025-06-22T05:33:38.372397",
     "exception": false,
     "start_time": "2025-06-22T05:09:57.689173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 05:10:03.807922: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750569003.829827    2927 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750569003.836558    2927 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for training on 75.0% sparse 8-bit model.\r\n",
      "2025-06-22 05:10:11.935848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750569011.957670    2995 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750569011.964307    2995 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-22 05:10:20.079053: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750569020.100601    3062 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750569020.107208    3062 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 05:10:28.267338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750569028.288756    3131 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750569028.295322    3131 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-22 05:10:36.429992: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750569036.451602    3223 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750569036.458184    3223 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "\r\n",
      "Loaded pruned, quantized model with 502.84M total parameters.\r\n",
      "Trainable LoRA parameters: 8.81M\r\n",
      "\r\n",
      "  Epoch 1/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:45<00:00,  2.54it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.60it/s]\r\n",
      "  Epoch 1 | Validation F1-micro: 0.6919\r\n",
      "  Epoch 2/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:44<00:00,  2.55it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.60it/s]\r\n",
      "  Epoch 2 | Validation F1-micro: 0.7792\r\n",
      "  Epoch 3/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:46<00:00,  2.53it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.60it/s]\r\n",
      "  Epoch 3 | Validation F1-micro: 0.7920\r\n",
      "  Epoch 4/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:44<00:00,  2.55it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.60it/s]\r\n",
      "  Epoch 4 | Validation F1-micro: 0.7969\r\n",
      "  Epoch 5/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:45<00:00,  2.55it/s]\r\n",
      "Validating:   1%|▏                              | 1/128 [00:00<00:49,  2.59it/s][E622 05:32:45.502501316 ProcessGroupNCCL.cpp:542] [Rank 3] Collective WorkNCCL(SeqNum=11467, OpType=ALLREDUCE, NumelIn=1937408, NumelOut=1937408, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\r\n",
      "ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. \r\n",
      "Last error:\r\n",
      "\r\n",
      "Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):\r\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7bf27fab9446 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\r\n",
      "frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7bf235a18f80 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7bf235a191cc in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7bf235a194a0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7bf235a21378 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7bf235a2261d in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #6: <unknown function> + 0x145c0 (0x7bf27fed35c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\r\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7bf2a5477ac3 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "frame #8: clone + 0x44 (0x7bf2a5508a04 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:49<00:00,  2.59it/s]\r\n",
      "  Epoch 5 | Validation F1-micro: 0.7870\r\n",
      "\r\n",
      "Training complete. Saving final model...\r\n"
     ]
    }
   ],
   "source": [
    "!python train_pruned_quantized_8bit_75pct.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11376393,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 11615683,
     "sourceId": 97669,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 71342,
     "modelInstanceId": 52023,
     "sourceId": 62292,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141432,
     "sourceId": 166218,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301527,
     "sourceId": 363149,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7344.616024,
   "end_time": "2025-06-22T05:33:39.779947",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-22T03:31:15.163923",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
