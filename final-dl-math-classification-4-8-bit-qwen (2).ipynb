{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7c8ab6",
   "metadata": {
    "papermill": {
     "duration": 0.006324,
     "end_time": "2025-06-21T21:16:38.453684",
     "exception": false,
     "start_time": "2025-06-21T21:16:38.447360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install All Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d871e847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T21:16:38.465436Z",
     "iopub.status.busy": "2025-06-21T21:16:38.464910Z",
     "iopub.status.idle": "2025-06-21T21:17:48.172983Z",
     "shell.execute_reply": "2025-06-21T21:17:48.172197Z"
    },
    "papermill": {
     "duration": 69.715366,
     "end_time": "2025-06-21T21:17:48.174408",
     "exception": false,
     "start_time": "2025-06-21T21:16:38.459042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers bitsandbytes accelerate peft pandas torch scikit-learn numpy tqdm -q\n",
    "!pip install -U -q bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c8aa4",
   "metadata": {
    "papermill": {
     "duration": 0.017649,
     "end_time": "2025-06-21T21:17:48.210479",
     "exception": false,
     "start_time": "2025-06-21T21:17:48.192830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Experiment 1: Tune LoRA for Higher Capacity\n",
    "This experiment uses cross-validation to fine-tune the model with an increased LoRA rank and alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2386c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T21:17:48.247781Z",
     "iopub.status.busy": "2025-06-21T21:17:48.247537Z",
     "iopub.status.idle": "2025-06-21T21:17:48.255843Z",
     "shell.execute_reply": "2025-06-21T21:17:48.255359Z"
    },
    "papermill": {
     "duration": 0.02823,
     "end_time": "2025-06-21T21:17:48.256811",
     "exception": false,
     "start_time": "2025-06-21T21:17:48.228581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_s1_lora_capacity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_s1_lora_capacity.py\n",
    "\n",
    "# STRATEGY 1: Fine-tuning with a higher LoRA capacity.\n",
    "# - r = 16, lora_alpha = 32\n",
    "# - LoRA targets all linear layers.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Imports for Multi-GPU Distributed Data Parallel (DDP)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, AutoConfig, get_cosine_schedule_with_warmup, BitsAndBytesConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Constants ---\n",
    "MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "NUM_FOLDS = 3\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "# --- Tokenizer Setup (Global) ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets):\n",
    "        self.prompts = prompts\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompts[idx], self.targets[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "# --- Model Definition ---\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=rank,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        # OPTIMIZATION: Increased LoRA capacity and targeting all linear layers\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            bias='none',\n",
    "            inference_mode=False,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05\n",
    "        )\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1][:, -1, :]\n",
    "        return self.head(last_hidden_state)\n",
    "\n",
    "# --- DDP Setup function ---\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "# --- Main Training Function (for each process) ---\n",
    "def train_process(rank, world_size, fold, train_index, val_index, all_prompts, all_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    if rank == 0:\n",
    "        print(f\"--- Starting Fold {fold+1}/{NUM_FOLDS} on {world_size} GPUs ---\")\n",
    "\n",
    "    train_prompts = [all_prompts[i] for i in train_index]\n",
    "    val_prompts = [all_prompts[i] for i in val_index]\n",
    "    train_targets = [all_targets[i] for i in train_index]\n",
    "    val_targets = [all_targets[i] for i in val_index]\n",
    "\n",
    "    class_weights = 1 / (np.unique(train_targets, return_counts=True)[1] / len(train_targets))\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(rank)\n",
    "\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    val_dataset = MathDataset(val_prompts, val_targets)\n",
    "\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "\n",
    "    model = Net(MODEL_PATH, rank).to(rank)\n",
    "    if rank == 0:\n",
    "        # Print model size\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable model parameters: {total_params/1_000_000:.2f}M\")\n",
    "        \n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer, num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        model.train()\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "            pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else:\n",
    "            pbar = train_loader\n",
    "\n",
    "        for step, (batch_prompts, batch_targets) in enumerate(pbar):\n",
    "            encodings = tokenizer(\n",
    "                batch_prompts, return_tensors='pt', padding='max_length',\n",
    "                truncation=True, max_length=MAX_LEN\n",
    "            ).to(rank)\n",
    "            batch_targets = batch_targets.long().to(rank)\n",
    "\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets, weight=class_weights)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "        \n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch_prompts, batch_targets in tqdm(val_loader, desc=\"Validating\"):\n",
    "                    encodings = tokenizer(\n",
    "                        batch_prompts, return_tensors='pt', padding='max_length',\n",
    "                        truncation=True, max_length=MAX_LEN\n",
    "                    ).to(rank)\n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        logits = model.module(encodings)\n",
    "                        preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "                    all_preds.extend(preds)\n",
    "                    all_labels.extend(batch_targets.tolist())\n",
    "\n",
    "            f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "            print(f'  Fold {fold+1} | Epoch {epoch+1} | Validation F1-micro: {f1:.4f}')\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                print(f\"  New best F1 score: {best_f1:.4f}. Saving model...\")\n",
    "                model.module.backbone.save_pretrained(f'qwen2_s1_backbone_fold_{fold}_best')\n",
    "                torch.save(model.module.head.state_dict(), f'qwen2_s1_head_fold_{fold}_best.pt')\n",
    "\n",
    "    destroy_process_group()\n",
    "\n",
    "# --- Launcher Function ---\n",
    "def main():\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    world_size = torch.cuda.device_count()\n",
    "    print(f\"Using {world_size} GPUs.\")\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    df = pd.read_csv('/kaggle/input/classification-of-math-problems-by-kasut-academy/train.csv')\n",
    "    df.columns = ['problem', 'target']\n",
    "\n",
    "    prompts = [\n",
    "        f\"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem into one of these eight topics using a machine learning or NLP-based approach.\n",
    "0: Algebra\n",
    "1: Geometry and Trigonometry\n",
    "2: Calculus and Analysis\n",
    "3: Probability and Statistics\n",
    "4: Number Theory\n",
    "5: Combinatorics and Discrete Math\n",
    "6: Linear Algebra\n",
    "7: Abstract Algebra and Topology\n",
    "\n",
    "Your answer should be an integer that assigns the most appropriate topic category to the given Math problem based on its content and required reasoning.\n",
    "\n",
    "Math Problem: {p.strip()}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "        for p in df['problem']\n",
    "    ]\n",
    "    targets = df['target'].tolist()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "    splits = list(skf.split(prompts, targets))\n",
    "\n",
    "    for fold in range(NUM_FOLDS):\n",
    "        train_index, val_index = splits[fold]\n",
    "        args = (world_size, fold, train_index, val_index, prompts, targets)\n",
    "        mp.spawn(train_process, args=args, nprocs=world_size)\n",
    "\n",
    "    print(\"\\n--- All folds completed successfully! ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff2fcd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T21:17:48.294441Z",
     "iopub.status.busy": "2025-06-21T21:17:48.293961Z",
     "iopub.status.idle": "2025-06-21T22:14:42.872743Z",
     "shell.execute_reply": "2025-06-21T22:14:42.871976Z"
    },
    "papermill": {
     "duration": 3414.599251,
     "end_time": "2025-06-21T22:14:42.874108",
     "exception": false,
     "start_time": "2025-06-21T21:17:48.274857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 21:18:05.745022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750540685.971664      57 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750540686.041554      57 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "tokenizer_config.json: 100%|███████████████| 1.29k/1.29k [00:00<00:00, 9.96MB/s]\r\n",
      "vocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 38.9MB/s]\r\n",
      "merges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 8.56MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 27.0MB/s]\r\n",
      "PyTorch version: 2.5.1+cu124\r\n",
      "Using 4 GPUs.\r\n",
      "2025-06-21 21:18:26.628939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750540706.650461     126 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750540706.657030     126 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-21 21:18:34.903534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750540714.925070     193 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750540714.931768     193 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 100%|█████████████████████████████| 659/659 [00:00<00:00, 3.11MB/s]\r\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\r\n",
      "model.safetensors: 100%|██████████████████████| 988M/988M [00:03<00:00, 252MB/s]\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 21:18:43.291502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750540723.313450     260 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750540723.320140     260 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "generation_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 2.08MB/s]\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 21:18:51.576193: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750540731.597884     355 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750540731.604544     355 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Starting Fold 1/3 on 4 GPUs ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Trainable model parameters: 8.81M\r\n",
      "  Epoch 1/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:10<00:00,  3.25it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.93it/s]\r\n",
      "  Fold 1 | Epoch 1 | Validation F1-micro: 0.8381\r\n",
      "  New best F1 score: 0.8381. Saving model...\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:09<00:00,  3.26it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.92it/s]\r\n",
      "  Fold 1 | Epoch 2 | Validation F1-micro: 0.8484\r\n",
      "  New best F1 score: 0.8484. Saving model...\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:09<00:00,  3.28it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.91it/s]\r\n",
      "  Fold 1 | Epoch 3 | Validation F1-micro: 0.8481\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:10<00:00,  3.26it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.92it/s]\r\n",
      "  Fold 1 | Epoch 4 | Validation F1-micro: 0.8590\r\n",
      "  New best F1 score: 0.8590. Saving model...\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:09<00:00,  3.28it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.92it/s]\r\n",
      "  Fold 1 | Epoch 5 | Validation F1-micro: 0.8637\r\n",
      "  New best F1 score: 0.8637. Saving model...\r\n",
      "2025-06-21 21:37:07.303266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750541827.326324     534 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750541827.333475     534 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-21 21:37:15.625816: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750541835.647296     601 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750541835.653859     601 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 21:37:23.942226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750541843.963751     670 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750541843.970371     670 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 21:37:32.303116: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750541852.324858     762 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750541852.331474     762 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Starting Fold 2/3 on 4 GPUs ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Trainable model parameters: 8.81M\r\n",
      "  Epoch 1/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:12<00:00,  3.20it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.93it/s]\r\n",
      "  Fold 2 | Epoch 1 | Validation F1-micro: 0.8289\r\n",
      "  New best F1 score: 0.8289. Saving model...\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:12<00:00,  3.21it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.93it/s]\r\n",
      "  Fold 2 | Epoch 2 | Validation F1-micro: 0.8557\r\n",
      "  New best F1 score: 0.8557. Saving model...\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:11<00:00,  3.23it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.93it/s]\r\n",
      "  Fold 2 | Epoch 3 | Validation F1-micro: 0.8592\r\n",
      "  New best F1 score: 0.8592. Saving model...\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:11<00:00,  3.21it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.92it/s]\r\n",
      "  Fold 2 | Epoch 4 | Validation F1-micro: 0.8537\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:11<00:00,  3.23it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.92it/s]\r\n",
      "  Fold 2 | Epoch 5 | Validation F1-micro: 0.8537\r\n",
      "2025-06-21 21:55:57.241068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750542957.263303     941 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750542957.270049     941 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-21 21:56:05.597804: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750542965.619547    1008 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750542965.626210    1008 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 21:56:13.991245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750542974.013093    1077 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750542974.019809    1077 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 21:56:22.328187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750542982.350113    1169 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750542982.356739    1169 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Starting Fold 3/3 on 4 GPUs ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Trainable model parameters: 8.81M\r\n",
      "  Epoch 1/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:12<00:00,  3.21it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.93it/s]\r\n",
      "  Fold 3 | Epoch 1 | Validation F1-micro: 0.7983\r\n",
      "  New best F1 score: 0.7983. Saving model...\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:11<00:00,  3.23it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.93it/s]\r\n",
      "  Fold 3 | Epoch 2 | Validation F1-micro: 0.8298\r\n",
      "  New best F1 score: 0.8298. Saving model...\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:10<00:00,  3.25it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.92it/s]\r\n",
      "  Fold 3 | Epoch 3 | Validation F1-micro: 0.8481\r\n",
      "  New best F1 score: 0.8481. Saving model...\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:11<00:00,  3.22it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.92it/s]\r\n",
      "  Fold 3 | Epoch 4 | Validation F1-micro: 0.8528\r\n",
      "  New best F1 score: 0.8528. Saving model...\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 424/424 [02:11<00:00,  3.22it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 425/425 [01:26<00:00,  4.92it/s]\r\n",
      "  Fold 3 | Epoch 5 | Validation F1-micro: 0.8548\r\n",
      "  New best F1 score: 0.8548. Saving model...\r\n",
      "\r\n",
      "--- All folds completed successfully! ---\r\n"
     ]
    }
   ],
   "source": [
    "!python train_s1_lora_capacity.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f2e77",
   "metadata": {
    "papermill": {
     "duration": 0.617515,
     "end_time": "2025-06-21T22:14:44.050854",
     "exception": false,
     "start_time": "2025-06-21T22:14:43.433339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 2: Full Train Dataset on 4-bit Model\n",
    "This experiment trains on the entire dataset without cross-validation using the best hyperparameters found previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aabf5dde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T22:14:45.216923Z",
     "iopub.status.busy": "2025-06-21T22:14:45.216656Z",
     "iopub.status.idle": "2025-06-21T22:14:45.223929Z",
     "shell.execute_reply": "2025-06-21T22:14:45.223447Z"
    },
    "papermill": {
     "duration": 0.621657,
     "end_time": "2025-06-21T22:14:45.224839",
     "exception": false,
     "start_time": "2025-06-21T22:14:44.603182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_final_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_final_model.py\n",
    "\n",
    "# This script trains the final model on the entire dataset using the best-performing hyperparameters.\n",
    "# - LoRA r=16, lora_alpha=32, targeting all linear layers.\n",
    "# - No cross-validation; uses 100% of the data for training.\n",
    "# - Designed for a multi-GPU (DDP) setup.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Imports for Multi-GPU Distributed Data Parallel (DDP)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, AutoConfig, get_cosine_schedule_with_warmup, BitsAndBytesConfig\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Constants ---\n",
    "MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "# We train for 5 epochs as this appeared to be the sweet spot in our experiments\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "# --- Tokenizer Setup (Global) ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets):\n",
    "        self.prompts = prompts\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompts[idx], self.targets[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "# --- Model Definition (Using the winning LoRA configuration) ---\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=rank,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        # Using the best-performing LoRA configuration\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            bias='none',\n",
    "            inference_mode=False,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05\n",
    "        )\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1][:, -1, :]\n",
    "        return self.head(last_hidden_state)\n",
    "\n",
    "# --- DDP Setup function ---\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "# --- Main Training Function (for each process) ---\n",
    "def train_process(rank, world_size, all_prompts, all_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    if rank == 0:\n",
    "        print(f\"--- Starting final training on {world_size} GPUs ---\")\n",
    "\n",
    "    # Use the entire dataset for training\n",
    "    train_dataset = MathDataset(all_prompts, all_targets)\n",
    "\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    model = Net(MODEL_PATH, rank).to(rank)\n",
    "    if rank == 0:\n",
    "        # Print model size\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable model parameters: {total_params/1_000_000:.2f}M\")\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer, num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        model.train()\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "            pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else:\n",
    "            pbar = train_loader\n",
    "\n",
    "        for step, (batch_prompts, batch_targets) in enumerate(pbar):\n",
    "            encodings = tokenizer(\n",
    "                batch_prompts, return_tensors='pt', padding='max_length',\n",
    "                truncation=True, max_length=MAX_LEN\n",
    "            ).to(rank)\n",
    "            batch_targets = batch_targets.long().to(rank)\n",
    "\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                # We don't need class weights when training on the full balanced dataset at the end\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "        \n",
    "    # Save the final model only on the main process\n",
    "    if rank == 0:\n",
    "        print(\"Training complete. Saving final model...\")\n",
    "        model.module.backbone.save_pretrained('final_model_backbone')\n",
    "        torch.save(model.module.head.state_dict(), 'final_model_head.pt')\n",
    "        print(\"Final model saved to 'final_model_backbone/' and 'final_model_head.pt'\")\n",
    "\n",
    "    destroy_process_group()\n",
    "\n",
    "# --- Launcher Function ---\n",
    "def main():\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    world_size = torch.cuda.device_count()\n",
    "    print(f\"Using {world_size} GPUs for final training run.\")\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    df = pd.read_csv('/kaggle/input/classification-of-math-problems-by-kasut-academy/train.csv')\n",
    "    df.columns = ['problem', 'target']\n",
    "\n",
    "    prompts = [\n",
    "        f\"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem into one of these eight topics using a machine learning or NLP-based approach.\n",
    "0: Algebra\n",
    "1: Geometry and Trigonometry\n",
    "2: Calculus and Analysis\n",
    "3: Probability and Statistics\n",
    "4: Number Theory\n",
    "5: Combinatorics and Discrete Math\n",
    "6: Linear Algebra\n",
    "7: Abstract Algebra and Topology\n",
    "\n",
    "Your answer should be an integer that assigns the most appropriate topic category to the given Math problem based on its content and required reasoning.\n",
    "\n",
    "Math Problem: {p.strip()}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "        for p in df['problem']\n",
    "    ]\n",
    "    targets = df['target'].tolist()\n",
    "\n",
    "    # No cross-validation split, we use all data for training\n",
    "    args = (world_size, prompts, targets)\n",
    "    mp.spawn(train_process, args=args, nprocs=world_size)\n",
    "\n",
    "    print(\"\\n--- Final model training completed successfully! ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a12e064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T22:14:46.389993Z",
     "iopub.status.busy": "2025-06-21T22:14:46.389360Z",
     "iopub.status.idle": "2025-06-21T22:31:59.045535Z",
     "shell.execute_reply": "2025-06-21T22:31:59.044771Z"
    },
    "papermill": {
     "duration": 1033.269819,
     "end_time": "2025-06-21T22:31:59.046897",
     "exception": false,
     "start_time": "2025-06-21T22:14:45.777078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 22:14:51.918398: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750544091.941072    1348 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750544091.948227    1348 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "PyTorch version: 2.5.1+cu124\r\n",
      "Using 4 GPUs for final training run.\r\n",
      "2025-06-21 22:15:00.343781: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750544100.365968    1416 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750544100.372711    1416 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-21 22:15:08.704998: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750544108.726590    1483 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750544108.733219    1483 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 22:15:17.124766: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750544117.146216    1552 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750544117.152819    1552 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 22:15:25.549183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750544125.570470    1644 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750544125.576981    1644 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Starting final training on 4 GPUs ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Trainable model parameters: 8.81M\r\n",
      "  Epoch 1/5\r\n",
      "Training: 100%|███████████████████████████████| 637/637 [03:16<00:00,  3.24it/s]\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 637/637 [03:15<00:00,  3.25it/s]\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 637/637 [03:17<00:00,  3.23it/s]\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 637/637 [03:17<00:00,  3.23it/s]\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 637/637 [03:16<00:00,  3.24it/s]\r\n",
      "Training complete. Saving final model...\r\n",
      "Final model saved to 'final_model_backbone/' and 'final_model_head.pt'\r\n",
      "\r\n",
      "--- Final model training completed successfully! ---\r\n"
     ]
    }
   ],
   "source": [
    "!python train_final_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831dc042",
   "metadata": {
    "papermill": {
     "duration": 0.746305,
     "end_time": "2025-06-21T22:32:00.484078",
     "exception": false,
     "start_time": "2025-06-21T22:31:59.737773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Splitting Data for Final Evaluation (90:10)\n",
    "This step creates the training and validation sets that will be used for the remaining experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c5b35e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T22:32:01.910075Z",
     "iopub.status.busy": "2025-06-21T22:32:01.909323Z",
     "iopub.status.idle": "2025-06-21T22:32:02.850912Z",
     "shell.execute_reply": "2025-06-21T22:32:02.850250Z"
    },
    "papermill": {
     "duration": 1.687553,
     "end_time": "2025-06-21T22:32:02.851964",
     "exception": false,
     "start_time": "2025-06-21T22:32:01.164411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 'final_train_set.csv' and 'final_validation_set.csv'\n",
      "Final training set size: 9170 rows\n",
      "Final validation set size: 1019 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This path should be correct inside your Kaggle notebook\n",
    "train_csv_path = '/kaggle/input/classification-of-math-problems-by-kasut-academy/train.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    # It's safer to rename columns in case the header is inconsistent\n",
    "    df.columns = ['problem', 'target']\n",
    "\n",
    "    # Split the data (90% for training, 10% for validation)\n",
    "    # stratify=df['target'] ensures both sets have a similar distribution of categories\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['target'])\n",
    "\n",
    "    # Save the new files to your /kaggle/working/ directory\n",
    "    train_df.to_csv('final_train_set.csv', index=False)\n",
    "    val_df.to_csv('final_validation_set.csv', index=False)\n",
    "\n",
    "    print(\"Successfully created 'final_train_set.csv' and 'final_validation_set.csv'\")\n",
    "    print(f\"Final training set size: {len(train_df)} rows\")\n",
    "    print(f\"Final validation set size: {len(val_df)} rows\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find the file at '{train_csv_path}'. Please double-check the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18106e19",
   "metadata": {
    "papermill": {
     "duration": 0.745433,
     "end_time": "2025-06-21T22:32:04.289658",
     "exception": false,
     "start_time": "2025-06-21T22:32:03.544225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 3: Train on 90% and Evaluate on 10% (4-bit)\n",
    "This script trains on the 90% split and evaluates on the 10% hold-out set to get a final performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef35938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T22:32:05.722869Z",
     "iopub.status.busy": "2025-06-21T22:32:05.722613Z",
     "iopub.status.idle": "2025-06-21T22:32:05.729890Z",
     "shell.execute_reply": "2025-06-21T22:32:05.729361Z"
    },
    "papermill": {
     "duration": 0.746322,
     "end_time": "2025-06-21T22:32:05.730820",
     "exception": false,
     "start_time": "2025-06-21T22:32:04.984498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_and_evaluate_final.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_and_evaluate_final.py\n",
    "\n",
    "# This script performs the final training and evaluation.\n",
    "# 1. Trains the best model config (r=16, alpha=32) on 90% of the data.\n",
    "# 2. Saves the final model.\n",
    "# 3. Loads the saved model and evaluates it on the 10% hold-out set.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Imports for DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "# Imports from Hugging Face Transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = './final_model_backbone/'\n",
    "FINAL_HEAD_PATH = './final_model_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "# --- Tokenizer Setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts = prompts\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None:\n",
    "            item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "# --- Model Definition ---\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "        \n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config, device_map=rank, torch_dtype=torch.bfloat16, use_cache=False)\n",
    "        \n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "# --- DDP Setup ---\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "# --- Training Process ---\n",
    "def train_process(rank, world_size, train_prompts, train_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    if rank == 0: print(f\"--- Starting final training on {world_size} GPUs ---\")\n",
    "\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    model = Net(BASE_MODEL_PATH, rank).to(rank)\n",
    "    if rank == 0:\n",
    "        # Print model size\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable model parameters: {total_params/1_000_000:.2f}M\")\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        model.train()\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        \n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "        print(f\"Final model saved to '{FINAL_BACKBONE_PATH}' and '{FINAL_HEAD_PATH}'\")\n",
    "\n",
    "    destroy_process_group()\n",
    "\n",
    "# --- Evaluation Process ---\n",
    "def evaluate_model(val_prompts, val_targets):\n",
    "    print(\"\\n--- Starting Evaluation ---\")\n",
    "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, quantization_config=bnb_config, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    \n",
    "    # Print model size\n",
    "    total_params = sum(p.numel() for p in base_model.parameters())\n",
    "    print(f\"Loaded base model with {total_params/1_000_000_000:.2f}B parameters.\")\n",
    "    \n",
    "    model_peft = PeftModel.from_pretrained(base_model, FINAL_BACKBONE_PATH)\n",
    "    \n",
    "    # Print model size after PEFT\n",
    "    total_params_peft = sum(p.numel() for p in model_peft.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable PEFT model parameters: {total_params_peft/1_000_000:.2f}M\")\n",
    "    \n",
    "    head = nn.Linear(AutoConfig.from_pretrained(BASE_MODEL_PATH).hidden_size, 8, bias=False)\n",
    "    head.load_state_dict(torch.load(FINAL_HEAD_PATH)); head.to(model_peft.device); model_peft.eval(); head.eval()\n",
    "\n",
    "    val_dataset = MathDataset(val_prompts, val_targets)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "    all_preds = []\n",
    "    \n",
    "    print(\"Running inference on validation set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(model_peft.device)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                outputs = model_peft(**encodings, output_hidden_states=True)\n",
    "                logits = head(outputs.hidden_states[-1][:, -1, :])\n",
    "                preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    print(\"\\n--- Final Evaluation Results ---\")\n",
    "    target_names = [\"0: Alg\", \"1: Geo\", \"2: Calc\", \"3: Prob\", \"4: Num\", \"5: Combo\", \"6: Lin.Alg\", \"7: Abs.Alg\"]\n",
    "    print(f\"\\nFinal F1 Score (Micro): {f1_score(val_targets, all_preds, average='micro'):.4f}\")\n",
    "    print(f\"Final F1 Score (Macro): {f1_score(val_targets, all_preds, average='macro'):.4f}\")\n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    print(classification_report(val_targets, all_preds, target_names=target_names))\n",
    "\n",
    "# --- Launcher Function ---\n",
    "def main():\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    \n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "        df_val = pd.read_csv(VAL_DATA_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Make sure '{TRAIN_DATA_PATH}' and '{VAL_DATA_PATH}' exist.\")\n",
    "        print(\"You may need to run the data-splitting script first.\")\n",
    "        return\n",
    "\n",
    "    df_train.columns = ['problem', 'target']; df_val.columns = ['problem', 'target']\n",
    "\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem into one of these eight topics...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    \n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for final training.\")\n",
    "        args = (world_size, train_prompts, train_targets); mp.spawn(train_process, args=args, nprocs=world_size)\n",
    "        # Evaluation is done on a single GPU after training is complete\n",
    "        evaluate_model(val_prompts, val_targets)\n",
    "    else:\n",
    "        print(\"No GPUs found. Please run on a GPU-enabled instance.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbfdab06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T22:32:07.157150Z",
     "iopub.status.busy": "2025-06-21T22:32:07.156495Z",
     "iopub.status.idle": "2025-06-21T22:48:08.748508Z",
     "shell.execute_reply": "2025-06-21T22:48:08.747745Z"
    },
    "papermill": {
     "duration": 962.274298,
     "end_time": "2025-06-21T22:48:08.749977",
     "exception": false,
     "start_time": "2025-06-21T22:32:06.475679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 22:32:12.575113: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750545132.597746    1885 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750545132.604620    1885 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for final training.\r\n",
      "2025-06-21 22:32:20.905693: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750545140.927670    1953 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750545140.934352    1953 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-21 22:32:29.334976: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750545149.357642    2020 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750545149.364687    2020 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 22:32:37.714546: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750545157.735966    2089 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750545157.742576    2089 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 22:32:46.056743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750545166.078297    2181 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750545166.084911    2181 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Starting final training on 4 GPUs ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Trainable model parameters: 8.81M\r\n",
      "  Epoch 1/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:57<00:00,  3.23it/s]\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:56<00:00,  3.25it/s]\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:56<00:00,  3.25it/s]\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:56<00:00,  3.25it/s]\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:56<00:00,  3.24it/s]\r\n",
      "\r\n",
      "Training complete. Saving final model...\r\n",
      "Final model saved to './final_model_backbone/' and './final_model_head.pt'\r\n",
      "\r\n",
      "--- Starting Evaluation ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Loaded base model with 0.32B parameters.\r\n",
      "Trainable PEFT model parameters: 0.00M\r\n",
      "/kaggle/working/train_and_evaluate_final.py:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  head.load_state_dict(torch.load(FINAL_HEAD_PATH)); head.to(model_peft.device); model_peft.eval(); head.eval()\r\n",
      "Running inference on validation set...\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:26<00:00,  4.89it/s]\r\n",
      "\r\n",
      "--- Final Evaluation Results ---\r\n",
      "\r\n",
      "Final F1 Score (Micro): 0.8763\r\n",
      "Final F1 Score (Macro): 0.8262\r\n",
      "\r\n",
      "Full Classification Report:\r\n",
      "              precision    recall  f1-score   support\r\n",
      "\r\n",
      "      0: Alg       0.82      0.89      0.85       262\r\n",
      "      1: Geo       0.96      0.96      0.96       244\r\n",
      "     2: Calc       0.86      0.77      0.81       104\r\n",
      "     3: Prob       0.87      0.89      0.88        37\r\n",
      "      4: Num       0.88      0.82      0.85       171\r\n",
      "    5: Combo       0.86      0.87      0.87       183\r\n",
      "  6: Lin.Alg       0.75      0.60      0.67        10\r\n",
      "  7: Abs.Alg       0.83      0.62      0.71         8\r\n",
      "\r\n",
      "    accuracy                           0.88      1019\r\n",
      "   macro avg       0.85      0.80      0.83      1019\r\n",
      "weighted avg       0.88      0.88      0.88      1019\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python train_and_evaluate_final.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15dd9a",
   "metadata": {
    "papermill": {
     "duration": 0.811293,
     "end_time": "2025-06-21T22:48:10.452818",
     "exception": false,
     "start_time": "2025-06-21T22:48:09.641525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 4: Full Train Dataset on 8-bit Model\n",
    "This experiment repeats the process from Experiment 3 but uses 8-bit quantization instead of 4-bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b01cbca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T22:48:12.215662Z",
     "iopub.status.busy": "2025-06-21T22:48:12.215388Z",
     "iopub.status.idle": "2025-06-21T22:48:12.222946Z",
     "shell.execute_reply": "2025-06-21T22:48:12.222465Z"
    },
    "papermill": {
     "duration": 0.89466,
     "end_time": "2025-06-21T22:48:12.223853",
     "exception": false,
     "start_time": "2025-06-21T22:48:11.329193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_and_evaluate_final_8bit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_and_evaluate_final_8bit.py\n",
    "\n",
    "# This script performs the final training and evaluation for the 8-BIT MODEL.\n",
    "# 1. Trains the best model config (r=16, alpha=32) on 90% of the data.\n",
    "# 2. Saves the final model.\n",
    "# 3. Loads the saved model and evaluates it on the 10% hold-out set.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Imports for DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = './final_model_8bit_backbone/'\n",
    "FINAL_HEAD_PATH = './final_model_8bit_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "# --- Tokenizer Setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts = prompts\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None:\n",
    "            item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "# --- Model Definition ---\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        \n",
    "        # --- KEY CHANGE: Using 8-bit quantization ---\n",
    "        bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        \n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config, device_map=rank, torch_dtype=torch.bfloat16, use_cache=False)\n",
    "        \n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "# --- DDP Setup ---\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "# --- Training Process ---\n",
    "def train_process(rank, world_size, train_prompts, train_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    if rank == 0: print(f\"--- Starting final 8-bit training on {world_size} GPUs ---\")\n",
    "\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    model = Net(BASE_MODEL_PATH, rank).to(rank)\n",
    "    if rank == 0:\n",
    "        # Print model size\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable model parameters: {total_params/1_000_000:.2f}M\")\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        model.train()\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        \n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final 8-bit model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "        print(f\"Final 8-bit model saved to '{FINAL_BACKBONE_PATH}' and '{FINAL_HEAD_PATH}'\")\n",
    "\n",
    "    destroy_process_group()\n",
    "\n",
    "# --- Evaluation Process ---\n",
    "def evaluate_model(val_prompts, val_targets):\n",
    "    print(\"\\n--- Starting Evaluation of 8-bit Model ---\")\n",
    "    \n",
    "    # --- KEY CHANGE: Using 8-bit quantization for evaluation model ---\n",
    "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, quantization_config=bnb_config, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "    # Print model size\n",
    "    total_params = sum(p.numel() for p in base_model.parameters())\n",
    "    print(f\"Loaded base model with {total_params/1_000_000_000:.2f}B parameters.\")\n",
    "    \n",
    "    model_peft = PeftModel.from_pretrained(base_model, FINAL_BACKBONE_PATH)\n",
    "\n",
    "    # Print model size after PEFT\n",
    "    total_params_peft = sum(p.numel() for p in model_peft.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable PEFT model parameters: {total_params_peft/1_000_000:.2f}M\")\n",
    "    \n",
    "    head = nn.Linear(AutoConfig.from_pretrained(BASE_MODEL_PATH).hidden_size, 8, bias=False)\n",
    "    head.load_state_dict(torch.load(FINAL_HEAD_PATH)); head.to(model_peft.device); model_peft.eval(); head.eval()\n",
    "\n",
    "    val_dataset = MathDataset(val_prompts, val_targets)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "    all_preds = []\n",
    "    \n",
    "    print(\"Running inference on validation set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(model_peft.device)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                outputs = model_peft(**encodings, output_hidden_states=True)\n",
    "                logits = head(outputs.hidden_states[-1][:, -1, :])\n",
    "                preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    print(\"\\n--- Final 8-bit Evaluation Results ---\")\n",
    "    target_names = [\"0: Alg\", \"1: Geo\", \"2: Calc\", \"3: Prob\", \"4: Num\", \"5: Combo\", \"6: Lin.Alg\", \"7: Abs.Alg\"]\n",
    "    print(f\"\\nFinal F1 Score (Micro): {f1_score(val_targets, all_preds, average='micro'):.4f}\")\n",
    "    print(f\"Final F1 Score (Macro): {f1_score(val_targets, all_preds, average='macro'):.4f}\")\n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    print(classification_report(val_targets, all_preds, target_names=target_names))\n",
    "\n",
    "# --- Launcher Function ---\n",
    "def main():\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    \n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "        df_val = pd.read_csv(VAL_DATA_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Make sure '{TRAIN_DATA_PATH}' and '{VAL_DATA_PATH}' exist.\")\n",
    "        print(\"You may need to run the data-splitting script first.\")\n",
    "        return\n",
    "\n",
    "    df_train.columns = ['problem', 'target']; df_val.columns = ['problem', 'target']\n",
    "\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    \n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for final training.\")\n",
    "        args = (world_size, train_prompts, train_targets); mp.spawn(train_process, args=args, nprocs=world_size)\n",
    "        evaluate_model(val_prompts, val_targets)\n",
    "    else:\n",
    "        print(\"No GPUs found. Please run on a GPU-enabled instance.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc2940b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T22:48:13.908472Z",
     "iopub.status.busy": "2025-06-21T22:48:13.907805Z",
     "iopub.status.idle": "2025-06-21T23:09:21.284550Z",
     "shell.execute_reply": "2025-06-21T23:09:21.283786Z"
    },
    "papermill": {
     "duration": 1268.253365,
     "end_time": "2025-06-21T23:09:21.285980",
     "exception": false,
     "start_time": "2025-06-21T22:48:13.032615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 22:48:19.307976: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750546099.330198    2388 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750546099.336952    2388 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for final training.\r\n",
      "2025-06-21 22:48:27.689737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750546107.711649    2456 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750546107.718296    2456 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-21 22:48:35.951459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750546115.973055    2523 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750546115.979692    2523 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 22:48:44.219821: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750546124.241681    2592 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750546124.248354    2592 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 22:48:52.629460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750546132.651066    2684 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750546132.657707    2684 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Starting final 8-bit training on 4 GPUs ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Trainable model parameters: 8.81M\r\n",
      "  Epoch 1/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:52<00:00,  2.46it/s]\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:52<00:00,  2.47it/s]\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:52<00:00,  2.46it/s]\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:53<00:00,  2.46it/s]\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:53<00:00,  2.46it/s]\r\n",
      "\r\n",
      "Training complete. Saving final 8-bit model...\r\n",
      "Final 8-bit model saved to './final_model_8bit_backbone/' and './final_model_8bit_head.pt'\r\n",
      "\r\n",
      "--- Starting Evaluation of 8-bit Model ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Loaded base model with 0.49B parameters.\r\n",
      "Trainable PEFT model parameters: 0.00M\r\n",
      "/kaggle/working/train_and_evaluate_final_8bit.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  head.load_state_dict(torch.load(FINAL_HEAD_PATH)); head.to(model_peft.device); model_peft.eval(); head.eval()\r\n",
      "Running inference on validation set...\r\n",
      "Validating:   0%|                                       | 0/128 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:51<00:00,  2.50it/s]\r\n",
      "\r\n",
      "--- Final 8-bit Evaluation Results ---\r\n",
      "\r\n",
      "Final F1 Score (Micro): 0.8822\r\n",
      "Final F1 Score (Macro): 0.8357\r\n",
      "\r\n",
      "Full Classification Report:\r\n",
      "              precision    recall  f1-score   support\r\n",
      "\r\n",
      "      0: Alg       0.85      0.90      0.87       262\r\n",
      "      1: Geo       0.96      0.95      0.96       244\r\n",
      "     2: Calc       0.82      0.81      0.82       104\r\n",
      "     3: Prob       0.89      0.89      0.89        37\r\n",
      "      4: Num       0.88      0.83      0.85       171\r\n",
      "    5: Combo       0.87      0.89      0.88       183\r\n",
      "  6: Lin.Alg       0.86      0.60      0.71        10\r\n",
      "  7: Abs.Alg       0.83      0.62      0.71         8\r\n",
      "\r\n",
      "    accuracy                           0.88      1019\r\n",
      "   macro avg       0.87      0.81      0.84      1019\r\n",
      "weighted avg       0.88      0.88      0.88      1019\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python train_and_evaluate_final_8bit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ee2d2",
   "metadata": {
    "papermill": {
     "duration": 0.947759,
     "end_time": "2025-06-21T23:09:23.263822",
     "exception": false,
     "start_time": "2025-06-21T23:09:22.316063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 5: Max Performance-Unquantized\n",
    "This experiment aims for maximum performance by not using any quantization, increasing LoRA capacity, and using differential learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b08f16ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T23:09:25.296521Z",
     "iopub.status.busy": "2025-06-21T23:09:25.295970Z",
     "iopub.status.idle": "2025-06-21T23:09:25.303797Z",
     "shell.execute_reply": "2025-06-21T23:09:25.303293Z"
    },
    "papermill": {
     "duration": 1.029037,
     "end_time": "2025-06-21T23:09:25.304721",
     "exception": false,
     "start_time": "2025-06-21T23:09:24.275684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_max_performance_0.5b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_max_performance_0.5b.py\n",
    "\n",
    "# This script combines multiple strategies to push the 0.5B model to its performance limit.\n",
    "# 1. Full Precision: The model is trained in unquantized bfloat16.\n",
    "# 2. Max LoRA Capacity: r=32, lora_alpha=64, targeting all linear layers.\n",
    "# 3. More Epochs: Trains for 5 epochs.\n",
    "# 4. Differential LR: Uses a slower learning rate for the backbone.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# DDP Imports\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, AutoConfig,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = './max_perf_unquantized_backbone/'\n",
    "FINAL_HEAD_PATH = './max_perf_unquantized_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5 # STRATEGY: Increased to 5\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "# --- Tokenizer Setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts = prompts\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None:\n",
    "            item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "# --- Model Definition ---\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        \n",
    "        # STRATEGY: No quantization, load in full bfloat16 precision\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=rank,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            use_cache=False\n",
    "        )\n",
    "        \n",
    "        # STRATEGY: Max LoRA capacity\n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=32, lora_alpha=64, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "# --- DDP Setup ---\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "# --- Training Process ---\n",
    "def train_process(rank, world_size, train_prompts, train_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    if rank == 0: print(f\"--- Starting Max Performance training on {world_size} GPUs ---\")\n",
    "\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    model = Net(BASE_MODEL_PATH, rank).to(rank)\n",
    "    if rank == 0:\n",
    "        # Print model size\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable model parameters: {total_params/1_000_000:.2f}M\")\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # STRATEGY: Differential learning rates\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.module.named_parameters() if \"backbone\" in n], \"lr\": 5e-5},\n",
    "        {\"params\": [p for n, p in model.module.named_parameters() if \"head\" in n], \"lr\": 3e-4},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        model.train()\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        \n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "        print(f\"Final model saved to '{FINAL_BACKBONE_PATH}' and '{FINAL_HEAD_PATH}'\")\n",
    "\n",
    "    destroy_process_group()\n",
    "\n",
    "# --- Evaluation Process ---\n",
    "def evaluate_model(val_prompts, val_targets):\n",
    "    print(\"\\n--- Starting Evaluation of Unquantized Model ---\")\n",
    "    \n",
    "    # Load the unquantized base model and apply trained adapters\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "    # Print model size\n",
    "    total_params = sum(p.numel() for p in base_model.parameters())\n",
    "    print(f\"Loaded base model with {total_params/1_000_000_000:.2f}B parameters.\")\n",
    "    \n",
    "    model_peft = PeftModel.from_pretrained(base_model, FINAL_BACKBONE_PATH)\n",
    "\n",
    "    # Print model size after PEFT\n",
    "    total_params_peft = sum(p.numel() for p in model_peft.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable PEFT model parameters: {total_params_peft/1_000_000:.2f}M\")\n",
    "    \n",
    "    head = nn.Linear(AutoConfig.from_pretrained(BASE_MODEL_PATH).hidden_size, 8, bias=False)\n",
    "    head.load_state_dict(torch.load(FINAL_HEAD_PATH)); head.to(model_peft.device); model_peft.eval(); head.eval()\n",
    "\n",
    "    val_dataset = MathDataset(val_prompts, val_targets)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "    all_preds = []\n",
    "    \n",
    "    print(\"Running inference on validation set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(model_peft.device)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                outputs = model_peft(**encodings, output_hidden_states=True)\n",
    "                logits = head(outputs.hidden_states[-1][:, -1, :])\n",
    "                preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    print(\"\\n--- Final Evaluation Results ---\")\n",
    "    target_names = [\"0: Alg\", \"1: Geo\", \"2: Calc\", \"3: Prob\", \"4: Num\", \"5: Combo\", \"6: Lin.Alg\", \"7: Abs.Alg\"]\n",
    "    print(f\"\\nFinal F1 Score (Micro): {f1_score(val_targets, all_preds, average='micro'):.4f}\")\n",
    "    print(f\"Final F1 Score (Macro): {f1_score(val_targets, all_preds, average='macro'):.4f}\")\n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    print(classification_report(val_targets, all_preds, target_names=target_names))\n",
    "\n",
    "# --- Launcher Function ---\n",
    "def main():\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    \n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "        df_val = pd.read_csv(VAL_DATA_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Make sure '{TRAIN_DATA_PATH}' and '{VAL_DATA_PATH}' exist.\")\n",
    "        return\n",
    "\n",
    "    df_train.columns = ['problem', 'target']; df_val.columns = ['problem', 'target']\n",
    "\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    \n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for final training.\")\n",
    "        args = (world_size, train_prompts, train_targets); mp.spawn(train_process, args=args, nprocs=world_size)\n",
    "        evaluate_model(val_prompts, val_targets)\n",
    "    else:\n",
    "        print(\"No GPUs found. Please run on a GPU-enabled instance.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca870fbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T23:09:27.274533Z",
     "iopub.status.busy": "2025-06-21T23:09:27.273904Z",
     "iopub.status.idle": "2025-06-21T23:22:50.195231Z",
     "shell.execute_reply": "2025-06-21T23:22:50.194518Z"
    },
    "papermill": {
     "duration": 803.943676,
     "end_time": "2025-06-21T23:22:50.196700",
     "exception": false,
     "start_time": "2025-06-21T23:09:26.253024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 23:09:32.684389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750547372.706225    2891 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750547372.712873    2891 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for final training.\r\n",
      "2025-06-21 23:09:41.024457: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750547381.046964    2959 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750547381.053829    2959 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-21 23:09:49.357186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750547389.378778    3026 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750547389.385425    3026 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 23:09:57.657419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750547397.679716    3095 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750547397.686507    3095 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 23:10:05.989157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750547406.011033    3187 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750547406.017713    3187 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Starting Max Performance training on 4 GPUs ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Trainable model parameters: 17.60M\r\n",
      "  Epoch 1/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:25<00:00,  3.94it/s]\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:24<00:00,  3.97it/s]\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:24<00:00,  3.97it/s]\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:24<00:00,  3.97it/s]\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:24<00:00,  3.97it/s]\r\n",
      "\r\n",
      "Training complete. Saving final model...\r\n",
      "Final model saved to './max_perf_unquantized_backbone/' and './max_perf_unquantized_head.pt'\r\n",
      "\r\n",
      "--- Starting Evaluation of Unquantized Model ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Loaded base model with 0.49B parameters.\r\n",
      "Trainable PEFT model parameters: 0.00M\r\n",
      "/kaggle/working/train_max_performance_0.5b.py:164: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  head.load_state_dict(torch.load(FINAL_HEAD_PATH)); head.to(model_peft.device); model_peft.eval(); head.eval()\r\n",
      "Running inference on validation set...\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:29<00:00,  4.29it/s]\r\n",
      "\r\n",
      "--- Final Evaluation Results ---\r\n",
      "\r\n",
      "Final F1 Score (Micro): 0.8695\r\n",
      "Final F1 Score (Macro): 0.8295\r\n",
      "\r\n",
      "Full Classification Report:\r\n",
      "              precision    recall  f1-score   support\r\n",
      "\r\n",
      "      0: Alg       0.83      0.88      0.86       262\r\n",
      "      1: Geo       0.95      0.95      0.95       244\r\n",
      "     2: Calc       0.79      0.76      0.77       104\r\n",
      "     3: Prob       0.87      0.92      0.89        37\r\n",
      "      4: Num       0.87      0.82      0.84       171\r\n",
      "    5: Combo       0.88      0.86      0.87       183\r\n",
      "  6: Lin.Alg       0.70      0.70      0.70        10\r\n",
      "  7: Abs.Alg       0.75      0.75      0.75         8\r\n",
      "\r\n",
      "    accuracy                           0.87      1019\r\n",
      "   macro avg       0.83      0.83      0.83      1019\r\n",
      "weighted avg       0.87      0.87      0.87      1019\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python train_max_performance_0.5b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05288e",
   "metadata": {
    "papermill": {
     "duration": 1.142867,
     "end_time": "2025-06-21T23:22:52.492387",
     "exception": false,
     "start_time": "2025-06-21T23:22:51.349520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 6: \"Low and Slow\" Maximum Performance (4-bit)\n",
    "This approach uses a slower learning rate and higher LoRA capacity with 4-bit quantization to potentially achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7168938b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T23:22:54.802001Z",
     "iopub.status.busy": "2025-06-21T23:22:54.801722Z",
     "iopub.status.idle": "2025-06-21T23:22:54.809267Z",
     "shell.execute_reply": "2025-06-21T23:22:54.808737Z"
    },
    "papermill": {
     "duration": 1.165017,
     "end_time": "2025-06-21T23:22:54.810307",
     "exception": false,
     "start_time": "2025-06-21T23:22:53.645290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_low_and_slow_4bit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_low_and_slow_4bit.py\n",
    "\n",
    "# STRATEGY: Maximize 4-bit model performance with a \"low and slow\" approach.\n",
    "# 1. Quantization: 4-bit\n",
    "# 2. Max LoRA Capacity: r=32, lora_alpha=64.\n",
    "# 3. More Epochs: Trains for 5 epochs.\n",
    "# 4. Lower Learning Rate: Uses a smaller learning rate of 5e-5.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# DDP Imports\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = './low_slow_4bit_backbone/'\n",
    "FINAL_HEAD_PATH = './low_slow_4bit_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "# --- Tokenizer Setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts = prompts\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None:\n",
    "            item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "# --- Model Definition ---\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        \n",
    "        bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "        \n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config, device_map=rank, torch_dtype=torch.bfloat16, use_cache=False)\n",
    "        \n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=32, lora_alpha=64, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "# --- DDP Setup ---\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "# --- Training Process ---\n",
    "def train_process(rank, world_size, train_prompts, train_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    if rank == 0: print(f\"--- Starting 'Low and Slow' 4-bit training on {world_size} GPUs ---\")\n",
    "\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    model = Net(BASE_MODEL_PATH, rank).to(rank)\n",
    "    if rank == 0:\n",
    "        # Print model size\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable model parameters: {total_params/1_000_000:.2f}M\")\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        model.train()\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        \n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final 4-bit model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "        print(f\"Final 4-bit model saved to '{FINAL_BACKBONE_PATH}' and '{FINAL_HEAD_PATH}'\")\n",
    "\n",
    "    destroy_process_group()\n",
    "\n",
    "# --- Evaluation Process ---\n",
    "def evaluate_model(val_prompts, val_targets):\n",
    "    print(\"\\n--- Starting Evaluation of 4-bit Model ---\")\n",
    "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, quantization_config=bnb_config, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "    # Print model size\n",
    "    total_params = sum(p.numel() for p in base_model.parameters())\n",
    "    print(f\"Loaded base model with {total_params/1_000_000_000:.2f}B parameters.\")\n",
    "    \n",
    "    model_peft = PeftModel.from_pretrained(base_model, FINAL_BACKBONE_PATH)\n",
    "\n",
    "    # Print model size after PEFT\n",
    "    total_params_peft = sum(p.numel() for p in model_peft.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable PEFT model parameters: {total_params_peft/1_000_000:.2f}M\")\n",
    "    \n",
    "    head = nn.Linear(AutoConfig.from_pretrained(BASE_MODEL_PATH).hidden_size, 8, bias=False)\n",
    "    head.load_state_dict(torch.load(FINAL_HEAD_PATH)); head.to(model_peft.device); model_peft.eval(); head.eval()\n",
    "\n",
    "    val_dataset = MathDataset(val_prompts, val_targets)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "    all_preds = []\n",
    "    \n",
    "    print(\"Running inference on validation set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(model_peft.device)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                outputs = model_peft(**encodings, output_hidden_states=True)\n",
    "                logits = head(outputs.hidden_states[-1][:, -1, :])\n",
    "                preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    print(\"\\n--- Final 4-bit Evaluation Results ---\")\n",
    "    target_names = [\"0: Alg\", \"1: Geo\", \"2: Calc\", \"3: Prob\", \"4: Num\", \"5: Combo\", \"6: Lin.Alg\", \"7: Abs.Alg\"]\n",
    "    print(f\"\\nFinal F1 Score (Micro): {f1_score(val_targets, all_preds, average='micro'):.4f}\")\n",
    "    print(f\"Final F1 Score (Macro): {f1_score(val_targets, all_preds, average='macro'):.4f}\")\n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    print(classification_report(val_targets, all_preds, target_names=target_names))\n",
    "\n",
    "# --- Launcher Function ---\n",
    "def main():\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    \n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "        df_val = pd.read_csv(VAL_DATA_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Make sure '{TRAIN_DATA_PATH}' and '{VAL_DATA_PATH}' exist.\")\n",
    "        return\n",
    "\n",
    "    df_train.columns = ['problem', 'target']; df_val.columns = ['problem', 'target']\n",
    "\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    \n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for final training.\")\n",
    "        args = (world_size, train_prompts, train_targets); mp.spawn(train_process, args=args, nprocs=world_size)\n",
    "        evaluate_model(val_prompts, val_targets)\n",
    "    else:\n",
    "        print(\"No GPUs found.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b75c0c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T23:22:57.037479Z",
     "iopub.status.busy": "2025-06-21T23:22:57.036939Z",
     "iopub.status.idle": "2025-06-21T23:39:19.329351Z",
     "shell.execute_reply": "2025-06-21T23:39:19.328620Z"
    },
    "papermill": {
     "duration": 983.366869,
     "end_time": "2025-06-21T23:39:19.330680",
     "exception": false,
     "start_time": "2025-06-21T23:22:55.963811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 23:23:02.473817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750548182.496257    3395 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750548182.503083    3395 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for final training.\r\n",
      "2025-06-21 23:23:10.834992: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750548190.856744    3463 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750548190.863435    3463 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-21 23:23:19.148496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750548199.170614    3530 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750548199.177172    3530 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 23:23:27.576015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750548207.597507    3599 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750548207.604128    3599 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 23:23:35.938933: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750548215.961004    3691 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750548215.967664    3691 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Starting 'Low and Slow' 4-bit training on 4 GPUs ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Trainable model parameters: 17.60M\r\n",
      "  Epoch 1/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:01<00:00,  3.15it/s]\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:01<00:00,  3.16it/s]\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:00<00:00,  3.17it/s]\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:59<00:00,  3.19it/s]\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [02:59<00:00,  3.19it/s]\r\n",
      "\r\n",
      "Training complete. Saving final 4-bit model...\r\n",
      "Final 4-bit model saved to './low_slow_4bit_backbone/' and './low_slow_4bit_head.pt'\r\n",
      "\r\n",
      "--- Starting Evaluation of 4-bit Model ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Loaded base model with 0.32B parameters.\r\n",
      "Trainable PEFT model parameters: 0.00M\r\n",
      "/kaggle/working/train_low_and_slow_4bit.py:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  head.load_state_dict(torch.load(FINAL_HEAD_PATH)); head.to(model_peft.device); model_peft.eval(); head.eval()\r\n",
      "Running inference on validation set...\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:26<00:00,  4.88it/s]\r\n",
      "\r\n",
      "--- Final 4-bit Evaluation Results ---\r\n",
      "\r\n",
      "Final F1 Score (Micro): 0.8724\r\n",
      "Final F1 Score (Macro): 0.8311\r\n",
      "\r\n",
      "Full Classification Report:\r\n",
      "              precision    recall  f1-score   support\r\n",
      "\r\n",
      "      0: Alg       0.83      0.87      0.85       262\r\n",
      "      1: Geo       0.95      0.96      0.96       244\r\n",
      "     2: Calc       0.79      0.82      0.81       104\r\n",
      "     3: Prob       0.86      0.86      0.86        37\r\n",
      "      4: Num       0.87      0.82      0.84       171\r\n",
      "    5: Combo       0.89      0.86      0.88       183\r\n",
      "  6: Lin.Alg       0.86      0.60      0.71        10\r\n",
      "  7: Abs.Alg       0.75      0.75      0.75         8\r\n",
      "\r\n",
      "    accuracy                           0.87      1019\r\n",
      "   macro avg       0.85      0.82      0.83      1019\r\n",
      "weighted avg       0.87      0.87      0.87      1019\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python train_low_and_slow_4bit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96830200",
   "metadata": {
    "papermill": {
     "duration": 1.280031,
     "end_time": "2025-06-21T23:39:21.897952",
     "exception": false,
     "start_time": "2025-06-21T23:39:20.617921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 7: \"Low and Slow\" Maximum Performance (8-bit)\n",
    "This is the 8-bit version of the \"low and slow\" strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99800bfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T23:39:24.477148Z",
     "iopub.status.busy": "2025-06-21T23:39:24.476875Z",
     "iopub.status.idle": "2025-06-21T23:39:24.484373Z",
     "shell.execute_reply": "2025-06-21T23:39:24.483875Z"
    },
    "papermill": {
     "duration": 1.299535,
     "end_time": "2025-06-21T23:39:24.485279",
     "exception": false,
     "start_time": "2025-06-21T23:39:23.185744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_low_and_slow_8bit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_low_and_slow_8bit.py\n",
    "\n",
    "# STRATEGY: Maximize 8-bit model performance with a \"low and slow\" approach.\n",
    "# 1. Quantization: 8-bit\n",
    "# 2. Max LoRA Capacity: r=32, lora_alpha=64.\n",
    "# 3. More Epochs: Trains for 5 epochs.\n",
    "# 4. Lower Learning Rate: Uses a smaller learning rate of 5e-5.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# DDP Imports\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = './low_slow_8bit_backbone/'\n",
    "FINAL_HEAD_PATH = './low_slow_8bit_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "# --- Tokenizer Setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts = prompts\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None:\n",
    "            item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "# --- Model Definition ---\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        \n",
    "        # --- KEY CHANGE: Using 8-bit quantization ---\n",
    "        bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        \n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config, device_map=rank, torch_dtype=torch.bfloat16, use_cache=False)\n",
    "        \n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=32, lora_alpha=64, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "# --- DDP Setup ---\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "# --- Training Process ---\n",
    "def train_process(rank, world_size, train_prompts, train_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    if rank == 0: print(f\"--- Starting 'Low and Slow' 8-bit training on {world_size} GPUs ---\")\n",
    "\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    model = Net(BASE_MODEL_PATH, rank).to(rank)\n",
    "    if rank == 0:\n",
    "        # Print model size\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable model parameters: {total_params/1_000_000:.2f}M\")\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        model.train()\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        \n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final 8-bit model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "        print(f\"Final 8-bit model saved to '{FINAL_BACKBONE_PATH}' and '{FINAL_HEAD_PATH}'\")\n",
    "\n",
    "    destroy_process_group()\n",
    "\n",
    "# --- Evaluation Process ---\n",
    "def evaluate_model(val_prompts, val_targets):\n",
    "    print(\"\\n--- Starting Evaluation of 8-bit Model ---\")\n",
    "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, quantization_config=bnb_config, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "    # Print model size\n",
    "    total_params = sum(p.numel() for p in base_model.parameters())\n",
    "    print(f\"Loaded base model with {total_params/1_000_000_000:.2f}B parameters.\")\n",
    "    \n",
    "    model_peft = PeftModel.from_pretrained(base_model, FINAL_BACKBONE_PATH)\n",
    "\n",
    "    # Print model size after PEFT\n",
    "    total_params_peft = sum(p.numel() for p in model_peft.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable PEFT model parameters: {total_params_peft/1_000_000:.2f}M\")\n",
    "    \n",
    "    head = nn.Linear(AutoConfig.from_pretrained(BASE_MODEL_PATH).hidden_size, 8, bias=False)\n",
    "    head.load_state_dict(torch.load(FINAL_HEAD_PATH)); head.to(model_peft.device); model_peft.eval(); head.eval()\n",
    "\n",
    "    val_dataset = MathDataset(val_prompts, val_targets)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "    all_preds = []\n",
    "    \n",
    "    print(\"Running inference on validation set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(model_peft.device)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                outputs = model_peft(**encodings, output_hidden_states=True)\n",
    "                logits = head(outputs.hidden_states[-1][:, -1, :])\n",
    "                preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    print(\"\\n--- Final 8-bit Evaluation Results ---\")\n",
    "    target_names = [\"0: Alg\", \"1: Geo\", \"2: Calc\", \"3: Prob\", \"4: Num\", \"5: Combo\", \"6: Lin.Alg\", \"7: Abs.Alg\"]\n",
    "    print(f\"\\nFinal F1 Score (Micro): {f1_score(val_targets, all_preds, average='micro'):.4f}\")\n",
    "    print(f\"Final F1 Score (Macro): {f1_score(val_targets, all_preds, average='macro'):.4f}\")\n",
    "    print(\"\\nFull Classification Report:\")\n",
    "    print(classification_report(val_targets, all_preds, target_names=target_names))\n",
    "\n",
    "# --- Launcher Function ---\n",
    "def main():\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    \n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "        df_val = pd.read_csv(VAL_DATA_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Make sure '{TRAIN_DATA_PATH}' and '{VAL_DATA_PATH}' exist.\")\n",
    "        return\n",
    "\n",
    "    df_train.columns = ['problem', 'target']; df_val.columns = ['problem', 'target']\n",
    "\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    \n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for final training.\")\n",
    "        args = (world_size, train_prompts, train_targets); mp.spawn(train_process, args=args, nprocs=world_size)\n",
    "        evaluate_model(val_prompts, val_targets)\n",
    "    else:\n",
    "        print(\"No GPUs found.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b840d3d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T23:39:26.968702Z",
     "iopub.status.busy": "2025-06-21T23:39:26.967968Z",
     "iopub.status.idle": "2025-06-22T00:00:37.951499Z",
     "shell.execute_reply": "2025-06-22T00:00:37.950744Z"
    },
    "papermill": {
     "duration": 1272.266356,
     "end_time": "2025-06-22T00:00:37.952820",
     "exception": false,
     "start_time": "2025-06-21T23:39:25.686464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 23:39:32.390555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750549172.413292    3898 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750549172.420135    3898 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for final training.\r\n",
      "2025-06-21 23:39:40.764022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750549180.786216    3966 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750549180.792999    3966 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-21 23:39:49.110601: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750549189.132013    4033 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750549189.138570    4033 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 23:39:57.523389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750549197.545387    4102 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750549197.552196    4102 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "2025-06-21 23:40:05.893487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750549205.915043    4194 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750549205.921702    4194 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Starting 'Low and Slow' 8-bit training on 4 GPUs ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Trainable model parameters: 17.60M\r\n",
      "  Epoch 1/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:54<00:00,  2.45it/s]\r\n",
      "  Epoch 2/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:53<00:00,  2.46it/s]\r\n",
      "  Epoch 3/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:52<00:00,  2.46it/s]\r\n",
      "  Epoch 4/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:53<00:00,  2.46it/s]\r\n",
      "  Epoch 5/5\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:54<00:00,  2.45it/s]\r\n",
      "\r\n",
      "Training complete. Saving final 8-bit model...\r\n",
      "Final 8-bit model saved to './low_slow_8bit_backbone/' and './low_slow_8bit_head.pt'\r\n",
      "[E621 23:59:40.399979889 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=14331, OpType=ALLREDUCE, NumelIn=4059136, NumelOut=4059136, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\r\n",
      "ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. \r\n",
      "Last error:\r\n",
      "\r\n",
      "Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):\r\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ec7f496c446 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\r\n",
      "frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7ec7aa818f80 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7ec7aa8191cc in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x7ec7aa8193e0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7ec7aa820b5a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ec7aa82261d in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\r\n",
      "frame #6: <unknown function> + 0x145c0 (0x7ec7f4e095c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\r\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7ec81a3cdac3 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "frame #8: clone + 0x44 (0x7ec81a45ea04 in /lib/x86_64-linux-gnu/libc.so.6)\r\n",
      "\r\n",
      "\r\n",
      "--- Starting Evaluation of 8-bit Model ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Loaded base model with 0.49B parameters.\r\n",
      "Trainable PEFT model parameters: 0.00M\r\n",
      "/kaggle/working/train_low_and_slow_8bit.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  head.load_state_dict(torch.load(FINAL_HEAD_PATH)); head.to(model_peft.device); model_peft.eval(); head.eval()\r\n",
      "Running inference on validation set...\r\n",
      "Validating:   0%|                                       | 0/128 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:50<00:00,  2.51it/s]\r\n",
      "\r\n",
      "--- Final 8-bit Evaluation Results ---\r\n",
      "\r\n",
      "Final F1 Score (Micro): 0.8665\r\n",
      "Final F1 Score (Macro): 0.8305\r\n",
      "\r\n",
      "Full Classification Report:\r\n",
      "              precision    recall  f1-score   support\r\n",
      "\r\n",
      "      0: Alg       0.83      0.86      0.85       262\r\n",
      "      1: Geo       0.94      0.95      0.95       244\r\n",
      "     2: Calc       0.80      0.82      0.81       104\r\n",
      "     3: Prob       0.94      0.92      0.93        37\r\n",
      "      4: Num       0.84      0.82      0.83       171\r\n",
      "    5: Combo       0.87      0.84      0.85       183\r\n",
      "  6: Lin.Alg       0.83      0.50      0.62        10\r\n",
      "  7: Abs.Alg       0.86      0.75      0.80         8\r\n",
      "\r\n",
      "    accuracy                           0.87      1019\r\n",
      "   macro avg       0.86      0.81      0.83      1019\r\n",
      "weighted avg       0.87      0.87      0.87      1019\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python train_low_and_slow_8bit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e255a",
   "metadata": {
    "papermill": {
     "duration": 1.415893,
     "end_time": "2025-06-22T00:00:40.794657",
     "exception": false,
     "start_time": "2025-06-22T00:00:39.378764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 8: Dynamic Adaptive Quantization(Novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba2bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T00:00:43.624845Z",
     "iopub.status.busy": "2025-06-22T00:00:43.624557Z",
     "iopub.status.idle": "2025-06-22T00:00:43.632745Z",
     "shell.execute_reply": "2025-06-22T00:00:43.632209Z"
    },
    "papermill": {
     "duration": 1.424299,
     "end_time": "2025-06-22T00:00:43.633723",
     "exception": false,
     "start_time": "2025-06-22T00:00:42.209424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_dynamic_adaptive_full.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_dynamic_adaptive_full.py\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "\n",
    "# DDP, Transformers, PEFT imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, AutoConfig,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType\n",
    "import bitsandbytes as bnb\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = './dynamic_adaptive_backbone/'\n",
    "FINAL_HEAD_PATH = './dynamic_adaptive_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "# --- Tokenizer Setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts, self.targets = prompts, targets\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None: item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "    def __len__(self): return len(self.prompts)\n",
    "\n",
    "# --- Model Definition with Dynamic Quantization ---\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        \n",
    "        if rank == 0: print(\"--- Determining dynamic quantization policy ---\")\n",
    "        with init_empty_weights():\n",
    "            meta_model = AutoModelForCausalLM.from_config(self.config)\n",
    "        \n",
    "        layer_sizes = defaultdict(list)\n",
    "        for name, module in meta_model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                size = module.in_features * module.out_features\n",
    "                layer_sizes[size].append(name)\n",
    "        \n",
    "        unique_sizes = sorted(layer_sizes.keys(), reverse=True)\n",
    "        if rank == 0: print(f\"Found {len(unique_sizes)} unique linear layer sizes.\")\n",
    "\n",
    "        policy_map = {}\n",
    "        for size in unique_sizes[:2]:\n",
    "            for layer_name in layer_sizes[size]: policy_map[layer_name] = 8\n",
    "        for size in unique_sizes[2:]:\n",
    "            for layer_name in layer_sizes[size]: policy_map[layer_name] = 4\n",
    "        \n",
    "        if rank == 0:\n",
    "            eight_bit_count = sum(1 for v in policy_map.values() if v == 8)\n",
    "            four_bit_count = sum(1 for v in policy_map.values() if v == 4)\n",
    "            print(f\"Policy determined: {eight_bit_count} layers to 8-bit, {four_bit_count} layers to 4-bit.\")\n",
    "            print(\"Building custom model...\")\n",
    "\n",
    "        with init_empty_weights():\n",
    "            model_to_quantize = AutoModelForCausalLM.from_config(self.config)\n",
    "            \n",
    "            # Use list() to create a copy, allowing modification during iteration\n",
    "            for name, module in list(model_to_quantize.named_modules()):\n",
    "                if isinstance(module, nn.Linear) and name in policy_map:\n",
    "                    if policy_map[name] == 8:\n",
    "                        new_module = bnb.nn.Linear8bitLt(module.in_features, module.out_features, bias=module.bias is not None, has_fp16_weights=False, threshold=6.0)\n",
    "                    else:\n",
    "                        new_module = bnb.nn.Linear4bit(module.in_features, module.out_features, bias=module.bias is not None, compute_dtype=torch.bfloat16, compress_statistics=True, quant_type='nf4')\n",
    "                    \n",
    "                    # --- FIX: Handle both nested and top-level modules ---\n",
    "                    if '.' in name:\n",
    "                        parent_name, child_name = name.rsplit('.', 1)\n",
    "                        parent_module = model_to_quantize.get_submodule(parent_name)\n",
    "                        setattr(parent_module, child_name, new_module)\n",
    "                    else:\n",
    "                        setattr(model_to_quantize, name, new_module)\n",
    "        \n",
    "        model_to_quantize.is_quantized = True\n",
    "\n",
    "        if rank == 0: print(\"Loading weights into adaptively quantized model...\")\n",
    "        local_model_path = snapshot_download(repo_id=model_path)\n",
    "        \n",
    "        self.backbone = load_checkpoint_and_dispatch(\n",
    "            model_to_quantize, local_model_path, device_map={'':rank},\n",
    "            no_split_module_classes=[\"Qwen2DecoderLayer\"], dtype=torch.bfloat16\n",
    "        )\n",
    "        if rank == 0: print(\"Model loading complete.\")\n",
    "\n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def train_and_validate_process(rank, world_size, train_prompts, train_targets, val_prompts, val_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    \n",
    "    try:\n",
    "        model = Net(BASE_MODEL_PATH, rank).to(rank)\n",
    "        if rank == 0:\n",
    "            # Print model size\n",
    "            total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            print(f\"Trainable model parameters: {total_params/1_000_000:.2f}M\")\n",
    "        model = DDP(model, device_ids=[rank], find_unused_parameters=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[GPU {rank}] Failed during model initialization: {e}\")\n",
    "        destroy_process_group(); return\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train(); train_sampler.set_epoch(epoch)\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward();\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        \n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            val_dataset = MathDataset(val_prompts, val_targets)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "            all_preds = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                    encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        logits = model.module(encodings)\n",
    "                        preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "                    all_preds.extend(preds)\n",
    "            \n",
    "            f1_micro = f1_score(val_targets, all_preds, average='micro')\n",
    "            print(f'  Epoch {epoch+1} | Validation F1-micro: {f1_micro:.4f}')\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "        print(f\"Final model saved to '{FINAL_BACKBONE_PATH}' and '{FINAL_HEAD_PATH}'\")\n",
    "\n",
    "    destroy_process_group()\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    \n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_DATA_PATH); df_train.columns = ['problem', 'target']\n",
    "        df_val = pd.read_csv(VAL_DATA_PATH); df_val.columns = ['problem', 'target']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Make sure '{TRAIN_DATA_PATH}' and '{VAL_DATA_PATH}' exist.\")\n",
    "        return\n",
    "\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    \n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for adaptive training.\")\n",
    "        args = (world_size, train_prompts, train_targets, val_prompts, val_targets)\n",
    "        mp.spawn(train_and_validate_process, args=args, nprocs=world_size)\n",
    "    else:\n",
    "        print(\"This script requires GPUs.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd888697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T00:00:46.402647Z",
     "iopub.status.busy": "2025-06-22T00:00:46.402057Z",
     "iopub.status.idle": "2025-06-22T00:01:35.106625Z",
     "shell.execute_reply": "2025-06-22T00:01:35.105873Z"
    },
    "papermill": {
     "duration": 50.038546,
     "end_time": "2025-06-22T00:01:35.108006",
     "exception": false,
     "start_time": "2025-06-22T00:00:45.069460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 00:00:51.818655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750550451.841051    4401 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750550451.847938    4401 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for adaptive training.\r\n",
      "2025-06-22 00:01:00.382518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750550460.404518    4469 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750550460.411236    4469 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-22 00:01:08.924515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750550468.946170    4536 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750550468.952813    4536 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Fetching 10 files:   0%|                                 | 0/10 [00:00<?, ?it/s]\r\n",
      "LICENSE: 100%|█████████████████████████████| 11.3k/11.3k [00:00<00:00, 45.0MB/s]\r\n",
      "\r\n",
      ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 13.2MB/s]\r\n",
      "Fetching 10 files:  10%|██▌                      | 1/10 [00:00<00:01,  5.12it/s]\r\n",
      "README.md: 100%|███████████████████████████| 3.56k/3.56k [00:00<00:00, 34.9MB/s]\r\n",
      "Fetching 10 files: 100%|████████████████████████| 10/10 [00:00<00:00, 44.71it/s]\r\n",
      "[GPU 1] Failed during model initialization: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\r\n",
      "2025-06-22 00:01:17.511047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750550477.532613    4605 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750550477.539171    4605 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Fetching 10 files: 100%|█████████████████████| 10/10 [00:00<00:00, 55998.72it/s]\r\n",
      "[GPU 2] Failed during model initialization: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\r\n",
      "2025-06-22 00:01:26.190487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750550486.212506    4706 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750550486.219293    4706 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Determining dynamic quantization policy ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Found 4 unique linear layer sizes.\r\n",
      "Policy determined: 73 layers to 8-bit, 96 layers to 4-bit.\r\n",
      "Building custom model...\r\n",
      "Loading weights into adaptively quantized model...\r\n",
      "Fetching 10 files: 100%|████████████████████| 10/10 [00:00<00:00, 114598.47it/s]\r\n",
      "Fetching 10 files: 100%|████████████████████| 10/10 [00:00<00:00, 116508.44it/s]\r\n",
      "[GPU 3] Failed during model initialization: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\r\n",
      "[GPU 0] Failed during model initialization: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\r\n"
     ]
    }
   ],
   "source": [
    "!python train_dynamic_adaptive_full.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db6dda",
   "metadata": {
    "papermill": {
     "duration": 1.415988,
     "end_time": "2025-06-22T00:01:37.938705",
     "exception": false,
     "start_time": "2025-06-22T00:01:36.522717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 9: Dynamic Adaptive Hybrid Model\n",
    "This highly experimental approach loads both a 4-bit and 8-bit model into memory and \"transplants\" layers to create a hybrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b8a08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T00:01:40.770010Z",
     "iopub.status.busy": "2025-06-22T00:01:40.769728Z",
     "iopub.status.idle": "2025-06-22T00:01:40.777798Z",
     "shell.execute_reply": "2025-06-22T00:01:40.777266Z"
    },
    "papermill": {
     "duration": 1.421228,
     "end_time": "2025-06-22T00:01:40.778719",
     "exception": false,
     "start_time": "2025-06-22T00:01:39.357491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_dynamic_adaptive_hybrid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_dynamic_adaptive_hybrid.py\n",
    "\n",
    "# A \"Hybrid Model\" approach to dynamic adaptive quantization.\n",
    "# This script loads a full 8-bit and a full 4-bit model, then combines them.\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "import gc # For garbage collection\n",
    "\n",
    "# DDP, Transformers, PEFT imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "TRAIN_DATA_PATH = './final_train_set.csv'\n",
    "VAL_DATA_PATH = './final_validation_set.csv'\n",
    "FINAL_BACKBONE_PATH = './dynamic_hybrid_backbone/'\n",
    "FINAL_HEAD_PATH = './dynamic_hybrid_head.pt'\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_LEN = 512\n",
    "SEED = 42\n",
    "\n",
    "# --- Tokenizer Setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, prompts, targets=None):\n",
    "        self.prompts, self.targets = prompts, targets\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"prompt\": self.prompts[idx]}\n",
    "        if self.targets is not None: item[\"target\"] = self.targets[idx]\n",
    "        return item\n",
    "    def __len__(self): return len(self.prompts)\n",
    "\n",
    "# --- Model Definition with Hybrid Strategy ---\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, model_path, rank):\n",
    "        super(Net, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        \n",
    "        # Stage 1: Determine the quantization policy on the main process\n",
    "        policy_map = {}\n",
    "        if rank == 0:\n",
    "            print(\"--- Determining dynamic quantization policy ---\")\n",
    "            with init_empty_weights():\n",
    "                meta_model = AutoModelForCausalLM.from_config(self.config)\n",
    "            \n",
    "            layer_sizes = defaultdict(list)\n",
    "            for name, module in meta_model.named_modules():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    layer_sizes[module.in_features * module.out_features].append(name)\n",
    "            \n",
    "            unique_sizes = sorted(layer_sizes.keys(), reverse=True)\n",
    "            print(f\"Found {len(unique_sizes)} unique linear layer sizes.\")\n",
    "\n",
    "            for size in unique_sizes[:2]:\n",
    "                for layer_name in layer_sizes[size]: policy_map[layer_name] = 8\n",
    "            for size in unique_sizes[2:]:\n",
    "                for layer_name in layer_sizes[size]: policy_map[layer_name] = 4\n",
    "            \n",
    "            eight_bit_count = sum(1 for v in policy_map.values() if v == 8)\n",
    "            four_bit_count = sum(1 for v in policy_map.values() if v == 4)\n",
    "            print(f\"Policy determined: {eight_bit_count} layers to 8-bit, {four_bit_count} layers to 4-bit.\")\n",
    "            del meta_model\n",
    "            gc.collect()\n",
    "\n",
    "        # Broadcast the policy map from rank 0 to all other processes\n",
    "        policy_obj = [policy_map]\n",
    "        torch.distributed.broadcast_object_list(policy_obj, src=0)\n",
    "        policy_map = policy_obj[0]\n",
    "\n",
    "        # Stage 2: Load two full models\n",
    "        if rank == 0: print(\"Loading full 8-bit and 4-bit models (this is memory intensive)...\")\n",
    "        \n",
    "        bnb_config_8bit = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model_8bit = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config_8bit, device_map=rank, torch_dtype=torch.bfloat16)\n",
    "        \n",
    "        bnb_config_4bit = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "        model_4bit = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config_4bit, device_map=rank, torch_dtype=torch.bfloat16)\n",
    "\n",
    "        # Stage 3: Perform layer \"transplants\"\n",
    "        if rank == 0: print(\"Creating hybrid model by swapping layers...\")\n",
    "        self.backbone = model_8bit\n",
    "\n",
    "        for name, _ in self.backbone.named_modules():\n",
    "            if name in policy_map and policy_map[name] == 4:\n",
    "                parent_name, child_name = name.rsplit('.', 1)\n",
    "                parent_module = self.backbone.get_submodule(parent_name)\n",
    "                source_layer = model_4bit.get_submodule(name)\n",
    "                setattr(parent_module, child_name, source_layer)\n",
    "\n",
    "        # Stage 4: Clean up and apply PEFT\n",
    "        del model_4bit\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        if rank == 0: print(\"Hybrid model created. Applying LoRA...\")\n",
    "\n",
    "        peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], bias='none', inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.05)\n",
    "        self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(**x, output_hidden_states=True)\n",
    "        return self.head(outputs.hidden_states[-1][:, -1, :])\n",
    "\n",
    "# --- DDP Setup ---\n",
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "# --- Combined Training and Validation Process ---\n",
    "def train_and_validate_process(rank, world_size, train_prompts, train_targets, val_prompts, val_targets):\n",
    "    ddp_setup(rank, world_size)\n",
    "    \n",
    "    model = Net(BASE_MODEL_PATH, rank).to(rank)\n",
    "    if rank == 0:\n",
    "        # Print model size\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable model parameters: {total_params/1_000_000:.2f}M\")\n",
    "    model = DDP(model, device_ids=[rank], find_unused_parameters=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    train_dataset = MathDataset(train_prompts, train_targets)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True, drop_last=True)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) // GRAD_ACCUM_STEPS) * NUM_EPOCHS)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train(); train_sampler.set_epoch(epoch)\n",
    "        if rank == 0: print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}\"); pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        else: pbar = train_loader\n",
    "\n",
    "        for step, batch in enumerate(pbar):\n",
    "            encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "            batch_targets = batch['target'].long().to(rank)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = model(encodings)\n",
    "                loss = nn.functional.cross_entropy(logits, batch_targets)\n",
    "                loss = loss / GRAD_ACCUM_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n",
    "        \n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            val_dataset = MathDataset(val_prompts, val_targets)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "            all_preds = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                    encodings = tokenizer(batch['prompt'], return_tensors='pt', padding='max_length', truncation=True, max_length=MAX_LEN).to(rank)\n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        logits = model.module(encodings)\n",
    "                        preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "                    all_preds.extend(preds)\n",
    "            \n",
    "            f1_micro = f1_score(val_targets, all_preds, average='micro')\n",
    "            print(f'  Epoch {epoch+1} | Validation F1-micro: {f1_micro:.4f}')\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"\\nTraining complete. Saving final model...\")\n",
    "        model.module.backbone.save_pretrained(FINAL_BACKBONE_PATH)\n",
    "        torch.save(model.module.head.state_dict(), FINAL_HEAD_PATH)\n",
    "        print(f\"Final model saved to '{FINAL_BACKBONE_PATH}' and '{FINAL_HEAD_PATH}'\")\n",
    "\n",
    "    destroy_process_group()\n",
    "\n",
    "# --- Launcher Function ---\n",
    "def main():\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "    \n",
    "    try:\n",
    "        df_train = pd.read_csv(TRAIN_DATA_PATH); df_train.columns = ['problem', 'target']\n",
    "        df_val = pd.read_csv(VAL_DATA_PATH); df_val.columns = ['problem', 'target']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Make sure '{TRAIN_DATA_PATH}' and '{VAL_DATA_PATH}' exist.\")\n",
    "        return\n",
    "\n",
    "    prompt_template = \"\"\"<|im_start|>user\n",
    "Your task is to classify each Math problem...\n",
    "Math Problem: {problem}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "    \n",
    "    train_prompts = [prompt_template.format(problem=p.strip()) for p in df_train['problem']]\n",
    "    train_targets = df_train['target'].astype(int).tolist()\n",
    "    val_prompts = [prompt_template.format(problem=p.strip()) for p in df_val['problem']]\n",
    "    val_targets = df_val['target'].astype(int).tolist()\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size > 0:\n",
    "        print(f\"Using {world_size} GPUs for adaptive training.\")\n",
    "        args = (world_size, train_prompts, train_targets, val_prompts, val_targets)\n",
    "        mp.spawn(train_and_validate_process, args=args, nprocs=world_size)\n",
    "    else:\n",
    "        print(\"This script requires GPUs.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "367f5bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T00:01:43.600595Z",
     "iopub.status.busy": "2025-06-22T00:01:43.599829Z",
     "iopub.status.idle": "2025-06-22T00:23:10.495185Z",
     "shell.execute_reply": "2025-06-22T00:23:10.494456Z"
    },
    "papermill": {
     "duration": 1288.306216,
     "end_time": "2025-06-22T00:23:10.496595",
     "exception": false,
     "start_time": "2025-06-22T00:01:42.190379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-22 00:01:48.989356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750550509.011646    4866 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750550509.018481    4866 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using 4 GPUs for adaptive training.\r\n",
      "2025-06-22 00:01:57.311077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750550517.332972    4934 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750550517.339700    4934 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-22 00:02:05.620020: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750550525.641623    5001 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750550525.648286    5001 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-22 00:02:13.937855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750550533.960022    5070 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750550533.966777    5070 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-06-22 00:02:22.211107: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1750550542.232560    5139 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1750550542.239134    5139 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Determining dynamic quantization policy ---\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Found 4 unique linear layer sizes.\r\n",
      "Policy determined: 73 layers to 8-bit, 96 layers to 4-bit.\r\n",
      "Loading full 8-bit and 4-bit models (this is memory intensive)...\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\r\n",
      "Creating hybrid model by swapping layers...\r\n",
      "Hybrid model created. Applying LoRA...\r\n",
      "Trainable model parameters: 8.81M\r\n",
      "  Epoch 1/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "[rank0]:[W622 00:02:32.708637252 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "[rank3]:[W622 00:02:32.726905562 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "[rank2]:[W622 00:02:32.736746306 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "[rank1]:[W622 00:02:32.737879786 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:25<00:00,  2.79it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:41<00:00,  3.09it/s]\r\n",
      "  Epoch 1 | Validation F1-micro: 0.8469\r\n",
      "  Epoch 2/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:26<00:00,  2.78it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:41<00:00,  3.08it/s]\r\n",
      "  Epoch 2 | Validation F1-micro: 0.8656\r\n",
      "  Epoch 3/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:25<00:00,  2.79it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:41<00:00,  3.07it/s]\r\n",
      "  Epoch 3 | Validation F1-micro: 0.8754\r\n",
      "  Epoch 4/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:24<00:00,  2.81it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:41<00:00,  3.07it/s]\r\n",
      "  Epoch 4 | Validation F1-micro: 0.8813\r\n",
      "  Epoch 5/5\r\n",
      "Training:   0%|                                         | 0/573 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\r\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\r\n",
      "Training: 100%|███████████████████████████████| 573/573 [03:24<00:00,  2.81it/s]\r\n",
      "Validating: 100%|█████████████████████████████| 128/128 [00:41<00:00,  3.05it/s]\r\n",
      "  Epoch 5 | Validation F1-micro: 0.8793\r\n",
      "\r\n",
      "Training complete. Saving final model...\r\n",
      "Final model saved to './dynamic_hybrid_backbone/' and './dynamic_hybrid_head.pt'\r\n"
     ]
    }
   ],
   "source": [
    "!python train_dynamic_adaptive_hybrid.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11376393,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 11615683,
     "sourceId": 97669,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 71342,
     "modelInstanceId": 52023,
     "sourceId": 62292,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141432,
     "sourceId": 166218,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301527,
     "sourceId": 363149,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11198.670478,
   "end_time": "2025-06-22T00:23:12.520718",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-21T21:16:33.850240",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
