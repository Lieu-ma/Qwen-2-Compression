{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":62292,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":52023,"modelId":71342}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:18:42.585741Z","iopub.execute_input":"2025-06-21T10:18:42.585918Z","iopub.status.idle":"2025-06-21T10:18:42.938998Z","shell.execute_reply.started":"2025-06-21T10:18:42.585902Z","shell.execute_reply":"2025-06-21T10:18:42.938142Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/qwen2/transformers/qwen2-0.5b-instruct/1/config.json\n/kaggle/input/qwen2/transformers/qwen2-0.5b-instruct/1/merges.txt\n/kaggle/input/qwen2/transformers/qwen2-0.5b-instruct/1/LICENSE\n/kaggle/input/qwen2/transformers/qwen2-0.5b-instruct/1/README.md\n/kaggle/input/qwen2/transformers/qwen2-0.5b-instruct/1/tokenizer.json\n/kaggle/input/qwen2/transformers/qwen2-0.5b-instruct/1/vocab.json\n/kaggle/input/qwen2/transformers/qwen2-0.5b-instruct/1/tokenizer_config.json\n/kaggle/input/qwen2/transformers/qwen2-0.5b-instruct/1/model.safetensors\n/kaggle/input/qwen2/transformers/qwen2-0.5b-instruct/1/configuration.json\n/kaggle/input/qwen2/transformers/qwen2-0.5b-instruct/1/generation_config.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers datasets evaluate torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:18:42.940045Z","iopub.execute_input":"2025-06-21T10:18:42.940539Z","iopub.status.idle":"2025-06-21T10:18:47.821868Z","shell.execute_reply.started":"2025-06-21T10:18:42.940482Z","shell.execute_reply":"2025-06-21T10:18:47.820978Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# --- 1. Load Model and Tokenizer ---\nprint(f\"Loading model: {model_path}\")\ntokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nprint(\"Model and tokenizer loaded successfully.\")\n\n# --- 2. Sort All Layers by Parameter Proportion ---\nprint(\"\\n--- Analyzing Model Layers ---\")\nlayer_params = []\nfor name, module in model.named_modules():\n    if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Embedding):\n        num_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n        if num_params > 0:\n            layer_params.append((name, num_params))\n\nsorted_layers = sorted(layer_params, key=lambda x: x[1], reverse=True)\ndf_layers = pd.DataFrame(sorted_layers, columns=['Layer Name', 'Number of Parameters'])\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ndf_layers['Percentage of Total'] = (df_layers['Number of Parameters'] / total_params) * 100\nprint(\"All Layers Sorted by Parameter Count:\")\nprint(df_layers.to_string())\n\n# --- 3. Select the Layer with the Highest Percentage of Parameters ---\ntop_layer_info = df_layers.iloc[0]\ntop_layer_name = top_layer_info['Layer Name']\nprint(f\"\\nSelected for pruning (Largest Layer): '{top_layer_name}' with {top_layer_info['Number of Parameters']:,} parameters ({top_layer_info['Percentage of Total']:.2f}% of total)\")\n\n# --- 4. Pruning and Evaluation Workflow ---\n\ndef evaluate_perplexity(model, tokenizer, dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"test\"):\n    print(\"Evaluating perplexity with sliding window...\")\n    test_dataset = load_dataset(dataset, subset, split=split)\n    encodings = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n    max_length = 2048\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Evaluating Chunks\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n        if input_ids.size(1) < 2:\n            continue\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n    perplexity = torch.exp(torch.stack(nlls).mean())\n    return perplexity.item()\n\n# --- Main Loop for Pruning and Evaluation ---\nsparsity_levels = [0.0, 0.25, 0.50, 0.75, 0.90, 0.95]\nresults_data = []\ntarget_module = dict(model.named_modules())[top_layer_name]\n\n# *** FIX: Move the state_dict to CPU RAM before cloning to prevent OOM Error ***\nprint(\"\\nCreating a model backup on the CPU...\")\noriginal_state_dict = {k: v.to('cpu').clone() for k, v in model.state_dict().items()}\nprint(\"Backup created successfully.\")\n\ntorch.cuda.empty_cache()\n\nprint(\"\\n--- Starting Iterative Pruning and Evaluation ---\")\nfor sparsity in sparsity_levels:\n    # load_state_dict will automatically move the CPU tensors back to the correct GPU device\n    model.load_state_dict(original_state_dict)\n    print(f\"\\n--- Processing Sparsity: {sparsity*100:.0f}% ---\")\n    if sparsity > 0.0:\n        print(f\"Applying {sparsity*100:.0f}% pruning to '{top_layer_name}'...\")\n        prune.l1_unstructured(target_module, name=\"weight\", amount=sparsity)\n        prune.remove(target_module, 'weight')\n    perplexity = evaluate_perplexity(model, tokenizer)\n    results_data.append({'Sparsity (%)': int(sparsity * 100), 'Perplexity': perplexity})\n    print(f\"Result: Sparsity = {sparsity*100:.0f}%, Perplexity = {perplexity:.4f}\")\n    torch.cuda.empty_cache()\n\n# --- 5. Identify the Maximum Pruning Ratio ---\nprint(\"\\n--- Pruning Experiment Complete ---\")\ndf_results = pd.DataFrame(results_data)\ndf_results.set_index('Sparsity (%)', inplace=True)\nprint(\"Final Results:\")\nprint(df_results)\nbaseline_perplexity = df_results.loc[0]['Perplexity']\nprint(f\"\\nAnalysis:\")\nprint(f\"Baseline perplexity (0% pruning) is: {baseline_perplexity:.4f}\")\nprint(\"Lower perplexity is better. A sharp increase indicates significant performance degradation.\")\nacceptable_threshold = baseline_perplexity * 2\nacceptable_pruning = df_results[df_results['Perplexity'] <= acceptable_threshold]\nif not acceptable_pruning.empty:\n    max_pruning_ratio = acceptable_pruning.index.max()\n    print(f\"The maximum pruning ratio that keeps perplexity below {acceptable_threshold:.2f} (2x baseline) is: {max_pruning_ratio}%\")\nelse:\n    print(\"Even 25% pruning caused a significant drop in performance beyond the 2x baseline threshold.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:18:47.823018Z","iopub.execute_input":"2025-06-21T10:18:47.823279Z","iopub.status.idle":"2025-06-21T10:28:12.866192Z","shell.execute_reply.started":"2025-06-21T10:18:47.823252Z","shell.execute_reply":"2025-06-21T10:28:12.865433Z"}},"outputs":[{"name":"stdout","text":"Loading model: Qwen/Qwen2-0.5B-Instruct\n","output_type":"stream"},{"name":"stderr","text":"2025-06-21 10:18:54.654664: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750501134.675217     127 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750501134.681076     127 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n","output_type":"stream"},{"name":"stdout","text":"Model and tokenizer loaded successfully.\n\n--- Analyzing Model Layers ---\nAll Layers Sorted by Parameter Count:\n                           Layer Name  Number of Parameters  Percentage of Total\n0                  model.embed_tokens             136134656            27.555795\n1                             lm_head             136134656            27.555795\n2        model.layers.0.mlp.gate_proj               4358144             0.882157\n3          model.layers.0.mlp.up_proj               4358144             0.882157\n4        model.layers.0.mlp.down_proj               4358144             0.882157\n5        model.layers.1.mlp.gate_proj               4358144             0.882157\n6          model.layers.1.mlp.up_proj               4358144             0.882157\n7        model.layers.1.mlp.down_proj               4358144             0.882157\n8        model.layers.2.mlp.gate_proj               4358144             0.882157\n9          model.layers.2.mlp.up_proj               4358144             0.882157\n10       model.layers.2.mlp.down_proj               4358144             0.882157\n11       model.layers.3.mlp.gate_proj               4358144             0.882157\n12         model.layers.3.mlp.up_proj               4358144             0.882157\n13       model.layers.3.mlp.down_proj               4358144             0.882157\n14       model.layers.4.mlp.gate_proj               4358144             0.882157\n15         model.layers.4.mlp.up_proj               4358144             0.882157\n16       model.layers.4.mlp.down_proj               4358144             0.882157\n17       model.layers.5.mlp.gate_proj               4358144             0.882157\n18         model.layers.5.mlp.up_proj               4358144             0.882157\n19       model.layers.5.mlp.down_proj               4358144             0.882157\n20       model.layers.6.mlp.gate_proj               4358144             0.882157\n21         model.layers.6.mlp.up_proj               4358144             0.882157\n22       model.layers.6.mlp.down_proj               4358144             0.882157\n23       model.layers.7.mlp.gate_proj               4358144             0.882157\n24         model.layers.7.mlp.up_proj               4358144             0.882157\n25       model.layers.7.mlp.down_proj               4358144             0.882157\n26       model.layers.8.mlp.gate_proj               4358144             0.882157\n27         model.layers.8.mlp.up_proj               4358144             0.882157\n28       model.layers.8.mlp.down_proj               4358144             0.882157\n29       model.layers.9.mlp.gate_proj               4358144             0.882157\n30         model.layers.9.mlp.up_proj               4358144             0.882157\n31       model.layers.9.mlp.down_proj               4358144             0.882157\n32      model.layers.10.mlp.gate_proj               4358144             0.882157\n33        model.layers.10.mlp.up_proj               4358144             0.882157\n34      model.layers.10.mlp.down_proj               4358144             0.882157\n35      model.layers.11.mlp.gate_proj               4358144             0.882157\n36        model.layers.11.mlp.up_proj               4358144             0.882157\n37      model.layers.11.mlp.down_proj               4358144             0.882157\n38      model.layers.12.mlp.gate_proj               4358144             0.882157\n39        model.layers.12.mlp.up_proj               4358144             0.882157\n40      model.layers.12.mlp.down_proj               4358144             0.882157\n41      model.layers.13.mlp.gate_proj               4358144             0.882157\n42        model.layers.13.mlp.up_proj               4358144             0.882157\n43      model.layers.13.mlp.down_proj               4358144             0.882157\n44      model.layers.14.mlp.gate_proj               4358144             0.882157\n45        model.layers.14.mlp.up_proj               4358144             0.882157\n46      model.layers.14.mlp.down_proj               4358144             0.882157\n47      model.layers.15.mlp.gate_proj               4358144             0.882157\n48        model.layers.15.mlp.up_proj               4358144             0.882157\n49      model.layers.15.mlp.down_proj               4358144             0.882157\n50      model.layers.16.mlp.gate_proj               4358144             0.882157\n51        model.layers.16.mlp.up_proj               4358144             0.882157\n52      model.layers.16.mlp.down_proj               4358144             0.882157\n53      model.layers.17.mlp.gate_proj               4358144             0.882157\n54        model.layers.17.mlp.up_proj               4358144             0.882157\n55      model.layers.17.mlp.down_proj               4358144             0.882157\n56      model.layers.18.mlp.gate_proj               4358144             0.882157\n57        model.layers.18.mlp.up_proj               4358144             0.882157\n58      model.layers.18.mlp.down_proj               4358144             0.882157\n59      model.layers.19.mlp.gate_proj               4358144             0.882157\n60        model.layers.19.mlp.up_proj               4358144             0.882157\n61      model.layers.19.mlp.down_proj               4358144             0.882157\n62      model.layers.20.mlp.gate_proj               4358144             0.882157\n63        model.layers.20.mlp.up_proj               4358144             0.882157\n64      model.layers.20.mlp.down_proj               4358144             0.882157\n65      model.layers.21.mlp.gate_proj               4358144             0.882157\n66        model.layers.21.mlp.up_proj               4358144             0.882157\n67      model.layers.21.mlp.down_proj               4358144             0.882157\n68      model.layers.22.mlp.gate_proj               4358144             0.882157\n69        model.layers.22.mlp.up_proj               4358144             0.882157\n70      model.layers.22.mlp.down_proj               4358144             0.882157\n71      model.layers.23.mlp.gate_proj               4358144             0.882157\n72        model.layers.23.mlp.up_proj               4358144             0.882157\n73      model.layers.23.mlp.down_proj               4358144             0.882157\n74    model.layers.0.self_attn.q_proj                803712             0.162684\n75    model.layers.1.self_attn.q_proj                803712             0.162684\n76    model.layers.2.self_attn.q_proj                803712             0.162684\n77    model.layers.3.self_attn.q_proj                803712             0.162684\n78    model.layers.4.self_attn.q_proj                803712             0.162684\n79    model.layers.5.self_attn.q_proj                803712             0.162684\n80    model.layers.6.self_attn.q_proj                803712             0.162684\n81    model.layers.7.self_attn.q_proj                803712             0.162684\n82    model.layers.8.self_attn.q_proj                803712             0.162684\n83    model.layers.9.self_attn.q_proj                803712             0.162684\n84   model.layers.10.self_attn.q_proj                803712             0.162684\n85   model.layers.11.self_attn.q_proj                803712             0.162684\n86   model.layers.12.self_attn.q_proj                803712             0.162684\n87   model.layers.13.self_attn.q_proj                803712             0.162684\n88   model.layers.14.self_attn.q_proj                803712             0.162684\n89   model.layers.15.self_attn.q_proj                803712             0.162684\n90   model.layers.16.self_attn.q_proj                803712             0.162684\n91   model.layers.17.self_attn.q_proj                803712             0.162684\n92   model.layers.18.self_attn.q_proj                803712             0.162684\n93   model.layers.19.self_attn.q_proj                803712             0.162684\n94   model.layers.20.self_attn.q_proj                803712             0.162684\n95   model.layers.21.self_attn.q_proj                803712             0.162684\n96   model.layers.22.self_attn.q_proj                803712             0.162684\n97   model.layers.23.self_attn.q_proj                803712             0.162684\n98    model.layers.0.self_attn.o_proj                802816             0.162503\n99    model.layers.1.self_attn.o_proj                802816             0.162503\n100   model.layers.2.self_attn.o_proj                802816             0.162503\n101   model.layers.3.self_attn.o_proj                802816             0.162503\n102   model.layers.4.self_attn.o_proj                802816             0.162503\n103   model.layers.5.self_attn.o_proj                802816             0.162503\n104   model.layers.6.self_attn.o_proj                802816             0.162503\n105   model.layers.7.self_attn.o_proj                802816             0.162503\n106   model.layers.8.self_attn.o_proj                802816             0.162503\n107   model.layers.9.self_attn.o_proj                802816             0.162503\n108  model.layers.10.self_attn.o_proj                802816             0.162503\n109  model.layers.11.self_attn.o_proj                802816             0.162503\n110  model.layers.12.self_attn.o_proj                802816             0.162503\n111  model.layers.13.self_attn.o_proj                802816             0.162503\n112  model.layers.14.self_attn.o_proj                802816             0.162503\n113  model.layers.15.self_attn.o_proj                802816             0.162503\n114  model.layers.16.self_attn.o_proj                802816             0.162503\n115  model.layers.17.self_attn.o_proj                802816             0.162503\n116  model.layers.18.self_attn.o_proj                802816             0.162503\n117  model.layers.19.self_attn.o_proj                802816             0.162503\n118  model.layers.20.self_attn.o_proj                802816             0.162503\n119  model.layers.21.self_attn.o_proj                802816             0.162503\n120  model.layers.22.self_attn.o_proj                802816             0.162503\n121  model.layers.23.self_attn.o_proj                802816             0.162503\n122   model.layers.0.self_attn.k_proj                114816             0.023241\n123   model.layers.0.self_attn.v_proj                114816             0.023241\n124   model.layers.1.self_attn.k_proj                114816             0.023241\n125   model.layers.1.self_attn.v_proj                114816             0.023241\n126   model.layers.2.self_attn.k_proj                114816             0.023241\n127   model.layers.2.self_attn.v_proj                114816             0.023241\n128   model.layers.3.self_attn.k_proj                114816             0.023241\n129   model.layers.3.self_attn.v_proj                114816             0.023241\n130   model.layers.4.self_attn.k_proj                114816             0.023241\n131   model.layers.4.self_attn.v_proj                114816             0.023241\n132   model.layers.5.self_attn.k_proj                114816             0.023241\n133   model.layers.5.self_attn.v_proj                114816             0.023241\n134   model.layers.6.self_attn.k_proj                114816             0.023241\n135   model.layers.6.self_attn.v_proj                114816             0.023241\n136   model.layers.7.self_attn.k_proj                114816             0.023241\n137   model.layers.7.self_attn.v_proj                114816             0.023241\n138   model.layers.8.self_attn.k_proj                114816             0.023241\n139   model.layers.8.self_attn.v_proj                114816             0.023241\n140   model.layers.9.self_attn.k_proj                114816             0.023241\n141   model.layers.9.self_attn.v_proj                114816             0.023241\n142  model.layers.10.self_attn.k_proj                114816             0.023241\n143  model.layers.10.self_attn.v_proj                114816             0.023241\n144  model.layers.11.self_attn.k_proj                114816             0.023241\n145  model.layers.11.self_attn.v_proj                114816             0.023241\n146  model.layers.12.self_attn.k_proj                114816             0.023241\n147  model.layers.12.self_attn.v_proj                114816             0.023241\n148  model.layers.13.self_attn.k_proj                114816             0.023241\n149  model.layers.13.self_attn.v_proj                114816             0.023241\n150  model.layers.14.self_attn.k_proj                114816             0.023241\n151  model.layers.14.self_attn.v_proj                114816             0.023241\n152  model.layers.15.self_attn.k_proj                114816             0.023241\n153  model.layers.15.self_attn.v_proj                114816             0.023241\n154  model.layers.16.self_attn.k_proj                114816             0.023241\n155  model.layers.16.self_attn.v_proj                114816             0.023241\n156  model.layers.17.self_attn.k_proj                114816             0.023241\n157  model.layers.17.self_attn.v_proj                114816             0.023241\n158  model.layers.18.self_attn.k_proj                114816             0.023241\n159  model.layers.18.self_attn.v_proj                114816             0.023241\n160  model.layers.19.self_attn.k_proj                114816             0.023241\n161  model.layers.19.self_attn.v_proj                114816             0.023241\n162  model.layers.20.self_attn.k_proj                114816             0.023241\n163  model.layers.20.self_attn.v_proj                114816             0.023241\n164  model.layers.21.self_attn.k_proj                114816             0.023241\n165  model.layers.21.self_attn.v_proj                114816             0.023241\n166  model.layers.22.self_attn.k_proj                114816             0.023241\n167  model.layers.22.self_attn.v_proj                114816             0.023241\n168  model.layers.23.self_attn.k_proj                114816             0.023241\n169  model.layers.23.self_attn.v_proj                114816             0.023241\n\nSelected for pruning (Largest Layer): 'model.embed_tokens' with 136,134,656 parameters (27.56% of total)\n\nCreating a model backup on the CPU...\nBackup created successfully.\n\n--- Starting Iterative Pruning and Evaluation ---\n\n--- Processing Sparsity: 0% ---\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f4b72b025be4c4b892a70f961814705"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"377856cecab9454db50bdca605ac9b92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"962d681169454f3f907285fb74f794fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55625e592bb54f2aad180361a8afcb11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa8061af1b94eab9ecfc06165af118a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38234e730be54ed48bc8cb9c262d752b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ae534529173463d841a32a7dda75522"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 32768). Running this sequence through the model will result in indexing errors\nEvaluating Chunks:  99%|█████████▉| 581/585 [01:25<00:00,  6.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Result: Sparsity = 0%, Perplexity = 12.5985\n\n--- Processing Sparsity: 25% ---\nApplying 25% pruning to 'model.embed_tokens'...\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Chunks:  99%|█████████▉| 581/585 [01:28<00:00,  6.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Result: Sparsity = 25%, Perplexity = 12.9024\n\n--- Processing Sparsity: 50% ---\nApplying 50% pruning to 'model.embed_tokens'...\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Chunks:  99%|█████████▉| 581/585 [01:28<00:00,  6.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Result: Sparsity = 50%, Perplexity = 17.0475\n\n--- Processing Sparsity: 75% ---\nApplying 75% pruning to 'model.embed_tokens'...\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Chunks:  99%|█████████▉| 581/585 [01:28<00:00,  6.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Result: Sparsity = 75%, Perplexity = 89.3184\n\n--- Processing Sparsity: 90% ---\nApplying 90% pruning to 'model.embed_tokens'...\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Chunks:  99%|█████████▉| 581/585 [01:28<00:00,  6.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Result: Sparsity = 90%, Perplexity = 6319.6558\n\n--- Processing Sparsity: 95% ---\nApplying 95% pruning to 'model.embed_tokens'...\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Chunks:  99%|█████████▉| 581/585 [01:28<00:00,  6.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Result: Sparsity = 95%, Perplexity = 38864.8047\n\n--- Pruning Experiment Complete ---\nFinal Results:\n                Perplexity\nSparsity (%)              \n0                12.598527\n25               12.902409\n50               17.047497\n75               89.318359\n90             6319.655762\n95            38864.804688\n\nAnalysis:\nBaseline perplexity (0% pruning) is: 12.5985\nLower perplexity is better. A sharp increase indicates significant performance degradation.\nThe maximum pruning ratio that keeps perplexity below 25.20 (2x baseline) is: 50%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Re-create the DataFrame from your results\ndata = {\n    'Sparsity (%)': [0, 25, 50, 75, 90, 95],\n    'Perplexity': [12.5985, 12.9024, 17.0475, 89.3184, 6319.6558, 38864.8047]\n}\ndf_results = pd.DataFrame(data)\n\n# Create the plot\nplt.figure(figsize=(10, 6))\nplt.plot(df_results['Sparsity (%)'], df_results['Perplexity'], marker='o', linestyle='-')\n\n# Add titles and labels for clarity\nplt.title('Impact of Pruning on Model Perplexity', fontsize=16)\nplt.xlabel('Sparsity (%) on embed_tokens Layer', fontsize=12)\nplt.ylabel('Perplexity (Lower is Better)', fontsize=12)\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\n\n# Annotate the \"elbow\" point\nplt.annotate('Performance \"Elbow\"',\n             xy=(75, 89.32),\n             xytext=(50, 5000),\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             fontsize=12)\n\n# Use a logarithmic scale for the y-axis to better visualize the huge jump\nplt.yscale('log')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:28:12.868293Z","iopub.execute_input":"2025-06-21T10:28:12.868844Z","iopub.status.idle":"2025-06-21T10:28:13.423273Z","shell.execute_reply.started":"2025-06-21T10:28:12.868822Z","shell.execute_reply":"2025-06-21T10:28:13.422432Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\n# The model path is from the provided notebook.\n# Ensure you have access to this model, e.g., via Hugging Face Hub.\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# --- 1. Load Model and Tokenizer ---\nprint(f\"Loading model: {model_path}\")\ntokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Load the model onto the GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nprint(\"Model and tokenizer loaded successfully.\")\n\n# --- 2. Perplexity Evaluation Function (from the original notebook) ---\ndef evaluate_perplexity(model, tokenizer, dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"test\"):\n    \"\"\"\n    Evaluates the perplexity of a model on a given dataset using a sliding window approach.\n    \"\"\"\n    print(\"Evaluating perplexity with sliding window...\")\n    test_dataset = load_dataset(dataset, subset, split=split)\n    encodings = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n\n    max_length = 2048  # Max length for each chunk\n    stride = 512     # Sliding window stride\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Evaluating Chunks\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        if input_ids.size(1) < 2:\n            continue\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    perplexity = torch.exp(torch.stack(nlls).mean())\n    return perplexity.item()\n\n# --- 3. Pruning and Evaluation Workflow for ALL Layers ---\n\n# Sparsity levels to test, as requested\nsparsity_levels = [0.0, 0.25, 0.50]\nresults_data = []\n\n# Create a backup of the original model's state on CPU to prevent GPU OOM errors\nprint(\"\\nCreating a model backup on the CPU...\")\noriginal_state_dict = {k: v.to('cpu').clone() for k, v in model.state_dict().items()}\nprint(\"Backup created successfully.\")\n\n# Identify all prunable layers (Linear and Embedding)\nprunable_modules = []\nfor name, module in model.named_modules():\n    if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Embedding):\n        prunable_modules.append(module)\n\nprint(f\"\\nIdentified {len(prunable_modules)} prunable layers to be pruned equally.\")\n\n# Clear cache before starting the main loop\ntorch.cuda.empty_cache()\n\nprint(\"\\n--- Starting Iterative Pruning and Evaluation on ALL Prunable Layers ---\")\nfor sparsity in sparsity_levels:\n    # Reset the model to its original, unpruned state before applying a new sparsity level\n    model.load_state_dict(original_state_dict)\n    print(f\"\\n--- Processing Sparsity: {sparsity*100:.0f}% ---\")\n\n    if sparsity > 0.0:\n        print(f\"Applying {sparsity*100:.0f}% L1 unstructured pruning to all identified layers...\")\n        \n        # Apply the pruning mask to each identified module\n        for module in prunable_modules:\n            prune.l1_unstructured(module, name=\"weight\", amount=sparsity)\n        \n        # Make the pruning permanent by removing the re-parameterization hooks\n        for module in prunable_modules:\n            prune.remove(module, 'weight')\n\n    # Evaluate the perplexity of the pruned model\n    perplexity = evaluate_perplexity(model, tokenizer)\n    results_data.append({'Sparsity (%)': int(sparsity * 100), 'Perplexity': perplexity})\n    print(f\"Result: Sparsity = {sparsity*100:.0f}%, Perplexity = {perplexity:.4f}\")\n    \n    # Clean up GPU memory\n    torch.cuda.empty_cache()\n\n# --- 4. Display Final Results ---\nprint(\"\\n--- Pruning Experiment Complete ---\")\ndf_results = pd.DataFrame(results_data)\ndf_results.set_index('Sparsity (%)', inplace=True)\nprint(\"Final Results:\")\nprint(df_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:28:13.424229Z","iopub.execute_input":"2025-06-21T10:28:13.424577Z","iopub.status.idle":"2025-06-21T10:32:56.048753Z","shell.execute_reply.started":"2025-06-21T10:28:13.424549Z","shell.execute_reply":"2025-06-21T10:32:56.047910Z"}},"outputs":[{"name":"stdout","text":"Loading model: Qwen/Qwen2-0.5B-Instruct\nModel and tokenizer loaded successfully.\n\nCreating a model backup on the CPU...\nBackup created successfully.\n\nIdentified 170 prunable layers to be pruned equally.\n\n--- Starting Iterative Pruning and Evaluation on ALL Prunable Layers ---\n\n--- Processing Sparsity: 0% ---\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 32768). Running this sequence through the model will result in indexing errors\nEvaluating Chunks:  99%|█████████▉| 581/585 [01:29<00:00,  6.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Result: Sparsity = 0%, Perplexity = 12.5985\n\n--- Processing Sparsity: 25% ---\nApplying 25% L1 unstructured pruning to all identified layers...\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Chunks:  99%|█████████▉| 581/585 [01:28<00:00,  6.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Result: Sparsity = 25%, Perplexity = 15.2146\n\n--- Processing Sparsity: 50% ---\nApplying 50% L1 unstructured pruning to all identified layers...\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Chunks:  99%|█████████▉| 581/585 [01:28<00:00,  6.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Result: Sparsity = 50%, Perplexity = 184.5858\n\n--- Pruning Experiment Complete ---\nFinal Results:\n              Perplexity\nSparsity (%)            \n0              12.598527\n25             15.214593\n50            184.585800\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# --- Constants and Configuration ---\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\noutput_csv_filename = \"statistical_analysis_of_layers.csv\"\n\n# --- 1. Load Model and Tokenizer ---\nprint(f\"Loading model: {model_path}\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nprint(\"Model loaded successfully.\")\n\n\n# --- 2. Perform Statistical Analysis ---\nprint(\"\\n--- Performing Statistical Analysis of Model Layers ---\")\n\nlayer_stats = []\n# Iterate through all named modules in the model\nfor name, module in model.named_modules():\n    # We are interested in layers that have a 'weight' parameter\n    if (isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Embedding)) and hasattr(module, 'weight'):\n        # Use torch.no_grad() to ensure no gradients are computed during this analysis\n        with torch.no_grad():\n            weights = module.weight.data\n            num_params = weights.numel()\n\n            # Move weights to float32 for stable calculations, then get stats\n            # .item() moves the scalar value from the GPU to the CPU\n            mean_val = weights.to(torch.float32).mean().item()\n            std_val = weights.to(torch.float32).std().item()\n            min_val = weights.min().item()\n            max_val = weights.max().item()\n\n            layer_stats.append({\n                'Layer Name': name,\n                'Number of Parameters': num_params,\n                'Mean': mean_val,\n                'Std Dev': std_val,\n                'Min': min_val,\n                'Max': max_val\n            })\n\n# Create a DataFrame from the collected statistics\ndf_stats = pd.DataFrame(layer_stats)\n\n# Sort the DataFrame by the number of parameters in descending order for better analysis\ndf_stats = df_stats.sort_values(by='Number of Parameters', ascending=False).reset_index(drop=True)\n\n\n# --- 3. Save DataFrame to CSV File ---\n# This line saves the DataFrame to a CSV file.\n# The `index=False` argument prevents pandas from writing the DataFrame index as a column.\ndf_stats.to_csv(output_csv_filename, index=False)\n\nprint(f\"\\nStatistical analysis complete. The results have been saved to '{output_csv_filename}'.\")\n\n# (Optional) Print the table to the console as well\nprint(\"\\nStatistical Analysis of Model Layer Weights:\")\npd.set_option('display.max_rows', 15) # Adjust how many rows to print\nprint(df_stats)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T11:03:16.923386Z","iopub.execute_input":"2025-06-21T11:03:16.923679Z","iopub.status.idle":"2025-06-21T11:03:18.785596Z","shell.execute_reply.started":"2025-06-21T11:03:16.923660Z","shell.execute_reply":"2025-06-21T11:03:18.784990Z"}},"outputs":[{"name":"stdout","text":"Loading model: Qwen/Qwen2-0.5B-Instruct\nModel loaded successfully.\n\n--- Performing Statistical Analysis of Model Layers ---\n\nStatistical analysis complete. The results have been saved to 'statistical_analysis_of_layers.csv'.\n\nStatistical Analysis of Model Layer Weights:\n                           Layer Name  Number of Parameters      Mean  \\\n0                  model.embed_tokens             136134656  0.000137   \n1                             lm_head             136134656  0.000137   \n2         model.layers.11.mlp.up_proj               4358144 -0.000002   \n3       model.layers.11.mlp.gate_proj               4358144  0.000094   \n4       model.layers.10.mlp.down_proj               4358144  0.000006   \n..                                ...                   ...       ...   \n165   model.layers.1.self_attn.v_proj                114688 -0.000029   \n166   model.layers.2.self_attn.k_proj                114688  0.000007   \n167   model.layers.2.self_attn.v_proj                114688  0.000002   \n168  model.layers.22.self_attn.k_proj                114688 -0.000081   \n169  model.layers.22.self_attn.v_proj                114688 -0.000054   \n\n      Std Dev       Min       Max  \n0    0.015229 -0.196289  0.127930  \n1    0.015229 -0.196289  0.127930  \n2    0.017418 -0.460938  0.359375  \n3    0.020454 -0.371094  0.402344  \n4    0.016688 -0.349609  0.443359  \n..        ...       ...       ...  \n165  0.009526 -0.073730  0.072754  \n166  0.029109 -0.219727  0.199219  \n167  0.014231 -0.106445  0.135742  \n168  0.018371 -0.167969  0.224609  \n169  0.029680 -0.219727  0.239258  \n\n[170 rows x 6 columns]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:38:16.317181Z","iopub.execute_input":"2025-06-21T10:38:16.317868Z","iopub.status.idle":"2025-06-21T10:38:19.819909Z","shell.execute_reply.started":"2025-06-21T10:38:16.317835Z","shell.execute_reply":"2025-06-21T10:38:19.818776Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!pip install -U -q bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:42:21.341009Z","iopub.execute_input":"2025-06-21T10:42:21.341178Z","iopub.status.idle":"2025-06-21T10:43:47.595507Z","shell.execute_reply.started":"2025-06-21T10:42:21.341162Z","shell.execute_reply":"2025-06-21T10:43:47.594792Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# --- Constants and Configuration ---\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\n\n# --- 1. Load Model and Tokenizer with 8-Bit Quantization ---\nprint(\"Loading model with 8-bit quantization...\")\n\n# Configuration for 8-bit quantization\nquantization_config_8bit = BitsAndBytesConfig(load_in_8bit=True)\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Load the model with the specified quantization config\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    quantization_config=quantization_config_8bit,\n    device_map=\"auto\",\n)\n\nprint(\"\\n--- Model Loaded with 8-Bit Quantization ---\")\n\n# --- 2. Verify Model Layers ---\n# You can inspect the model to see that the Linear layers have been replaced.\nprint(\"\\nVerifying a layer in the 8-bit quantized model:\")\nprint(model_8bit.model.layers[0].mlp.gate_proj)\n\n# --- 3. Statistical Analysis of the 8-Bit Model ---\n# You can run the same statistical analysis as before on the quantized model.\n# Note that accessing the '.weight' attribute of a bnb quantized layer\n# gives you the de-quantized weights for analysis.\n\nimport pandas as pd\n\nprint(\"\\n--- Performing Statistical Analysis of 8-Bit Model Layers ---\")\n\nlayer_stats_8bit = []\nfor name, module in model_8bit.named_modules():\n    # Check for bitsandbytes quantized linear layers\n    if isinstance(module, torch.nn.Module) and 'Linear' in str(type(module)):\n         if hasattr(module, 'weight'):\n            with torch.no_grad():\n                weights = module.weight.data\n                num_params = weights.numel()\n\n                mean_val = weights.to(torch.float32).mean().item()\n                std_val = weights.to(torch.float32).std().item()\n                min_val = weights.min().item()\n                max_val = weights.max().item()\n\n                layer_stats_8bit.append({\n                    'Layer Name': name,\n                    'Data Type': weights.dtype,\n                    'Number of Parameters': num_params,\n                    'Mean': mean_val,\n                    'Std Dev': std_val,\n                    'Min': min_val,\n                    'Max': max_val\n                })\n\ndf_stats_8bit = pd.DataFrame(layer_stats_8bit)\ndf_stats_8bit.to_csv(\"statistical_analysis_8bit_model.csv\", index=False)\nprint(\"\\n8-bit model analysis saved to 'statistical_analysis_8bit_model.csv'\")\nprint(df_stats_8bit.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:43:47.599122Z","iopub.execute_input":"2025-06-21T10:43:47.599561Z","iopub.status.idle":"2025-06-21T10:44:24.694417Z","shell.execute_reply.started":"2025-06-21T10:43:47.599525Z","shell.execute_reply":"2025-06-21T10:44:24.692922Z"}},"outputs":[{"name":"stdout","text":"Loading model with 8-bit quantization...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e738d5a00f44480c954b46bee1873a57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c1f3e3aab234aa1bd2423988d7925b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe2fd60fb991401199f18a33242b24af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5390a00ffcb417c80de6062438ac01a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39fc92dabb4848e5a36874408f4519a8"}},"metadata":{}},{"name":"stderr","text":"2025-06-21 10:44:04.984113: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750502645.209175      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750502645.275127      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0232bc159fa34a648fd8a90aba07dbc9"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"926c7b7a231a4a139988bfc79eebf129"}},"metadata":{}},{"name":"stdout","text":"\n--- Model Loaded with 8-Bit Quantization ---\n\nVerifying a layer in the 8-bit quantized model:\nLinear8bitLt(in_features=896, out_features=4864, bias=False)\n\n--- Performing Statistical Analysis of 8-Bit Model Layers ---\n\n8-bit model analysis saved to 'statistical_analysis_8bit_model.csv'\n                        Layer Name   Data Type  Number of Parameters  \\\n0  model.layers.0.self_attn.q_proj  torch.int8                802816   \n1  model.layers.0.self_attn.k_proj  torch.int8                114688   \n2  model.layers.0.self_attn.v_proj  torch.int8                114688   \n3  model.layers.0.self_attn.o_proj  torch.int8                802816   \n4     model.layers.0.mlp.gate_proj  torch.int8               4358144   \n\n       Mean    Std Dev    Min    Max  \n0 -0.028345  31.696020 -127.0  127.0  \n1  0.043937  37.080254 -127.0  127.0  \n2 -0.023141  33.155426 -127.0  127.0  \n3 -0.015533  30.619982 -127.0  127.0  \n4  0.039224  35.988605 -127.0  127.0  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# --- Constants and Configuration ---\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\n\n# --- 1. Load Model and Tokenizer with 4-Bit Quantization ---\nprint(\"Loading model with 4-bit quantization...\")\n\n# Configuration for 4-bit quantization\n# bnb_4bit_quant_type=\"nf4\" specifies the NormalFloat4 quantization type.\n# bnb_4bit_compute_dtype=torch.float16 sets the compute dtype for faster operations.\nquantization_config_4bit = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Load the model with the specified quantization config\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    quantization_config=quantization_config_4bit,\n    device_map=\"auto\",\n)\n\nprint(\"\\n--- Model Loaded with 4-Bit Quantization ---\")\n\n# --- 2. Verify Model Layers ---\n# The Linear layers are now bnb.nn.Linear4bit objects.\nprint(\"\\nVerifying a layer in the 4-bit quantized model:\")\nprint(model_4bit.model.layers[0].mlp.gate_proj)\n\n# --- 3. Statistical Analysis of the 4-Bit Model ---\nimport pandas as pd\n\nprint(\"\\n--- Performing Statistical Analysis of 4-Bit Model Layers ---\")\n\nlayer_stats_4bit = []\nfor name, module in model_4bit.named_modules():\n    # Check for bitsandbytes quantized linear layers\n    if isinstance(module, torch.nn.Module) and 'Linear' in str(type(module)):\n         if hasattr(module, 'weight'):\n            with torch.no_grad():\n                weights = module.weight.data\n                num_params = weights.numel()\n\n                mean_val = weights.to(torch.float32).mean().item()\n                std_val = weights.to(torch.float32).std().item()\n                min_val = weights.min().item()\n                max_val = weights.max().item()\n\n                layer_stats_4bit.append({\n                    'Layer Name': name,\n                    'Data Type': weights.dtype, # Note: This shows compute dtype\n                    'Number of Parameters': num_params,\n                    'Mean': mean_val,\n                    'Std Dev': std_val,\n                    'Min': min_val,\n                    'Max': max_val\n                })\n\ndf_stats_4bit = pd.DataFrame(layer_stats_4bit)\ndf_stats_4bit.to_csv(\"statistical_analysis_4bit_model.csv\", index=False)\nprint(\"\\n4-bit model analysis saved to 'statistical_analysis_4bit_model.csv'\")\nprint(df_stats_4bit.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:44:53.694422Z","iopub.execute_input":"2025-06-21T10:44:53.695006Z","iopub.status.idle":"2025-06-21T10:44:56.013461Z","shell.execute_reply.started":"2025-06-21T10:44:53.694980Z","shell.execute_reply":"2025-06-21T10:44:56.012792Z"}},"outputs":[{"name":"stdout","text":"Loading model with 4-bit quantization...\n\n--- Model Loaded with 4-Bit Quantization ---\n\nVerifying a layer in the 4-bit quantized model:\nLinear4bit(in_features=896, out_features=4864, bias=False)\n\n--- Performing Statistical Analysis of 4-Bit Model Layers ---\n\n4-bit model analysis saved to 'statistical_analysis_4bit_model.csv'\n                        Layer Name    Data Type  Number of Parameters  \\\n0  model.layers.0.self_attn.q_proj  torch.uint8                401408   \n1  model.layers.0.self_attn.k_proj  torch.uint8                 57344   \n2  model.layers.0.self_attn.v_proj  torch.uint8                 57344   \n3  model.layers.0.self_attn.o_proj  torch.uint8                401408   \n4     model.layers.0.mlp.gate_proj  torch.uint8               2179072   \n\n         Mean    Std Dev  Min    Max  \n0  122.449493  57.013077  0.0  255.0  \n1  122.912239  60.789528  0.0  255.0  \n2  122.284431  57.163578  0.0  255.0  \n3  122.530807  58.723915  0.0  255.0  \n4  123.089676  60.116184  0.0  255.0  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# --- 1. Load Model with 8-Bit Quantization ---\nprint(\"Loading model with 8-bit quantization...\")\n\n# Define the configuration for 8-bit quantization\nquantization_config_8bit = BitsAndBytesConfig(load_in_8bit=True)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Load the model with the specified 8-bit quantization config\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    quantization_config=quantization_config_8bit,\n    device_map=\"auto\",\n)\nprint(\"8-bit quantized model and tokenizer loaded successfully.\")\n\n# --- 2. Perplexity Evaluation Function (from your notebook) ---\ndef evaluate_perplexity(model, tokenizer, dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"test\"):\n    \"\"\"\n    Evaluates the perplexity of a model on a given dataset using a sliding window approach.\n    \"\"\"\n    print(\"\\nEvaluating perplexity with sliding window...\")\n    # This requires the 'datasets' library: pip install datasets\n    test_dataset = load_dataset(dataset, subset, split=split)\n    encodings = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n\n    max_length = 2048  # Max length for each chunk\n    stride = 512     # Sliding window stride\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Evaluating Chunks\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        if input_ids.size(1) < 2:\n            continue\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            # The loss is the negative log likelihood\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    # Calculate perplexity as the exponential of the mean negative log likelihood\n    perplexity = torch.exp(torch.stack(nlls).mean())\n    return perplexity.item()\n\n\n# --- 3. Run the Evaluation ---\nprint(\"\\n--- Evaluating performance of the 8-bit quantized model ---\")\nperplexity_8bit = evaluate_perplexity(model_8bit, tokenizer)\n\n\n# --- 4. Display the Final Result ---\nprint(\"\\n--- Performance Evaluation Complete ---\")\nprint(f\"Perplexity of the 8-bit quantized model on WikiText: {perplexity_8bit:.4f}\")\n\n# For comparison, the baseline perplexity from your original notebook was 12.5985\nprint(f\"Baseline perplexity (original float16 model): 12.5985\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:45:03.408517Z","iopub.execute_input":"2025-06-21T10:45:03.408834Z","iopub.status.idle":"2025-06-21T10:48:18.114025Z","shell.execute_reply.started":"2025-06-21T10:45:03.408810Z","shell.execute_reply":"2025-06-21T10:48:18.113261Z"}},"outputs":[{"name":"stdout","text":"Loading model with 8-bit quantization...\n8-bit quantized model and tokenizer loaded successfully.\n\n--- Evaluating performance of the 8-bit quantized model ---\n\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6144edc0ba148238499d51590987175"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd6e7be1ef7444c87415f55ba77c6b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acff60c868e349a9960dfb7e74745de8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92b9f38628cf4a23a9b4d2c6666ebb05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58f0c216b27b473ca0ecd0e03b407eb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceb90ea2ffae4061b075da05195e62bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e41ad341ab3d4c6c998de0f50f073e75"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 32768). Running this sequence through the model will result in indexing errors\nEvaluating Chunks:  99%|█████████▉| 581/585 [03:06<00:01,  3.12it/s]","output_type":"stream"},{"name":"stdout","text":"\n--- Performance Evaluation Complete ---\nPerplexity of the 8-bit quantized model on WikiText: 12.6522\nBaseline perplexity (original float16 model): 12.5985\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# --- 1. Load Model with 4-Bit Quantization ---\nprint(\"Loading model with 4-bit quantization...\")\n\n# Configuration for 4-bit quantization\nquantization_config_4bit = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Load the model with the specified 4-bit quantization config\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    quantization_config=quantization_config_4bit,\n    device_map=\"auto\",\n)\nprint(\"4-bit quantized model and tokenizer loaded successfully.\")\n\n# --- 2. Perplexity Evaluation Function ---\ndef evaluate_perplexity(model, tokenizer, dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"test\"):\n    \"\"\"\n    Evaluates the perplexity of a model on a given dataset using a sliding window approach.\n    \"\"\"\n    print(\"\\nEvaluating perplexity with sliding window...\")\n    test_dataset = load_dataset(dataset, subset, split=split)\n    encodings = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n\n    max_length = 2048\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Evaluating Chunks\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        if input_ids.size(1) < 2:\n            continue\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    perplexity = torch.exp(torch.stack(nlls).mean())\n    return perplexity.item()\n\n# --- 3. Run the Evaluation ---\nprint(\"\\n--- Evaluating performance of the 4-bit quantized model ---\")\nperplexity_4bit = evaluate_perplexity(model_4bit, tokenizer)\n\n# --- 4. Display the Final Result ---\nprint(\"\\n--- Performance Evaluation Complete ---\")\nprint(f\"Perplexity of the 4-bit quantized model on WikiText: {perplexity_4bit:.4f}\")\nprint(f\"Baseline perplexity (original float16 model): 12.5985\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:49:16.862481Z","iopub.execute_input":"2025-06-21T10:49:16.863460Z","iopub.status.idle":"2025-06-21T10:51:13.070494Z","shell.execute_reply.started":"2025-06-21T10:49:16.863432Z","shell.execute_reply":"2025-06-21T10:51:13.069811Z"}},"outputs":[{"name":"stdout","text":"Loading model with 4-bit quantization...\n4-bit quantized model and tokenizer loaded successfully.\n\n--- Evaluating performance of the 4-bit quantized model ---\n\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 32768). Running this sequence through the model will result in indexing errors\nEvaluating Chunks:  99%|█████████▉| 581/585 [01:50<00:00,  5.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Performance Evaluation Complete ---\nPerplexity of the 4-bit quantized model on WikiText: 13.7227\nBaseline perplexity (original float16 model): 12.5985\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\npruned_model_save_dir = \"./qwen2_mixed_pruned\" # Directory for the saved model\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# =============================================================================\n# STEP 1: Load the original model and identify layer groups\n# =============================================================================\nprint(f\"--- Step 1: Loading original model for pruning ---\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(\"\\nIdentifying layer groups for mixed-sparsity pruning...\")\n# Get a dictionary of all named modules\nall_modules = dict(model.named_modules())\n\n# Group 1: The two largest layers (embedding and language model head)\ngroup1_names = ['model.embed_tokens', 'lm_head']\ngroup1_modules = [all_modules[name] for name in group1_names if name in all_modules]\n\n# Group 2: All other Linear layers (excluding the ones in Group 1)\ngroup2_modules = []\nfor name, module in all_modules.items():\n    if isinstance(module, torch.nn.Linear) and name not in group1_names:\n        group2_modules.append(module)\n\nprint(f\"Group 1 (for 50% pruning): {len(group1_modules)} layers ({group1_names})\")\nprint(f\"Group 2 (for 25% pruning): {len(group2_modules)} other Linear layers\")\n\n\n# =============================================================================\n# STEP 2: Apply mixed-sparsity pruning\n# =============================================================================\nprint(\"\\n--- Step 2: Applying mixed-sparsity pruning ---\")\n# Prune Group 1 layers at 50% sparsity\nprint(\"Pruning Group 1 at 50% sparsity...\")\nfor module in group1_modules:\n    prune.l1_unstructured(module, name=\"weight\", amount=0.50)\n\n# Prune Group 2 layers at 25% sparsity\nprint(\"Pruning Group 2 at 25% sparsity...\")\nfor module in group2_modules:\n    prune.l1_unstructured(module, name=\"weight\", amount=0.25)\n\n# Make the pruning permanent for all pruned layers\nprint(\"Making pruning permanent...\")\nall_pruned_modules = group1_modules + group2_modules\nfor module in all_pruned_modules:\n    prune.remove(module, 'weight')\n\nprint(\"Mixed-sparsity pruning complete.\")\n\n# =============================================================================\n# STEP 3: Save the pruned model\n# =============================================================================\nprint(f\"\\n--- Step 3: Saving the pruned model to {pruned_model_save_dir} ---\")\nmodel.save_pretrained(pruned_model_save_dir)\ntokenizer.save_pretrained(pruned_model_save_dir)\nprint(\"Pruned model saved successfully.\")\n\n# Clean up memory\ndel model\ntorch.cuda.empty_cache()\n\n# =============================================================================\n# STEP 4: Load the pruned model with 4-bit quantization\n# =============================================================================\nprint(f\"\\n--- Step 4: Loading the pruned model with 4-bit quantization ---\")\nquantization_config_4bit = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nfinal_model = AutoModelForCausalLM.from_pretrained(\n    pruned_model_save_dir,\n    quantization_config=quantization_config_4bit,\n    device_map=\"auto\",\n)\nprint(\"Final pruned and quantized model loaded successfully.\")\n\n\n# =============================================================================\n# STEP 5: Evaluate the performance of the final model\n# =============================================================================\ndef evaluate_perplexity(model, tokenizer, dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"test\"):\n    print(\"\\nEvaluating perplexity with sliding window...\")\n    test_dataset = load_dataset(dataset, subset, split=split)\n    encodings = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n    max_length = 2048\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Evaluating Chunks\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n        if input_ids.size(1) < 2: continue\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len: break\n    perplexity = torch.exp(torch.stack(nlls).mean())\n    return perplexity.item()\n\nprint(\"\\n--- Step 5: Evaluating performance of the final model ---\")\nfinal_perplexity = evaluate_perplexity(final_model, tokenizer)\n\nprint(\"\\n--- Final Performance Result ---\")\nprint(f\"Perplexity of the mixed-pruned and 4-bit quantized model: {final_perplexity:.4f}\")\nprint(f\"Baseline perplexity (original float16 model): 12.5985\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:52:28.913507Z","iopub.execute_input":"2025-06-21T10:52:28.914306Z","iopub.status.idle":"2025-06-21T10:54:27.649724Z","shell.execute_reply.started":"2025-06-21T10:52:28.914279Z","shell.execute_reply":"2025-06-21T10:54:27.648905Z"}},"outputs":[{"name":"stdout","text":"--- Step 1: Loading original model for pruning ---\n\nIdentifying layer groups for mixed-sparsity pruning...\nGroup 1 (for 50% pruning): 2 layers (['model.embed_tokens', 'lm_head'])\nGroup 2 (for 25% pruning): 168 other Linear layers\n\n--- Step 2: Applying mixed-sparsity pruning ---\nPruning Group 1 at 50% sparsity...\nPruning Group 2 at 25% sparsity...\nMaking pruning permanent...\nMixed-sparsity pruning complete.\n\n--- Step 3: Saving the pruned model to ./qwen2_mixed_pruned ---\nPruned model saved successfully.\n\n--- Step 4: Loading the pruned model with 4-bit quantization ---\nFinal pruned and quantized model loaded successfully.\n\n--- Step 5: Evaluating performance of the final model ---\n\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 32768). Running this sequence through the model will result in indexing errors\nEvaluating Chunks:  99%|█████████▉| 581/585 [01:48<00:00,  5.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Final Performance Result ---\nPerplexity of the mixed-pruned and 4-bit quantized model: 24.3156\nBaseline perplexity (original float16 model): 12.5985\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\npruned_model_save_dir = \"./qwen2_mixed_pruned_4bit\"\noutput_stats_filename = \"mixed_pruned_4bit_layer_stats.csv\"\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# =============================================================================\n# STEP 1: Load the original model and identify layer groups\n# =============================================================================\nprint(f\"--- Step 1: Loading original model for pruning ---\")\n# Added trust_remote_code=True to prevent model loading errors\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(\"\\nIdentifying layer groups for mixed-sparsity pruning...\")\nall_modules = dict(model.named_modules())\n\n# Group 1: The two largest layers (embedding and language model head)\ngroup1_names = ['model.embed_tokens', 'lm_head']\ngroup1_modules = [all_modules[name] for name in group1_names if name in all_modules]\n\n# Group 2: All other Linear layers\ngroup2_modules = []\nfor name, module in all_modules.items():\n    if isinstance(module, torch.nn.Linear) and name not in group1_names:\n        group2_modules.append(module)\n\nprint(f\"Group 1 (for 50% pruning): {len(group1_modules)} layers ({group1_names})\")\nprint(f\"Group 2 (for 25% pruning): {len(group2_modules)} other Linear layers\")\n\n\n# =============================================================================\n# STEP 2: Apply mixed-sparsity pruning\n# =============================================================================\nprint(\"\\n--- Step 2: Applying mixed-sparsity pruning ---\")\n# Prune Group 1 layers at 50% sparsity\nprint(\"Pruning Group 1 at 50% sparsity...\")\nfor module in group1_modules:\n    prune.l1_unstructured(module, name=\"weight\", amount=0.50)\n\n# Prune Group 2 layers at 25% sparsity\nprint(\"Pruning Group 2 at 25% sparsity...\")\nfor module in group2_modules:\n    prune.l1_unstructured(module, name=\"weight\", amount=0.25)\n\n# Make the pruning permanent\nprint(\"\\nMaking pruning permanent...\")\nall_pruned_modules = group1_modules + group2_modules\nfor module in all_pruned_modules:\n    prune.remove(module, 'weight')\n\nprint(\"Mixed-sparsity pruning complete.\")\n\n# =============================================================================\n# STEP 3: Save the pruned model\n# =============================================================================\nprint(f\"\\n--- Step 3: Saving the pruned model to {pruned_model_save_dir} ---\")\nmodel.save_pretrained(pruned_model_save_dir)\ntokenizer.save_pretrained(pruned_model_save_dir)\nprint(\"Pruned model saved successfully.\")\n\n# Clean up memory\ndel model\ntorch.cuda.empty_cache()\n\n# =============================================================================\n# STEP 4: Load the pruned model with 4-bit quantization\n# =============================================================================\nprint(f\"\\n--- Step 4: Loading the pruned model with 4-bit quantization ---\")\nquantization_config_4bit = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nfinal_model = AutoModelForCausalLM.from_pretrained(\n    pruned_model_save_dir,\n    quantization_config=quantization_config_4bit,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nprint(\"Final pruned and quantized model loaded successfully.\")\n\n\n# =============================================================================\n# STEP 5: (Optional) Evaluate the performance of the final model\n# =============================================================================\n# Skipping perplexity evaluation to focus on statistical analysis as requested.\n# You can uncomment this section to run it again.\n#\n# def evaluate_perplexity(model, tokenizer, dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"test\"):\n#     ... (full function code) ...\n#\n# print(\"\\n--- Step 5: Evaluating performance of the final model ---\")\n# final_perplexity = evaluate_perplexity(final_model, tokenizer)\n# print(f\"Perplexity: {final_perplexity:.4f}\")\n\n\n# =============================================================================\n# STEP 6: Perform and Save Layer-wise Statistical Analysis\n# =============================================================================\ndef generate_layer_wise_stats(model, model_description=\"model\"):\n    \"\"\"\n    Analyzes each layer in a model and returns a DataFrame with its stats.\n    \"\"\"\n    print(f\"\\n--- Generating Layer-wise Statistical Analysis for: {model_description} ---\")\n    layer_stats = []\n    for name, module in model.named_modules():\n        # Check for layers that have a weight parameter\n        if hasattr(module, \"weight\") and isinstance(module.weight, torch.nn.Parameter):\n            with torch.no_grad():\n                weights = module.weight.data\n                \n                # Calculate the actual sparsity of the layer\n                non_zero_elements = torch.count_nonzero(weights).item()\n                total_elements = weights.numel()\n                # Use standard Python division, which works correctly on integers\n                actual_sparsity = 1.0 - (non_zero_elements / total_elements)\n\n                layer_stats.append({\n                    'Layer Name': name,\n                    'Data Type': weights.dtype,\n                    'Number of Params': total_elements,\n                    'Actual Sparsity': f\"{actual_sparsity:.2%}\",\n                    'Weight Mean': weights.to(torch.float32).mean().item(),\n                    'Weight Std Dev': weights.to(torch.float32).std().item(),\n                    'Weight Min': weights.min().item(),\n                    'Weight Max': weights.max().item(),\n                })\n    \n    return pd.DataFrame(layer_stats)\n\n# Generate the stats for the final model\nfinal_results_df = generate_layer_wise_stats(final_model, \"Mixed-Pruned + 4-bit Model\")\n\n# Save the results to a CSV file\nfinal_results_df.to_csv(output_stats_filename, index=False)\nprint(f\"\\nLayer-wise statistical analysis saved to '{output_stats_filename}'\")\n\n# Display the full results table\nprint(\"\\n--- Layer-wise Results ---\")\nprint(final_results_df.to_string())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T11:58:25.782576Z","iopub.execute_input":"2025-06-21T11:58:25.783172Z","iopub.status.idle":"2025-06-21T11:58:31.349499Z","shell.execute_reply.started":"2025-06-21T11:58:25.783151Z","shell.execute_reply":"2025-06-21T11:58:31.348645Z"}},"outputs":[{"name":"stdout","text":"--- Step 1: Loading original model for pruning ---\n\nIdentifying layer groups for mixed-sparsity pruning...\nGroup 1 (for 50% pruning): 2 layers (['model.embed_tokens', 'lm_head'])\nGroup 2 (for 25% pruning): 168 other Linear layers\n\n--- Step 2: Applying mixed-sparsity pruning ---\nPruning Group 1 at 50% sparsity...\nPruning Group 2 at 25% sparsity...\n\nMaking pruning permanent...\nMixed-sparsity pruning complete.\n\n--- Step 3: Saving the pruned model to ./qwen2_mixed_pruned_4bit ---\nPruned model saved successfully.\n\n--- Step 4: Loading the pruned model with 4-bit quantization ---\nFinal pruned and quantized model loaded successfully.\n\n--- Generating Layer-wise Statistical Analysis for: Mixed-Pruned + 4-bit Model ---\n\nLayer-wise statistical analysis saved to 'mixed_pruned_4bit_layer_stats.csv'\n\n--- Layer-wise Results ---\n                                   Layer Name      Data Type  Number of Params Actual Sparsity  Weight Mean  Weight Std Dev    Weight Min  Weight Max\n0                          model.embed_tokens  torch.float16         136134656          50.00%     0.000132        0.014714 -1.962891e-01    0.127930\n1             model.layers.0.self_attn.q_proj    torch.uint8            401408           0.02%   122.220657       56.265343  0.000000e+00  255.000000\n2             model.layers.0.self_attn.k_proj    torch.uint8             57344           0.03%   122.614105       59.949986  0.000000e+00  255.000000\n3             model.layers.0.self_attn.v_proj    torch.uint8             57344           0.03%   122.049446       56.787735  0.000000e+00  255.000000\n4             model.layers.0.self_attn.o_proj    torch.uint8            401408           0.03%   122.296555       58.033184  0.000000e+00  255.000000\n5                model.layers.0.mlp.gate_proj    torch.uint8           2179072           0.03%   122.869873       59.682205  0.000000e+00  255.000000\n6                  model.layers.0.mlp.up_proj    torch.uint8           2179072           0.03%   122.310295       59.191956  0.000000e+00  255.000000\n7                model.layers.0.mlp.down_proj    torch.uint8           2179072           0.03%   122.252281       58.007149  0.000000e+00  255.000000\n8              model.layers.0.input_layernorm  torch.float16               896           0.00%     0.033067        0.125686 -6.250000e-01    0.960938\n9     model.layers.0.post_attention_layernorm  torch.float16               896           0.00%     0.722957        0.140165  4.257812e-01    1.820312\n10            model.layers.1.self_attn.q_proj    torch.uint8            401408           0.03%   121.908501       56.510056  0.000000e+00  255.000000\n11            model.layers.1.self_attn.k_proj    torch.uint8             57344           0.03%   122.206238       57.786552  0.000000e+00  255.000000\n12            model.layers.1.self_attn.v_proj    torch.uint8             57344           0.02%   121.858894       53.330147  0.000000e+00  255.000000\n13            model.layers.1.self_attn.o_proj    torch.uint8            401408           0.04%   122.225464       59.198841  0.000000e+00  255.000000\n14               model.layers.1.mlp.gate_proj    torch.uint8           2179072           0.03%   122.413025       59.443989  0.000000e+00  255.000000\n15                 model.layers.1.mlp.up_proj    torch.uint8           2179072           0.03%   122.402443       59.126907  0.000000e+00  255.000000\n16               model.layers.1.mlp.down_proj    torch.uint8           2179072           0.03%   122.334267       59.050308  0.000000e+00  255.000000\n17             model.layers.1.input_layernorm  torch.float16               896           0.00%     0.797259        0.444815 -5.859375e-02    3.843750\n18    model.layers.1.post_attention_layernorm  torch.float16               896           0.00%     1.125865        0.291850  1.585484e-05    3.203125\n19            model.layers.2.self_attn.q_proj    torch.uint8            401408           0.03%   122.245415       58.456745  0.000000e+00  255.000000\n20            model.layers.2.self_attn.k_proj    torch.uint8             57344           0.03%   122.166321       58.577198  0.000000e+00  255.000000\n21            model.layers.2.self_attn.v_proj    torch.uint8             57344           0.01%   122.227699       55.838238  0.000000e+00  255.000000\n22            model.layers.2.self_attn.o_proj    torch.uint8            401408           0.03%   122.490578       59.475323  0.000000e+00  255.000000\n23               model.layers.2.mlp.gate_proj    torch.uint8           2179072           0.03%   122.190536       59.236118  0.000000e+00  255.000000\n24                 model.layers.2.mlp.up_proj    torch.uint8           2179072           0.03%   122.393661       59.035828  0.000000e+00  255.000000\n25               model.layers.2.mlp.down_proj    torch.uint8           2179072           0.03%   122.291267       58.613873  0.000000e+00  255.000000\n26             model.layers.2.input_layernorm  torch.float16               896           0.00%     0.921201        0.415566 -1.515625e+00    3.828125\n27    model.layers.2.post_attention_layernorm  torch.float16               896           0.00%     1.238739        0.299352 -2.443790e-06    3.156250\n28            model.layers.3.self_attn.q_proj    torch.uint8            401408           0.03%   122.020081       58.167694  0.000000e+00  255.000000\n29            model.layers.3.self_attn.k_proj    torch.uint8             57344           0.03%   121.888382       58.198505  0.000000e+00  255.000000\n30            model.layers.3.self_attn.v_proj    torch.uint8             57344           0.03%   121.877480       57.863121  0.000000e+00  255.000000\n31            model.layers.3.self_attn.o_proj    torch.uint8            401408           0.03%   122.424095       59.498119  0.000000e+00  255.000000\n32               model.layers.3.mlp.gate_proj    torch.uint8           2179072           0.03%   122.230240       59.341621  0.000000e+00  255.000000\n33                 model.layers.3.mlp.up_proj    torch.uint8           2179072           0.03%   122.311371       58.934456  0.000000e+00  255.000000\n34               model.layers.3.mlp.down_proj    torch.uint8           2179072           0.03%   122.273415       58.367397  0.000000e+00  255.000000\n35             model.layers.3.input_layernorm  torch.float16               896           0.00%     1.091552        0.287205 -9.921875e-01    3.593750\n36    model.layers.3.post_attention_layernorm  torch.float16               896           0.00%     1.345123        0.313676  4.553795e-05    3.140625\n37            model.layers.4.self_attn.q_proj    torch.uint8            401408           0.03%   122.211136       57.827000  0.000000e+00  255.000000\n38            model.layers.4.self_attn.k_proj    torch.uint8             57344           0.02%   122.259911       56.859016  0.000000e+00  255.000000\n39            model.layers.4.self_attn.v_proj    torch.uint8             57344           0.03%   122.305756       57.389824  0.000000e+00  255.000000\n40            model.layers.4.self_attn.o_proj    torch.uint8            401408           0.03%   122.364136       59.310688  0.000000e+00  255.000000\n41               model.layers.4.mlp.gate_proj    torch.uint8           2179072           0.03%   122.128296       58.602444  0.000000e+00  255.000000\n42                 model.layers.4.mlp.up_proj    torch.uint8           2179072           0.03%   122.328903       58.399178  0.000000e+00  255.000000\n43               model.layers.4.mlp.down_proj    torch.uint8           2179072           0.03%   122.231400       57.400455  0.000000e+00  255.000000\n44             model.layers.4.input_layernorm  torch.float16               896           0.00%     0.959832        0.339886  5.004883e-02    3.296875\n45    model.layers.4.post_attention_layernorm  torch.float16               896           0.00%     1.266706        0.343240 -1.764297e-04    7.093750\n46            model.layers.5.self_attn.q_proj    torch.uint8            401408           0.03%   122.051094       56.544472  0.000000e+00  255.000000\n47            model.layers.5.self_attn.k_proj    torch.uint8             57344           0.02%   121.854309       56.210461  0.000000e+00  255.000000\n48            model.layers.5.self_attn.v_proj    torch.uint8             57344           0.02%   122.386932       57.971573  0.000000e+00  255.000000\n49            model.layers.5.self_attn.o_proj    torch.uint8            401408           0.04%   122.352837       58.637459  0.000000e+00  255.000000\n50               model.layers.5.mlp.gate_proj    torch.uint8           2179072           0.03%   122.434906       59.286404  0.000000e+00  255.000000\n51                 model.layers.5.mlp.up_proj    torch.uint8           2179072           0.03%   122.382568       58.869900  0.000000e+00  255.000000\n52               model.layers.5.mlp.down_proj    torch.uint8           2179072           0.03%   122.382721       57.944683  0.000000e+00  255.000000\n53             model.layers.5.input_layernorm  torch.float16               896           0.00%     1.123697        0.402148 -1.257812e+00    3.937500\n54    model.layers.5.post_attention_layernorm  torch.float16               896           0.00%     1.297313        0.362451 -5.297852e-02    2.953125\n55            model.layers.6.self_attn.q_proj    torch.uint8            401408           0.02%   122.147980       58.721809  0.000000e+00  255.000000\n56            model.layers.6.self_attn.k_proj    torch.uint8             57344           0.03%   122.431175       58.182484  0.000000e+00  255.000000\n57            model.layers.6.self_attn.v_proj    torch.uint8             57344           0.03%   122.512421       56.216480  0.000000e+00  255.000000\n58            model.layers.6.self_attn.o_proj    torch.uint8            401408           0.04%   122.353653       59.303425  0.000000e+00  255.000000\n59               model.layers.6.mlp.gate_proj    torch.uint8           2179072           0.03%   122.281456       59.176517  0.000000e+00  255.000000\n60                 model.layers.6.mlp.up_proj    torch.uint8           2179072           0.03%   122.317413       58.772457  0.000000e+00  255.000000\n61               model.layers.6.mlp.down_proj    torch.uint8           2179072           0.03%   122.239792       57.986427  0.000000e+00  255.000000\n62             model.layers.6.input_layernorm  torch.float16               896           0.00%     1.148009        0.431344 -8.281250e-01    3.968750\n63    model.layers.6.post_attention_layernorm  torch.float16               896           0.00%     1.323709        0.409137  1.068115e-04    3.781250\n64            model.layers.7.self_attn.q_proj    torch.uint8            401408           0.03%   122.262474       58.322216  0.000000e+00  255.000000\n65            model.layers.7.self_attn.k_proj    torch.uint8             57344           0.02%   122.562805       57.871204  0.000000e+00  255.000000\n66            model.layers.7.self_attn.v_proj    torch.uint8             57344           0.01%   122.284096       56.761108  0.000000e+00  255.000000\n67            model.layers.7.self_attn.o_proj    torch.uint8            401408           0.03%   122.212189       59.374001  0.000000e+00  255.000000\n68               model.layers.7.mlp.gate_proj    torch.uint8           2179072           0.03%   122.253525       58.992966  0.000000e+00  255.000000\n69                 model.layers.7.mlp.up_proj    torch.uint8           2179072           0.03%   122.383995       58.640244  0.000000e+00  255.000000\n70               model.layers.7.mlp.down_proj    torch.uint8           2179072           0.02%   122.259789       57.410419  0.000000e+00  255.000000\n71             model.layers.7.input_layernorm  torch.float16               896           0.00%     1.319497        0.504894 -1.132812e+00    3.890625\n72    model.layers.7.post_attention_layernorm  torch.float16               896           0.00%     1.421060        0.476248 -2.789497e-05    3.593750\n73            model.layers.8.self_attn.q_proj    torch.uint8            401408           0.03%   122.066483       57.903713  0.000000e+00  255.000000\n74            model.layers.8.self_attn.k_proj    torch.uint8             57344           0.03%   122.307167       58.134487  0.000000e+00  255.000000\n75            model.layers.8.self_attn.v_proj    torch.uint8             57344           0.02%   121.408485       53.601402  0.000000e+00  255.000000\n76            model.layers.8.self_attn.o_proj    torch.uint8            401408           0.03%   122.260422       58.349045  0.000000e+00  255.000000\n77               model.layers.8.mlp.gate_proj    torch.uint8           2179072           0.03%   122.099228       58.169521  0.000000e+00  255.000000\n78                 model.layers.8.mlp.up_proj    torch.uint8           2179072           0.03%   122.252106       57.841953  0.000000e+00  255.000000\n79               model.layers.8.mlp.down_proj    torch.uint8           2179072           0.02%   122.164726       56.867161  0.000000e+00  255.000000\n80             model.layers.8.input_layernorm  torch.float16               896           0.00%     1.069055        0.557704 -1.162109e-01    3.265625\n81    model.layers.8.post_attention_layernorm  torch.float16               896           0.00%     1.269379        0.485914  2.578125e-01    3.531250\n82            model.layers.9.self_attn.q_proj    torch.uint8            401408           0.03%   122.133804       55.640400  0.000000e+00  255.000000\n83            model.layers.9.self_attn.k_proj    torch.uint8             57344           0.03%   121.792679       54.119263  0.000000e+00  255.000000\n84            model.layers.9.self_attn.v_proj    torch.uint8             57344           0.02%   122.308289       56.618584  0.000000e+00  255.000000\n85            model.layers.9.self_attn.o_proj    torch.uint8            401408           0.03%   122.326073       58.232246  0.000000e+00  255.000000\n86               model.layers.9.mlp.gate_proj    torch.uint8           2179072           0.02%   122.103416       57.938190  0.000000e+00  255.000000\n87                 model.layers.9.mlp.up_proj    torch.uint8           2179072           0.03%   122.218399       57.562435  0.000000e+00  255.000000\n88               model.layers.9.mlp.down_proj    torch.uint8           2179072           0.02%   122.139709       56.418808  0.000000e+00  255.000000\n89             model.layers.9.input_layernorm  torch.float16               896           0.00%     1.568166        0.972377 -8.046875e-01   10.750000\n90    model.layers.9.post_attention_layernorm  torch.float16               896           0.00%     1.362994        0.577671  5.960464e-08    3.203125\n91           model.layers.10.self_attn.q_proj    torch.uint8            401408           0.03%   122.190178       57.248882  0.000000e+00  255.000000\n92           model.layers.10.self_attn.k_proj    torch.uint8             57344           0.03%   122.161644       57.115143  0.000000e+00  255.000000\n93           model.layers.10.self_attn.v_proj    torch.uint8             57344           0.02%   122.091415       54.699528  0.000000e+00  255.000000\n94           model.layers.10.self_attn.o_proj    torch.uint8            401408           0.03%   122.290543       58.971870  0.000000e+00  255.000000\n95              model.layers.10.mlp.gate_proj    torch.uint8           2179072           0.03%   122.054390       57.077686  0.000000e+00  255.000000\n96                model.layers.10.mlp.up_proj    torch.uint8           2179072           0.03%   122.212776       57.012821  0.000000e+00  255.000000\n97              model.layers.10.mlp.down_proj    torch.uint8           2179072           0.02%   122.071198       55.635406  0.000000e+00  255.000000\n98            model.layers.10.input_layernorm  torch.float16               896           0.00%     1.337198        0.957490 -1.117188e+00    4.531250\n99   model.layers.10.post_attention_layernorm  torch.float16               896           0.00%     1.315180        0.660076  2.412109e-01    3.515625\n100          model.layers.11.self_attn.q_proj    torch.uint8            401408           0.02%   122.145607       54.728493  0.000000e+00  255.000000\n101          model.layers.11.self_attn.k_proj    torch.uint8             57344           0.02%   122.158455       54.523674  0.000000e+00  255.000000\n102          model.layers.11.self_attn.v_proj    torch.uint8             57344           0.02%   121.636902       49.691071  0.000000e+00  255.000000\n103          model.layers.11.self_attn.o_proj    torch.uint8            401408           0.03%   122.445831       58.227970  0.000000e+00  255.000000\n104             model.layers.11.mlp.gate_proj    torch.uint8           2179072           0.02%   122.390434       57.287998  0.000000e+00  255.000000\n105               model.layers.11.mlp.up_proj    torch.uint8           2179072           0.02%   122.231789       57.376804  0.000000e+00  255.000000\n106             model.layers.11.mlp.down_proj    torch.uint8           2179072           0.02%   122.136223       56.126656  0.000000e+00  255.000000\n107           model.layers.11.input_layernorm  torch.float16               896           0.00%     1.796278        1.100070 -3.955078e-02    7.375000\n108  model.layers.11.post_attention_layernorm  torch.float16               896           0.00%     1.319809        0.492273 -4.386902e-05    2.656250\n109          model.layers.12.self_attn.q_proj    torch.uint8            401408           0.03%   122.145203       55.875256  0.000000e+00  255.000000\n110          model.layers.12.self_attn.k_proj    torch.uint8             57344           0.02%   121.892723       53.682529  0.000000e+00  255.000000\n111          model.layers.12.self_attn.v_proj    torch.uint8             57344           0.03%   122.397133       55.121410  0.000000e+00  255.000000\n112          model.layers.12.self_attn.o_proj    torch.uint8            401408           0.03%   122.192749       58.840221  0.000000e+00  255.000000\n113             model.layers.12.mlp.gate_proj    torch.uint8           2179072           0.03%   122.310715       58.067898  0.000000e+00  255.000000\n114               model.layers.12.mlp.up_proj    torch.uint8           2179072           0.03%   122.283409       57.879833  0.000000e+00  255.000000\n115             model.layers.12.mlp.down_proj    torch.uint8           2179072           0.03%   122.273384       57.262596  0.000000e+00  255.000000\n116           model.layers.12.input_layernorm  torch.float16               896           0.00%     1.513124        0.614301 -9.062500e-01    3.187500\n117  model.layers.12.post_attention_layernorm  torch.float16               896           0.00%     1.389511        0.493660 -1.554489e-04    2.531250\n118          model.layers.13.self_attn.q_proj    torch.uint8            401408           0.03%   122.368561       58.433102  0.000000e+00  255.000000\n119          model.layers.13.self_attn.k_proj    torch.uint8             57344           0.02%   122.045990       58.120861  0.000000e+00  255.000000\n120          model.layers.13.self_attn.v_proj    torch.uint8             57344           0.03%   122.867332       58.256943  0.000000e+00  255.000000\n121          model.layers.13.self_attn.o_proj    torch.uint8            401408           0.03%   122.265823       58.931610  0.000000e+00  255.000000\n122             model.layers.13.mlp.gate_proj    torch.uint8           2179072           0.03%   122.393074       58.288658  0.000000e+00  255.000000\n123               model.layers.13.mlp.up_proj    torch.uint8           2179072           0.03%   122.241722       58.262180  0.000000e+00  255.000000\n124             model.layers.13.mlp.down_proj    torch.uint8           2179072           0.03%   122.258270       57.355961  0.000000e+00  255.000000\n125           model.layers.13.input_layernorm  torch.float16               896           0.00%     1.486157        0.668099  9.130859e-02    3.359375\n126  model.layers.13.post_attention_layernorm  torch.float16               896           0.00%     1.372921        0.469418  2.460938e-01    2.390625\n127          model.layers.14.self_attn.q_proj    torch.uint8            401408           0.03%   122.193954       57.813919  0.000000e+00  255.000000\n128          model.layers.14.self_attn.k_proj    torch.uint8             57344           0.02%   122.350502       58.013039  0.000000e+00  255.000000\n129          model.layers.14.self_attn.v_proj    torch.uint8             57344           0.03%   122.328552       57.609802  0.000000e+00  255.000000\n130          model.layers.14.self_attn.o_proj    torch.uint8            401408           0.03%   122.280449       59.049252  0.000000e+00  255.000000\n131             model.layers.14.mlp.gate_proj    torch.uint8           2179072           0.03%   122.643906       58.102177  0.000000e+00  255.000000\n132               model.layers.14.mlp.up_proj    torch.uint8           2179072           0.03%   122.258842       58.027477  0.000000e+00  255.000000\n133             model.layers.14.mlp.down_proj    torch.uint8           2179072           0.03%   122.187103       57.420071  0.000000e+00  255.000000\n134           model.layers.14.input_layernorm  torch.float16               896           0.00%     1.626853        0.703336 -7.070312e-01    3.421875\n135  model.layers.14.post_attention_layernorm  torch.float16               896           0.00%     1.400580        0.434556 -2.746582e-02    2.343750\n136          model.layers.15.self_attn.q_proj    torch.uint8            401408           0.03%   122.098450       58.034855  0.000000e+00  255.000000\n137          model.layers.15.self_attn.k_proj    torch.uint8             57344           0.04%   122.376854       57.959278  0.000000e+00  255.000000\n138          model.layers.15.self_attn.v_proj    torch.uint8             57344           0.02%   122.437607       57.735954  0.000000e+00  255.000000\n139          model.layers.15.self_attn.o_proj    torch.uint8            401408           0.04%   122.395737       59.227272  0.000000e+00  255.000000\n140             model.layers.15.mlp.gate_proj    torch.uint8           2179072           0.03%   122.416840       57.245945  0.000000e+00  255.000000\n141               model.layers.15.mlp.up_proj    torch.uint8           2179072           0.03%   122.239075       57.720562  0.000000e+00  255.000000\n142             model.layers.15.mlp.down_proj    torch.uint8           2179072           0.03%   122.198898       56.535347  0.000000e+00  255.000000\n143           model.layers.15.input_layernorm  torch.float16               896           0.00%     1.425349        0.598486 -6.718750e-01    3.218750\n144  model.layers.15.post_attention_layernorm  torch.float16               896           0.00%     1.477057        0.390902  3.964844e-01    2.640625\n145          model.layers.16.self_attn.q_proj    torch.uint8            401408           0.02%   122.389069       56.641407  0.000000e+00  255.000000\n146          model.layers.16.self_attn.k_proj    torch.uint8             57344           0.02%   122.041824       56.783283  0.000000e+00  255.000000\n147          model.layers.16.self_attn.v_proj    torch.uint8             57344           0.02%   121.397186       47.094311  0.000000e+00  255.000000\n148          model.layers.16.self_attn.o_proj    torch.uint8            401408           0.04%   122.305290       59.651913  0.000000e+00  255.000000\n149             model.layers.16.mlp.gate_proj    torch.uint8           2179072           0.03%   121.960861       56.625095  0.000000e+00  255.000000\n150               model.layers.16.mlp.up_proj    torch.uint8           2179072           0.03%   122.205727       57.423542  0.000000e+00  255.000000\n151             model.layers.16.mlp.down_proj    torch.uint8           2179072           0.02%   122.248772       56.668983  0.000000e+00  255.000000\n152           model.layers.16.input_layernorm  torch.float16               896           0.00%     1.660565        0.565342 -9.101562e-01    3.562500\n153  model.layers.16.post_attention_layernorm  torch.float16               896           0.00%     1.578326        0.341396  4.335938e-01    3.515625\n154          model.layers.17.self_attn.q_proj    torch.uint8            401408           0.02%   122.168785       57.096775  0.000000e+00  255.000000\n155          model.layers.17.self_attn.k_proj    torch.uint8             57344           0.02%   122.398094       56.816917  0.000000e+00  255.000000\n156          model.layers.17.self_attn.v_proj    torch.uint8             57344           0.03%   121.947342       56.602386  0.000000e+00  255.000000\n157          model.layers.17.self_attn.o_proj    torch.uint8            401408           0.03%   122.336014       59.600018  0.000000e+00  255.000000\n158             model.layers.17.mlp.gate_proj    torch.uint8           2179072           0.03%   121.857269       58.149830  0.000000e+00  255.000000\n159               model.layers.17.mlp.up_proj    torch.uint8           2179072           0.03%   122.266731       58.506012  0.000000e+00  255.000000\n160             model.layers.17.mlp.down_proj    torch.uint8           2179072           0.03%   122.324013       58.196335  0.000000e+00  255.000000\n161           model.layers.17.input_layernorm  torch.float16               896           0.00%     1.571923        0.485495  8.544922e-02    3.390625\n162  model.layers.17.post_attention_layernorm  torch.float16               896           0.00%     1.730824        0.371812  3.398438e-01    3.234375\n163          model.layers.18.self_attn.q_proj    torch.uint8            401408           0.02%   122.279137       57.737949  0.000000e+00  255.000000\n164          model.layers.18.self_attn.k_proj    torch.uint8             57344           0.02%   122.230629       56.579205  0.000000e+00  255.000000\n165          model.layers.18.self_attn.v_proj    torch.uint8             57344           0.02%   122.065193       56.027279  0.000000e+00  255.000000\n166          model.layers.18.self_attn.o_proj    torch.uint8            401408           0.03%   122.351822       59.644081  0.000000e+00  255.000000\n167             model.layers.18.mlp.gate_proj    torch.uint8           2179072           0.03%   122.198135       59.295494  0.000000e+00  255.000000\n168               model.layers.18.mlp.up_proj    torch.uint8           2179072           0.03%   122.394806       59.296696  0.000000e+00  255.000000\n169             model.layers.18.mlp.down_proj    torch.uint8           2179072           0.03%   122.343513       59.225658  0.000000e+00  255.000000\n170           model.layers.18.input_layernorm  torch.float16               896           0.00%     1.589983        0.380777 -9.179688e-01    3.312500\n171  model.layers.18.post_attention_layernorm  torch.float16               896           0.00%     1.719568        0.396017  3.222656e-01    4.125000\n172          model.layers.19.self_attn.q_proj    torch.uint8            401408           0.03%   122.159164       58.266991  0.000000e+00  255.000000\n173          model.layers.19.self_attn.k_proj    torch.uint8             57344           0.04%   122.375839       57.490078  0.000000e+00  255.000000\n174          model.layers.19.self_attn.v_proj    torch.uint8             57344           0.03%   122.016785       57.444473  0.000000e+00  255.000000\n175          model.layers.19.self_attn.o_proj    torch.uint8            401408           0.03%   122.390015       59.271412  0.000000e+00  255.000000\n176             model.layers.19.mlp.gate_proj    torch.uint8           2179072           0.03%   122.163231       59.178902  0.000000e+00  255.000000\n177               model.layers.19.mlp.up_proj    torch.uint8           2179072           0.03%   122.413811       59.196251  0.000000e+00  255.000000\n178             model.layers.19.mlp.down_proj    torch.uint8           2179072           0.03%   122.323280       58.749893  0.000000e+00  255.000000\n179           model.layers.19.input_layernorm  torch.float16               896           0.00%     1.457768        0.332832 -3.789062e-01    3.609375\n180  model.layers.19.post_attention_layernorm  torch.float16               896           0.00%     1.908927        0.385458  5.273438e-01    4.625000\n181          model.layers.20.self_attn.q_proj    torch.uint8            401408           0.06%   122.082016       56.204208  0.000000e+00  255.000000\n182          model.layers.20.self_attn.k_proj    torch.uint8             57344           0.03%   122.330849       57.171627  0.000000e+00  255.000000\n183          model.layers.20.self_attn.v_proj    torch.uint8             57344           0.02%   122.041199       50.481293  0.000000e+00  255.000000\n184          model.layers.20.self_attn.o_proj    torch.uint8            401408           0.03%   122.347824       58.993313  0.000000e+00  255.000000\n185             model.layers.20.mlp.gate_proj    torch.uint8           2179072           0.03%   122.216331       58.417572  0.000000e+00  255.000000\n186               model.layers.20.mlp.up_proj    torch.uint8           2179072           0.03%   122.301178       58.460403  0.000000e+00  255.000000\n187             model.layers.20.mlp.down_proj    torch.uint8           2179072           0.03%   122.232124       58.010979  0.000000e+00  255.000000\n188           model.layers.20.input_layernorm  torch.float16               896           0.00%     1.516144        0.670899 -3.906250e-02    4.375000\n189  model.layers.20.post_attention_layernorm  torch.float16               896           0.00%     2.016449        0.307135  4.062500e-01    5.812500\n190          model.layers.21.self_attn.q_proj    torch.uint8            401408           0.03%   122.016579       54.928905  0.000000e+00  255.000000\n191          model.layers.21.self_attn.k_proj    torch.uint8             57344           0.02%   121.907967       54.629387  0.000000e+00  255.000000\n192          model.layers.21.self_attn.v_proj    torch.uint8             57344           0.01%   121.534309       47.517990  0.000000e+00  255.000000\n193          model.layers.21.self_attn.o_proj    torch.uint8            401408           0.03%   122.385269       59.946896  0.000000e+00  255.000000\n194             model.layers.21.mlp.gate_proj    torch.uint8           2179072           0.03%   122.399406       59.049625  0.000000e+00  255.000000\n195               model.layers.21.mlp.up_proj    torch.uint8           2179072           0.03%   122.421967       58.970020  0.000000e+00  255.000000\n196             model.layers.21.mlp.down_proj    torch.uint8           2179072           0.03%   122.351685       59.003159  0.000000e+00  255.000000\n197           model.layers.21.input_layernorm  torch.float16               896           0.00%     1.617219        1.366368  5.712891e-02    7.062500\n198  model.layers.21.post_attention_layernorm  torch.float16               896           0.00%     2.010075        0.280976 -2.803802e-04    5.187500\n199          model.layers.22.self_attn.q_proj    torch.uint8            401408           0.02%   121.929794       55.028900  0.000000e+00  255.000000\n200          model.layers.22.self_attn.k_proj    torch.uint8             57344           0.02%   121.468147       53.900204  0.000000e+00  255.000000\n201          model.layers.22.self_attn.v_proj    torch.uint8             57344           0.01%   121.465736       48.900101  0.000000e+00  255.000000\n202          model.layers.22.self_attn.o_proj    torch.uint8            401408           0.03%   122.461334       60.235569  0.000000e+00  255.000000\n203             model.layers.22.mlp.gate_proj    torch.uint8           2179072           0.03%   122.321831       58.945057  0.000000e+00  255.000000\n204               model.layers.22.mlp.up_proj    torch.uint8           2179072           0.03%   122.298279       58.957108  0.000000e+00  255.000000\n205             model.layers.22.mlp.down_proj    torch.uint8           2179072           0.03%   122.360077       58.752781  0.000000e+00  255.000000\n206           model.layers.22.input_layernorm  torch.float16               896           0.00%     1.654051        1.173776  1.582031e-01    6.468750\n207  model.layers.22.post_attention_layernorm  torch.float16               896           0.00%     2.134290        0.347131 -3.504753e-05    9.312500\n208          model.layers.23.self_attn.q_proj    torch.uint8            401408           0.02%   122.109940       56.181843  0.000000e+00  255.000000\n209          model.layers.23.self_attn.k_proj    torch.uint8             57344           0.03%   122.178040       55.180115  0.000000e+00  255.000000\n210          model.layers.23.self_attn.v_proj    torch.uint8             57344           0.02%   121.675682       49.881477  0.000000e+00  255.000000\n211          model.layers.23.self_attn.o_proj    torch.uint8            401408           0.03%   122.325493       58.512363  0.000000e+00  255.000000\n212             model.layers.23.mlp.gate_proj    torch.uint8           2179072           0.03%   122.870224       59.238125  0.000000e+00  255.000000\n213               model.layers.23.mlp.up_proj    torch.uint8           2179072           0.03%   122.285706       59.006996  0.000000e+00  255.000000\n214             model.layers.23.mlp.down_proj    torch.uint8           2179072           0.03%   122.206703       57.523571  0.000000e+00  255.000000\n215           model.layers.23.input_layernorm  torch.float16               896           0.00%     1.902257        1.037063  9.843750e-01    9.312500\n216  model.layers.23.post_attention_layernorm  torch.float16               896           0.00%     2.330924        0.357700 -1.707077e-04    7.718750\n217                                model.norm  torch.float16               896           0.00%     7.463624        0.841196 -2.281250e+00   17.375000\n218                                   lm_head  torch.float16         136134656          50.00%     0.000132        0.014714 -1.962891e-01    0.127930\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\npruned_model_save_dir = \"./qwen2_mixed_pruned_8bit\" # Using a new directory for clarity\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# =============================================================================\n# STEP 1: Load the original model and identify layer groups\n# =============================================================================\nprint(f\"--- Step 1: Loading original model for pruning ---\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(\"\\nIdentifying layer groups for mixed-sparsity pruning...\")\nall_modules = dict(model.named_modules())\n\n# Group 1: The two largest layers (embedding and language model head)\ngroup1_names = ['model.embed_tokens', 'lm_head']\ngroup1_modules = [all_modules[name] for name in group1_names if name in all_modules]\n\n# Group 2: All other Linear layers\ngroup2_modules = []\nfor name, module in all_modules.items():\n    if isinstance(module, torch.nn.Linear) and name not in group1_names:\n        group2_modules.append(module)\n\nprint(f\"Group 1 (for 50% pruning): {len(group1_modules)} layers ({group1_names})\")\nprint(f\"Group 2 (for 25% pruning): {len(group2_modules)} other Linear layers\")\n\n\n# =============================================================================\n# STEP 2: Apply mixed-sparsity pruning\n# =============================================================================\nprint(\"\\n--- Step 2: Applying mixed-sparsity pruning ---\")\n# Prune Group 1 layers at 50% sparsity\nprint(\"Pruning Group 1 at 50% sparsity...\")\nfor module in group1_modules:\n    prune.l1_unstructured(module, name=\"weight\", amount=0.50)\n\n# Prune Group 2 layers at 25% sparsity\nprint(\"Pruning Group 2 at 25% sparsity...\")\nfor module in group2_modules:\n    prune.l1_unstructured(module, name=\"weight\", amount=0.25)\n\n# Make the pruning permanent\nprint(\"Making pruning permanent...\")\nall_pruned_modules = group1_modules + group2_modules\nfor module in all_pruned_modules:\n    prune.remove(module, 'weight')\n\nprint(\"Mixed-sparsity pruning complete.\")\n\n# =============================================================================\n# STEP 3: Save the pruned model\n# =============================================================================\nprint(f\"\\n--- Step 3: Saving the pruned model to {pruned_model_save_dir} ---\")\nmodel.save_pretrained(pruned_model_save_dir)\ntokenizer.save_pretrained(pruned_model_save_dir)\nprint(\"Pruned model saved successfully.\")\n\n# Clean up memory\ndel model\ntorch.cuda.empty_cache()\n\n# =============================================================================\n# STEP 4: Load the pruned model with 8-bit quantization\n# =============================================================================\nprint(f\"\\n--- Step 4: Loading the pruned model with 8-bit quantization ---\")\n# Define the 8-bit quantization configuration\nquantization_config_8bit = BitsAndBytesConfig(\n    load_in_8bit=True,\n)\n\n# Load the model FROM THE SAVED (PRUNED) DIRECTORY with quantization\nfinal_model_8bit = AutoModelForCausalLM.from_pretrained(\n    pruned_model_save_dir,\n    quantization_config=quantization_config_8bit,\n    device_map=\"auto\",\n)\nprint(\"Final pruned and 8-bit quantized model loaded successfully.\")\n\n\n# =============================================================================\n# STEP 5: Evaluate the performance of the final model\n# =============================================================================\ndef evaluate_perplexity(model, tokenizer, dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"test\"):\n    print(\"\\nEvaluating perplexity with sliding window...\")\n    test_dataset = load_dataset(dataset, subset, split=split)\n    encodings = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n    max_length = 2048\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Evaluating Chunks\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n        if input_ids.size(1) < 2: continue\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len: break\n    perplexity = torch.exp(torch.stack(nlls).mean())\n    return perplexity.item()\n\nprint(\"\\n--- Step 5: Evaluating performance of the final model ---\")\nfinal_perplexity_8bit = evaluate_perplexity(final_model_8bit, tokenizer)\n\nprint(\"\\n--- Final Performance Result ---\")\nprint(f\"Perplexity of the mixed-pruned and 8-bit quantized model: {final_perplexity_8bit:.4f}\")\nprint(f\"Baseline perplexity (original float16 model): 12.5985\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:56:26.825373Z","iopub.execute_input":"2025-06-21T10:56:26.825730Z","iopub.status.idle":"2025-06-21T10:59:42.648007Z","shell.execute_reply.started":"2025-06-21T10:56:26.825707Z","shell.execute_reply":"2025-06-21T10:59:42.647147Z"}},"outputs":[{"name":"stdout","text":"--- Step 1: Loading original model for pruning ---\n\nIdentifying layer groups for mixed-sparsity pruning...\nGroup 1 (for 50% pruning): 2 layers (['model.embed_tokens', 'lm_head'])\nGroup 2 (for 25% pruning): 168 other Linear layers\n\n--- Step 2: Applying mixed-sparsity pruning ---\nPruning Group 1 at 50% sparsity...\nPruning Group 2 at 25% sparsity...\nMaking pruning permanent...\nMixed-sparsity pruning complete.\n\n--- Step 3: Saving the pruned model to ./qwen2_mixed_pruned_8bit ---\nPruned model saved successfully.\n\n--- Step 4: Loading the pruned model with 8-bit quantization ---\nFinal pruned and 8-bit quantized model loaded successfully.\n\n--- Step 5: Evaluating performance of the final model ---\n\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 32768). Running this sequence through the model will result in indexing errors\nEvaluating Chunks:  99%|█████████▉| 581/585 [03:06<00:01,  3.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n--- Final Performance Result ---\nPerplexity of the mixed-pruned and 8-bit quantized model: 21.6283\nBaseline perplexity (original float16 model): 12.5985\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\npruned_model_save_dir = \"./qwen2_mixed_pruned_8bit\" # Using a new directory for clarity\noutput_stats_filename = \"mixed_pruned_8bit_layer_stats.csv\"\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# =============================================================================\n# STEP 1: Load the original model and identify layer groups\n# =============================================================================\nprint(f\"--- Step 1: Loading original model for pruning ---\")\n# Added trust_remote_code=True to prevent model loading errors\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(\"\\nIdentifying layer groups for mixed-sparsity pruning...\")\nall_modules = dict(model.named_modules())\n\n# Group 1: The two largest layers (embedding and language model head)\ngroup1_names = ['model.embed_tokens', 'lm_head']\ngroup1_modules = [all_modules[name] for name in group1_names if name in all_modules]\n\n# Group 2: All other Linear layers\ngroup2_modules = []\nfor name, module in all_modules.items():\n    if isinstance(module, torch.nn.Linear) and name not in group1_names:\n        group2_modules.append(module)\n\nprint(f\"Group 1 (for 50% pruning): {len(group1_modules)} layers ({group1_names})\")\nprint(f\"Group 2 (for 25% pruning): {len(group2_modules)} other Linear layers\")\n\n\n# =============================================================================\n# STEP 2: Apply mixed-sparsity pruning\n# =============================================================================\nprint(\"\\n--- Step 2: Applying mixed-sparsity pruning ---\")\n# Prune Group 1 layers at 50% sparsity\nprint(\"Pruning Group 1 at 50% sparsity...\")\nfor module in group1_modules:\n    prune.l1_unstructured(module, name=\"weight\", amount=0.50)\n\n# Prune Group 2 layers at 25% sparsity\nprint(\"Pruning Group 2 at 25% sparsity...\")\nfor module in group2_modules:\n    prune.l1_unstructured(module, name=\"weight\", amount=0.25)\n\n# Make the pruning permanent\nprint(\"\\nMaking pruning permanent...\")\nall_pruned_modules = group1_modules + group2_modules\nfor module in all_pruned_modules:\n    prune.remove(module, 'weight')\n\nprint(\"Mixed-sparsity pruning complete.\")\n\n# =============================================================================\n# STEP 3: Save the pruned model\n# =============================================================================\nprint(f\"\\n--- Step 3: Saving the pruned model to {pruned_model_save_dir} ---\")\nmodel.save_pretrained(pruned_model_save_dir)\ntokenizer.save_pretrained(pruned_model_save_dir)\nprint(\"Pruned model saved successfully.\")\n\n# Clean up memory\ndel model\ntorch.cuda.empty_cache()\n\n# =============================================================================\n# STEP 4: Load the pruned model with 8-bit quantization\n# =============================================================================\nprint(f\"\\n--- Step 4: Loading the pruned model with 8-bit quantization ---\")\n# Define the 8-bit quantization configuration\nquantization_config_8bit = BitsAndBytesConfig(\n    load_in_8bit=True,\n)\n\n# Load the model FROM THE SAVED (PRUNED) DIRECTORY with quantization\nfinal_model_8bit = AutoModelForCausalLM.from_pretrained(\n    pruned_model_save_dir,\n    quantization_config=quantization_config_8bit,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nprint(\"Final pruned and 8-bit quantized model loaded successfully.\")\n\n\n# =============================================================================\n# STEP 5: (Optional) Evaluate the performance of the final model\n# =============================================================================\n# Note: Skipping perplexity evaluation to focus on statistical analysis as requested.\n# You can uncomment this section to run it again.\n#\n# def evaluate_perplexity(model, tokenizer, dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"test\"):\n#     ... (full function code) ...\n#\n# print(\"\\n--- Step 5: Evaluating performance of the final model ---\")\n# final_perplexity_8bit = evaluate_perplexity(final_model_8bit, tokenizer)\n#\n# print(\"\\n--- Final Performance Result ---\")\n# print(f\"Perplexity of the mixed-pruned and 8-bit quantized model: {final_perplexity_8bit:.4f}\")\n\n\n# =============================================================================\n# STEP 6: Perform and Save Layer-wise Statistical Analysis\n# =============================================================================\ndef generate_layer_wise_stats(model, model_description=\"model\"):\n    \"\"\"\n    Analyzes each layer in a model and returns a DataFrame with its stats.\n    \"\"\"\n    print(f\"\\n--- Generating Layer-wise Statistical Analysis for: {model_description} ---\")\n    layer_stats = []\n    for name, module in model.named_modules():\n        # Check for layers that have a weight parameter\n        if hasattr(module, \"weight\") and isinstance(module.weight, torch.nn.Parameter):\n            with torch.no_grad():\n                weights = module.weight.data\n                \n                # Calculate the actual sparsity of the layer\n                non_zero_elements = torch.count_nonzero(weights).item()\n                total_elements = weights.numel()\n                # Use standard Python division, which works correctly on integers\n                actual_sparsity = 1.0 - (non_zero_elements / total_elements)\n\n                layer_stats.append({\n                    'Layer Name': name,\n                    'Data Type': weights.dtype,\n                    'Number of Params': total_elements,\n                    'Actual Sparsity': f\"{actual_sparsity:.2%}\",\n                    'Weight Mean': weights.to(torch.float32).mean().item(),\n                    'Weight Std Dev': weights.to(torch.float32).std().item(),\n                    'Weight Min': weights.min().item(),\n                    'Weight Max': weights.max().item(),\n                })\n    \n    return pd.DataFrame(layer_stats)\n\n# Generate the stats for the final model\nfinal_results_df = generate_layer_wise_stats(final_model_8bit, \"Mixed-Pruned + 8-bit Model\")\n\n# Save the results to a CSV file\nfinal_results_df.to_csv(output_stats_filename, index=False)\nprint(f\"\\nLayer-wise statistical analysis saved to '{output_stats_filename}'\")\n\n# Display the full results table\nprint(\"\\n--- Layer-wise Results ---\")\nprint(final_results_df.to_string())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T11:57:16.257728Z","iopub.execute_input":"2025-06-21T11:57:16.258338Z","iopub.status.idle":"2025-06-21T11:57:22.419854Z","shell.execute_reply.started":"2025-06-21T11:57:16.258313Z","shell.execute_reply":"2025-06-21T11:57:22.419102Z"}},"outputs":[{"name":"stdout","text":"--- Step 1: Loading original model for pruning ---\n\nIdentifying layer groups for mixed-sparsity pruning...\nGroup 1 (for 50% pruning): 2 layers (['model.embed_tokens', 'lm_head'])\nGroup 2 (for 25% pruning): 168 other Linear layers\n\n--- Step 2: Applying mixed-sparsity pruning ---\nPruning Group 1 at 50% sparsity...\nPruning Group 2 at 25% sparsity...\n\nMaking pruning permanent...\nMixed-sparsity pruning complete.\n\n--- Step 3: Saving the pruned model to ./qwen2_mixed_pruned_8bit ---\nPruned model saved successfully.\n\n--- Step 4: Loading the pruned model with 8-bit quantization ---\nFinal pruned and 8-bit quantized model loaded successfully.\n\n--- Generating Layer-wise Statistical Analysis for: Mixed-Pruned + 8-bit Model ---\n\nLayer-wise statistical analysis saved to 'mixed_pruned_8bit_layer_stats.csv'\n\n--- Layer-wise Results ---\n                                   Layer Name      Data Type  Number of Params Actual Sparsity  Weight Mean  Weight Std Dev    Weight Min  Weight Max\n0                          model.embed_tokens  torch.float16         136134656          50.00%     0.000132        0.014714 -1.962891e-01    0.127930\n1             model.layers.0.self_attn.q_proj     torch.int8            802816          25.00%    -0.021250       31.413420 -1.270000e+02  127.000000\n2             model.layers.0.self_attn.k_proj     torch.int8            114688          25.00%     0.041940       36.753944 -1.270000e+02  127.000000\n3             model.layers.0.self_attn.v_proj     torch.int8            114688          25.00%    -0.041521       33.030300 -1.270000e+02  127.000000\n4             model.layers.0.self_attn.o_proj     torch.int8            802816          25.00%    -0.012650       30.523140 -1.270000e+02  127.000000\n5                model.layers.0.mlp.gate_proj     torch.int8           4358144          25.00%     0.038527       35.831390 -1.270000e+02  127.000000\n6                  model.layers.0.mlp.up_proj     torch.int8           4358144          25.00%    -0.024930       36.041119 -1.270000e+02  127.000000\n7                model.layers.0.mlp.down_proj     torch.int8           4358144          25.00%     0.004043       29.833374 -1.270000e+02  127.000000\n8              model.layers.0.input_layernorm  torch.float16               896           0.00%     0.033067        0.125686 -6.250000e-01    0.960938\n9     model.layers.0.post_attention_layernorm  torch.float16               896           0.00%     0.722957        0.140165  4.257812e-01    1.820312\n10            model.layers.1.self_attn.q_proj     torch.int8            802816          25.00%    -0.071963       32.153160 -1.270000e+02  127.000000\n11            model.layers.1.self_attn.k_proj     torch.int8            114688          25.00%    -0.002572       34.352924 -1.270000e+02  127.000000\n12            model.layers.1.self_attn.v_proj     torch.int8            114688          25.00%    -0.078360       28.243513 -1.270000e+02  127.000000\n13            model.layers.1.self_attn.o_proj     torch.int8            802816          25.00%    -0.032489       30.876787 -1.270000e+02  127.000000\n14               model.layers.1.mlp.gate_proj     torch.int8           4358144          25.00%     0.038704       36.158501 -1.270000e+02  127.000000\n15                 model.layers.1.mlp.up_proj     torch.int8           4358144          25.00%     0.015489       36.023407 -1.270000e+02  127.000000\n16               model.layers.1.mlp.down_proj     torch.int8           4358144          25.00%    -0.019982       30.353373 -1.270000e+02  127.000000\n17             model.layers.1.input_layernorm  torch.float16               896           0.00%     0.797259        0.444815 -5.859375e-02    3.843750\n18    model.layers.1.post_attention_layernorm  torch.float16               896           0.00%     1.125865        0.291850  1.585484e-05    3.203125\n19            model.layers.2.self_attn.q_proj     torch.int8            802816          25.00%     0.052749       33.586525 -1.270000e+02  127.000000\n20            model.layers.2.self_attn.k_proj     torch.int8            114688          25.00%     0.048959       33.288181 -1.270000e+02  127.000000\n21            model.layers.2.self_attn.v_proj     torch.int8            114688          25.00%    -0.012791       32.576122 -1.270000e+02  127.000000\n22            model.layers.2.self_attn.o_proj     torch.int8            802816          25.00%     0.043922       33.110592 -1.270000e+02  127.000000\n23               model.layers.2.mlp.gate_proj     torch.int8           4358144          25.00%    -0.016667       35.768143 -1.270000e+02  127.000000\n24                 model.layers.2.mlp.up_proj     torch.int8           4358144          25.00%     0.004995       35.926437 -1.270000e+02  127.000000\n25               model.layers.2.mlp.down_proj     torch.int8           4358144          25.00%     0.002340       30.155380 -1.270000e+02  127.000000\n26             model.layers.2.input_layernorm  torch.float16               896           0.00%     0.921201        0.415566 -1.515625e+00    3.828125\n27    model.layers.2.post_attention_layernorm  torch.float16               896           0.00%     1.238739        0.299352 -2.443790e-06    3.156250\n28            model.layers.3.self_attn.q_proj     torch.int8            802816          25.00%    -0.060607       33.656040 -1.270000e+02  127.000000\n29            model.layers.3.self_attn.k_proj     torch.int8            114688          25.00%    -0.108111       33.605247 -1.270000e+02  127.000000\n30            model.layers.3.self_attn.v_proj     torch.int8            114688          25.00%    -0.065848       34.595081 -1.270000e+02  127.000000\n31            model.layers.3.self_attn.o_proj     torch.int8            802816          25.00%    -0.001718       32.799129 -1.270000e+02  127.000000\n32               model.layers.3.mlp.gate_proj     torch.int8           4358144          25.00%     0.046224       35.941128 -1.270000e+02  127.000000\n33                 model.layers.3.mlp.up_proj     torch.int8           4358144          25.00%    -0.009558       35.946468 -1.270000e+02  127.000000\n34               model.layers.3.mlp.down_proj     torch.int8           4358144          25.00%    -0.002681       29.442915 -1.270000e+02  127.000000\n35             model.layers.3.input_layernorm  torch.float16               896           0.00%     1.091552        0.287205 -9.921875e-01    3.593750\n36    model.layers.3.post_attention_layernorm  torch.float16               896           0.00%     1.345123        0.313676  4.553795e-05    3.140625\n37            model.layers.4.self_attn.q_proj     torch.int8            802816          25.00%    -0.042385       33.873241 -1.270000e+02  127.000000\n38            model.layers.4.self_attn.k_proj     torch.int8            114688          25.00%     0.017691       33.011646 -1.270000e+02  127.000000\n39            model.layers.4.self_attn.v_proj     torch.int8            114688          25.00%     0.085362       34.200161 -1.270000e+02  127.000000\n40            model.layers.4.self_attn.o_proj     torch.int8            802816          25.00%     0.027903       32.836090 -1.270000e+02  127.000000\n41               model.layers.4.mlp.gate_proj     torch.int8           4358144          25.00%    -0.079554       35.251816 -1.270000e+02  127.000000\n42                 model.layers.4.mlp.up_proj     torch.int8           4358144          25.00%     0.013749       35.296467 -1.270000e+02  127.000000\n43               model.layers.4.mlp.down_proj     torch.int8           4358144          25.00%    -0.001499       28.271141 -1.270000e+02  127.000000\n44             model.layers.4.input_layernorm  torch.float16               896           0.00%     0.959832        0.339886  5.004883e-02    3.296875\n45    model.layers.4.post_attention_layernorm  torch.float16               896           0.00%     1.266706        0.343240 -1.764297e-04    7.093750\n46            model.layers.5.self_attn.q_proj     torch.int8            802816          25.00%     0.084120       33.012585 -1.270000e+02  127.000000\n47            model.layers.5.self_attn.k_proj     torch.int8            114688          25.00%    -0.143389       33.037487 -1.270000e+02  127.000000\n48            model.layers.5.self_attn.v_proj     torch.int8            114688          25.00%     0.024606       34.942089 -1.270000e+02  127.000000\n49            model.layers.5.self_attn.o_proj     torch.int8            802816          25.00%     0.010507       32.245213 -1.270000e+02  127.000000\n50               model.layers.5.mlp.gate_proj     torch.int8           4358144          25.00%     0.027129       35.831089 -1.270000e+02  127.000000\n51                 model.layers.5.mlp.up_proj     torch.int8           4358144          25.00%     0.032744       35.707207 -1.270000e+02  127.000000\n52               model.layers.5.mlp.down_proj     torch.int8           4358144          25.00%     0.029663       28.526768 -1.270000e+02  127.000000\n53             model.layers.5.input_layernorm  torch.float16               896           0.00%     1.123697        0.402148 -1.257812e+00    3.937500\n54    model.layers.5.post_attention_layernorm  torch.float16               896           0.00%     1.297313        0.362451 -5.297852e-02    2.953125\n55            model.layers.6.self_attn.q_proj     torch.int8            802816          25.00%    -0.129357       34.866459 -1.270000e+02  127.000000\n56            model.layers.6.self_attn.k_proj     torch.int8            114688          25.00%     0.050694       33.248295 -1.270000e+02  127.000000\n57            model.layers.6.self_attn.v_proj     torch.int8            114688          25.00%     0.101170       33.243061 -1.270000e+02  127.000000\n58            model.layers.6.self_attn.o_proj     torch.int8            802816          25.00%     0.005988       32.515430 -1.270000e+02  127.000000\n59               model.layers.6.mlp.gate_proj     torch.int8           4358144          25.00%     0.008881       35.809452 -1.270000e+02  127.000000\n60                 model.layers.6.mlp.up_proj     torch.int8           4358144          25.00%    -0.012627       35.596123 -1.270000e+02  127.000000\n61               model.layers.6.mlp.down_proj     torch.int8           4358144          25.00%    -0.010948       27.798483 -1.270000e+02  127.000000\n62             model.layers.6.input_layernorm  torch.float16               896           0.00%     1.148009        0.431344 -8.281250e-01    3.968750\n63    model.layers.6.post_attention_layernorm  torch.float16               896           0.00%     1.323709        0.409137  1.068115e-04    3.781250\n64            model.layers.7.self_attn.q_proj     torch.int8            802816          25.00%     0.019970       33.771313 -1.270000e+02  127.000000\n65            model.layers.7.self_attn.k_proj     torch.int8            114688          25.00%     0.088030       33.622547 -1.270000e+02  127.000000\n66            model.layers.7.self_attn.v_proj     torch.int8            114688          25.00%     0.006923       33.503548 -1.270000e+02  127.000000\n67            model.layers.7.self_attn.o_proj     torch.int8            802816          25.00%    -0.025008       31.382612 -1.270000e+02  127.000000\n68               model.layers.7.mlp.gate_proj     torch.int8           4358144          25.00%     0.063821       35.513157 -1.270000e+02  127.000000\n69                 model.layers.7.mlp.up_proj     torch.int8           4358144          25.00%     0.033539       35.579975 -1.270000e+02  127.000000\n70               model.layers.7.mlp.down_proj     torch.int8           4358144          25.00%     0.024713       27.085625 -1.270000e+02  127.000000\n71             model.layers.7.input_layernorm  torch.float16               896           0.00%     1.319497        0.504894 -1.132812e+00    3.890625\n72    model.layers.7.post_attention_layernorm  torch.float16               896           0.00%     1.421060        0.476248 -2.789497e-05    3.593750\n73            model.layers.8.self_attn.q_proj     torch.int8            802816          25.00%     0.007464       33.946712 -1.270000e+02  127.000000\n74            model.layers.8.self_attn.k_proj     torch.int8            114688          25.00%     0.219291       34.388340 -1.270000e+02  127.000000\n75            model.layers.8.self_attn.v_proj     torch.int8            114688          25.00%    -0.132455       28.427774 -1.270000e+02  127.000000\n76            model.layers.8.self_attn.o_proj     torch.int8            802816          25.00%    -0.025797       30.947569 -1.270000e+02  127.000000\n77               model.layers.8.mlp.gate_proj     torch.int8           4358144          25.00%    -0.065935       34.586292 -1.270000e+02  127.000000\n78                 model.layers.8.mlp.up_proj     torch.int8           4358144          25.00%    -0.000177       34.673149 -1.270000e+02  127.000000\n79               model.layers.8.mlp.down_proj     torch.int8           4358144          25.00%     0.001254       26.903561 -1.270000e+02  127.000000\n80             model.layers.8.input_layernorm  torch.float16               896           0.00%     1.069055        0.557704 -1.162109e-01    3.265625\n81    model.layers.8.post_attention_layernorm  torch.float16               896           0.00%     1.269379        0.485914  2.578125e-01    3.531250\n82            model.layers.9.self_attn.q_proj     torch.int8            802816          25.00%     0.029577       32.043968 -1.270000e+02  127.000000\n83            model.layers.9.self_attn.k_proj     torch.int8            114688          25.00%    -0.004892       30.866831 -1.270000e+02  127.000000\n84            model.layers.9.self_attn.v_proj     torch.int8            114688          25.00%     0.053763       34.020241 -1.270000e+02  127.000000\n85            model.layers.9.self_attn.o_proj     torch.int8            802816          25.00%     0.026615       30.690624 -1.270000e+02  127.000000\n86               model.layers.9.mlp.gate_proj     torch.int8           4358144          25.00%     0.021803       34.414948 -1.270000e+02  127.000000\n87                 model.layers.9.mlp.up_proj     torch.int8           4358144          25.00%    -0.000556       34.360489 -1.270000e+02  127.000000\n88               model.layers.9.mlp.down_proj     torch.int8           4358144          25.00%     0.003672       25.922571 -1.270000e+02  127.000000\n89             model.layers.9.input_layernorm  torch.float16               896           0.00%     1.568166        0.972377 -8.046875e-01   10.750000\n90    model.layers.9.post_attention_layernorm  torch.float16               896           0.00%     1.362994        0.577671  5.960464e-08    3.203125\n91           model.layers.10.self_attn.q_proj     torch.int8            802816          25.00%    -0.037042       32.816578 -1.270000e+02  127.000000\n92           model.layers.10.self_attn.k_proj     torch.int8            114688          25.00%    -0.064540       33.037228 -1.270000e+02  127.000000\n93           model.layers.10.self_attn.v_proj     torch.int8            114688          25.00%     0.111738       31.548416 -1.270000e+02  127.000000\n94           model.layers.10.self_attn.o_proj     torch.int8            802816          25.00%     0.011882       30.156738 -1.270000e+02  127.000000\n95              model.layers.10.mlp.gate_proj     torch.int8           4358144          25.00%     0.053920       33.494568 -1.270000e+02  127.000000\n96                model.layers.10.mlp.up_proj     torch.int8           4358144          25.00%     0.008085       33.595665 -1.270000e+02  127.000000\n97              model.layers.10.mlp.down_proj     torch.int8           4358144          25.00%     0.006158       24.895643 -1.270000e+02  127.000000\n98            model.layers.10.input_layernorm  torch.float16               896           0.00%     1.337198        0.957490 -1.117188e+00    4.531250\n99   model.layers.10.post_attention_layernorm  torch.float16               896           0.00%     1.315180        0.660076  2.412109e-01    3.515625\n100          model.layers.11.self_attn.q_proj     torch.int8            802816          25.00%    -0.053640       30.876289 -1.270000e+02  127.000000\n101          model.layers.11.self_attn.k_proj     torch.int8            114688          25.00%     0.068795       30.741535 -1.270000e+02  127.000000\n102          model.layers.11.self_attn.v_proj     torch.int8            114688          25.00%    -0.053449       26.914436 -1.270000e+02  127.000000\n103          model.layers.11.self_attn.o_proj     torch.int8            802816          25.00%     0.011230       25.576267 -1.270000e+02  127.000000\n104             model.layers.11.mlp.gate_proj     torch.int8           4358144          25.00%     0.145906       33.745392 -1.270000e+02  127.000000\n105               model.layers.11.mlp.up_proj     torch.int8           4358144          25.00%    -0.001148       33.854145 -1.270000e+02  127.000000\n106             model.layers.11.mlp.down_proj     torch.int8           4358144          25.00%    -0.005163       25.864639 -1.270000e+02  127.000000\n107           model.layers.11.input_layernorm  torch.float16               896           0.00%     1.796278        1.100070 -3.955078e-02    7.375000\n108  model.layers.11.post_attention_layernorm  torch.float16               896           0.00%     1.319809        0.492273 -4.386902e-05    2.656250\n109          model.layers.12.self_attn.q_proj     torch.int8            802816          25.00%     0.000356       31.846403 -1.270000e+02  127.000000\n110          model.layers.12.self_attn.k_proj     torch.int8            114688          25.00%    -0.077271       29.658899 -1.270000e+02  127.000000\n111          model.layers.12.self_attn.v_proj     torch.int8            114688          25.00%     0.011509       32.452614 -1.270000e+02  127.000000\n112          model.layers.12.self_attn.o_proj     torch.int8            802816          25.00%    -0.028792       30.616190 -1.270000e+02  127.000000\n113             model.layers.12.mlp.gate_proj     torch.int8           4358144          25.00%     0.105617       34.521294 -1.270000e+02  127.000000\n114               model.layers.12.mlp.up_proj     torch.int8           4358144          25.00%     0.014395       34.560295 -1.270000e+02  127.000000\n115             model.layers.12.mlp.down_proj     torch.int8           4358144          25.00%     0.011691       26.815569 -1.270000e+02  127.000000\n116           model.layers.12.input_layernorm  torch.float16               896           0.00%     1.513124        0.614301 -9.062500e-01    3.187500\n117  model.layers.12.post_attention_layernorm  torch.float16               896           0.00%     1.389511        0.493660 -1.554489e-04    2.531250\n118          model.layers.13.self_attn.q_proj     torch.int8            802816          25.00%     0.032374       34.334843 -1.270000e+02  127.000000\n119          model.layers.13.self_attn.k_proj     torch.int8            114688          25.00%     0.040170       34.333305 -1.270000e+02  127.000000\n120          model.layers.13.self_attn.v_proj     torch.int8            114688          25.00%     0.223877       35.287922 -1.270000e+02  127.000000\n121          model.layers.13.self_attn.o_proj     torch.int8            802816          25.00%    -0.024459       32.880253 -1.270000e+02  127.000000\n122             model.layers.13.mlp.gate_proj     torch.int8           4358144          25.00%     0.120367       34.630531 -1.270000e+02  127.000000\n123               model.layers.13.mlp.up_proj     torch.int8           4358144          25.00%     0.005909       34.964752 -1.270000e+02  127.000000\n124             model.layers.13.mlp.down_proj     torch.int8           4358144          25.00%     0.000781       26.951872 -1.270000e+02  127.000000\n125           model.layers.13.input_layernorm  torch.float16               896           0.00%     1.486157        0.668099  9.130859e-02    3.359375\n126  model.layers.13.post_attention_layernorm  torch.float16               896           0.00%     1.372921        0.469418  2.460938e-01    2.390625\n127          model.layers.14.self_attn.q_proj     torch.int8            802816          25.00%    -0.030757       33.464207 -1.270000e+02  127.000000\n128          model.layers.14.self_attn.k_proj     torch.int8            114688          25.00%     0.113639       34.290573 -1.270000e+02  127.000000\n129          model.layers.14.self_attn.v_proj     torch.int8            114688          25.00%    -0.118914       35.338764 -1.270000e+02  127.000000\n130          model.layers.14.self_attn.o_proj     torch.int8            802816          25.00%     0.013232       32.723263 -1.270000e+02  127.000000\n131             model.layers.14.mlp.gate_proj     torch.int8           4358144          25.00%     0.213512       34.581860 -1.270000e+02  127.000000\n132               model.layers.14.mlp.up_proj     torch.int8           4358144          25.00%     0.007027       34.756256 -1.270000e+02  127.000000\n133             model.layers.14.mlp.down_proj     torch.int8           4358144          25.00%     0.005657       26.986864 -1.270000e+02  127.000000\n134           model.layers.14.input_layernorm  torch.float16               896           0.00%     1.626853        0.703336 -7.070312e-01    3.421875\n135  model.layers.14.post_attention_layernorm  torch.float16               896           0.00%     1.400580        0.434556 -2.746582e-02    2.343750\n136          model.layers.15.self_attn.q_proj     torch.int8            802816          25.00%    -0.045102       34.270882 -1.270000e+02  127.000000\n137          model.layers.15.self_attn.k_proj     torch.int8            114688          25.00%     0.165920       34.986786 -1.270000e+02  127.000000\n138          model.layers.15.self_attn.v_proj     torch.int8            114688          25.00%    -0.026219       34.969879 -1.270000e+02  127.000000\n139          model.layers.15.self_attn.o_proj     torch.int8            802816          25.00%    -0.004386       32.521145 -1.270000e+02  127.000000\n140             model.layers.15.mlp.gate_proj     torch.int8           4358144          25.00%     0.154155       33.657673 -1.270000e+02  127.000000\n141               model.layers.15.mlp.up_proj     torch.int8           4358144          25.00%     0.014790       34.337997 -1.270000e+02  127.000000\n142             model.layers.15.mlp.down_proj     torch.int8           4358144          25.00%     0.003401       26.506905 -1.270000e+02  127.000000\n143           model.layers.15.input_layernorm  torch.float16               896           0.00%     1.425349        0.598486 -6.718750e-01    3.218750\n144  model.layers.15.post_attention_layernorm  torch.float16               896           0.00%     1.477057        0.390902  3.964844e-01    2.640625\n145          model.layers.16.self_attn.q_proj     torch.int8            802816          25.00%     0.105566       33.277199 -1.270000e+02  127.000000\n146          model.layers.16.self_attn.k_proj     torch.int8            114688          25.00%    -0.007760       33.316818 -1.270000e+02  127.000000\n147          model.layers.16.self_attn.v_proj     torch.int8            114688          25.00%     0.005781       27.301527 -1.270000e+02  127.000000\n148          model.layers.16.self_attn.o_proj     torch.int8            802816          25.00%    -0.017127       30.256948 -1.270000e+02  127.000000\n149             model.layers.16.mlp.gate_proj     torch.int8           4358144          25.00%     0.015202       32.708858 -1.270000e+02  127.000000\n150               model.layers.16.mlp.up_proj     torch.int8           4358144          25.00%    -0.020660       33.819023 -1.270000e+02  127.000000\n151             model.layers.16.mlp.down_proj     torch.int8           4358144          25.00%     0.019317       27.052563 -1.270000e+02  127.000000\n152           model.layers.16.input_layernorm  torch.float16               896           0.00%     1.660565        0.565342 -9.101562e-01    3.562500\n153  model.layers.16.post_attention_layernorm  torch.float16               896           0.00%     1.578326        0.341396  4.335938e-01    3.515625\n154          model.layers.17.self_attn.q_proj     torch.int8            802816          25.00%    -0.014474       32.305164 -1.270000e+02  127.000000\n155          model.layers.17.self_attn.k_proj     torch.int8            114688          25.00%     0.051854       30.783367 -1.270000e+02  127.000000\n156          model.layers.17.self_attn.v_proj     torch.int8            114688          25.00%    -0.092215       33.742302 -1.270000e+02  127.000000\n157          model.layers.17.self_attn.o_proj     torch.int8            802816          25.00%     0.000384       32.790020 -1.270000e+02  127.000000\n158             model.layers.17.mlp.gate_proj     torch.int8           4358144          25.00%    -0.163403       34.493324 -1.270000e+02  127.000000\n159               model.layers.17.mlp.up_proj     torch.int8           4358144          25.00%    -0.021919       35.201004 -1.270000e+02  127.000000\n160             model.layers.17.mlp.down_proj     torch.int8           4358144          25.00%     0.001572       28.667173 -1.270000e+02  127.000000\n161           model.layers.17.input_layernorm  torch.float16               896           0.00%     1.571923        0.485495  8.544922e-02    3.390625\n162  model.layers.17.post_attention_layernorm  torch.float16               896           0.00%     1.730824        0.371812  3.398438e-01    3.234375\n163          model.layers.18.self_attn.q_proj     torch.int8            802816          25.00%    -0.000128       32.993805 -1.270000e+02  127.000000\n164          model.layers.18.self_attn.k_proj     torch.int8            114688          25.00%    -0.092521       31.147635 -1.270000e+02  127.000000\n165          model.layers.18.self_attn.v_proj     torch.int8            114688          25.00%    -0.083156       33.204113 -1.270000e+02  127.000000\n166          model.layers.18.self_attn.o_proj     torch.int8            802816          25.00%    -0.018671       32.664646 -1.270000e+02  127.000000\n167             model.layers.18.mlp.gate_proj     torch.int8           4358144          25.00%    -0.050281       36.006634 -1.270000e+02  127.000000\n168               model.layers.18.mlp.up_proj     torch.int8           4358144          25.00%    -0.020751       36.221779 -1.270000e+02  127.000000\n169             model.layers.18.mlp.down_proj     torch.int8           4358144          25.00%     0.001355       29.430990 -1.270000e+02  127.000000\n170           model.layers.18.input_layernorm  torch.float16               896           0.00%     1.589983        0.380777 -9.179688e-01    3.312500\n171  model.layers.18.post_attention_layernorm  torch.float16               896           0.00%     1.719568        0.396017  3.222656e-01    4.125000\n172          model.layers.19.self_attn.q_proj     torch.int8            802816          25.00%    -0.026562       34.405346 -1.270000e+02  127.000000\n173          model.layers.19.self_attn.k_proj     torch.int8            114688          25.00%    -0.028015       33.229939 -1.270000e+02  127.000000\n174          model.layers.19.self_attn.v_proj     torch.int8            114688          25.00%    -0.090524       34.796272 -1.270000e+02  127.000000\n175          model.layers.19.self_attn.o_proj     torch.int8            802816          25.00%    -0.022199       32.791618 -1.270000e+02  127.000000\n176             model.layers.19.mlp.gate_proj     torch.int8           4358144          25.00%    -0.060670       36.006741 -1.270000e+02  127.000000\n177               model.layers.19.mlp.up_proj     torch.int8           4358144          25.00%     0.029411       36.131153 -1.270000e+02  127.000000\n178             model.layers.19.mlp.down_proj     torch.int8           4358144          25.00%    -0.002021       29.349266 -1.270000e+02  127.000000\n179           model.layers.19.input_layernorm  torch.float16               896           0.00%     1.457768        0.332832 -3.789062e-01    3.609375\n180  model.layers.19.post_attention_layernorm  torch.float16               896           0.00%     1.908927        0.385458  5.273438e-01    4.625000\n181          model.layers.20.self_attn.q_proj     torch.int8            802816          25.00%    -0.005195       32.946293 -1.270000e+02  127.000000\n182          model.layers.20.self_attn.k_proj     torch.int8            114688          25.00%     0.094596       33.400116 -1.270000e+02  127.000000\n183          model.layers.20.self_attn.v_proj     torch.int8            114688          25.00%     0.163696       25.563549 -1.270000e+02  127.000000\n184          model.layers.20.self_attn.o_proj     torch.int8            802816          25.00%     0.009777       30.979660 -1.270000e+02  127.000000\n185             model.layers.20.mlp.gate_proj     torch.int8           4358144          25.00%    -0.025350       35.251740 -1.270000e+02  127.000000\n186               model.layers.20.mlp.up_proj     torch.int8           4358144          25.00%    -0.006556       35.485374 -1.270000e+02  127.000000\n187             model.layers.20.mlp.down_proj     torch.int8           4358144          25.00%    -0.021969       28.599619 -1.270000e+02  127.000000\n188           model.layers.20.input_layernorm  torch.float16               896           0.00%     1.516144        0.670899 -3.906250e-02    4.375000\n189  model.layers.20.post_attention_layernorm  torch.float16               896           0.00%     2.016449        0.307135  4.062500e-01    5.812500\n190          model.layers.21.self_attn.q_proj     torch.int8            802816          25.00%    -0.018197       30.245399 -1.270000e+02  127.000000\n191          model.layers.21.self_attn.k_proj     torch.int8            114688          25.00%     0.047520       29.845579 -1.270000e+02  127.000000\n192          model.layers.21.self_attn.v_proj     torch.int8            114688          25.00%    -0.053798       24.415119 -1.270000e+02  127.000000\n193          model.layers.21.self_attn.o_proj     torch.int8            802816          25.00%    -0.015763       35.350914 -1.270000e+02  127.000000\n194             model.layers.21.mlp.gate_proj     torch.int8           4358144          25.00%     0.060286       35.811977 -1.270000e+02  127.000000\n195               model.layers.21.mlp.up_proj     torch.int8           4358144          25.00%     0.002657       36.011868 -1.270000e+02  127.000000\n196             model.layers.21.mlp.down_proj     torch.int8           4358144          25.00%    -0.011065       29.296122 -1.270000e+02  127.000000\n197           model.layers.21.input_layernorm  torch.float16               896           0.00%     1.617219        1.366368  5.712891e-02    7.062500\n198  model.layers.21.post_attention_layernorm  torch.float16               896           0.00%     2.010075        0.280976 -2.803802e-04    5.187500\n199          model.layers.22.self_attn.q_proj     torch.int8            802816          25.00%    -0.003690       30.630625 -1.270000e+02  127.000000\n200          model.layers.22.self_attn.k_proj     torch.int8            114688          25.00%    -0.131984       28.943899 -1.270000e+02  127.000000\n201          model.layers.22.self_attn.v_proj     torch.int8            114688          25.00%    -0.059431       25.293749 -1.270000e+02  127.000000\n202          model.layers.22.self_attn.o_proj     torch.int8            802816          25.00%     0.008236       33.127560 -1.270000e+02  127.000000\n203             model.layers.22.mlp.gate_proj     torch.int8           4358144          25.00%    -0.021092       35.434013 -1.270000e+02  127.000000\n204               model.layers.22.mlp.up_proj     torch.int8           4358144          25.00%    -0.030016       35.808197 -1.270000e+02  127.000000\n205             model.layers.22.mlp.down_proj     torch.int8           4358144          25.00%    -0.003690       28.887489 -1.270000e+02  127.000000\n206           model.layers.22.input_layernorm  torch.float16               896           0.00%     1.654051        1.173776  1.582031e-01    6.468750\n207  model.layers.22.post_attention_layernorm  torch.float16               896           0.00%     2.134290        0.347131 -3.504753e-05    9.312500\n208          model.layers.23.self_attn.q_proj     torch.int8            802816          25.00%     0.024507       32.327190 -1.270000e+02  127.000000\n209          model.layers.23.self_attn.k_proj     torch.int8            114688          25.00%     0.025600       30.503771 -1.270000e+02  127.000000\n210          model.layers.23.self_attn.v_proj     torch.int8            114688          25.00%     0.083252       28.503887 -1.270000e+02  127.000000\n211          model.layers.23.self_attn.o_proj     torch.int8            802816          25.00%    -0.008221       33.634487 -1.270000e+02  127.000000\n212             model.layers.23.mlp.gate_proj     torch.int8           4358144          25.00%     0.141965       35.849609 -1.270000e+02  127.000000\n213               model.layers.23.mlp.up_proj     torch.int8           4358144          25.00%    -0.023904       35.405285 -1.270000e+02  127.000000\n214             model.layers.23.mlp.down_proj     torch.int8           4358144          25.00%    -0.002172       28.736134 -1.270000e+02  127.000000\n215           model.layers.23.input_layernorm  torch.float16               896           0.00%     1.902257        1.037063  9.843750e-01    9.312500\n216  model.layers.23.post_attention_layernorm  torch.float16               896           0.00%     2.330924        0.357700 -1.707077e-04    7.718750\n217                                model.norm  torch.float16               896           0.00%     7.463624        0.841196 -2.281250e+00   17.375000\n218                                   lm_head  torch.float16         136134656          50.00%     0.000132        0.014714 -1.962891e-01    0.127930\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\npruned_model_save_dir = \"./qwen2_ranked_pruned\"\nlayer_stats_csv = \"statistical_analysis_of_layers.csv\"\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# =============================================================================\n# STEP 1: Load the original model and identify layer groups from CSV\n# =============================================================================\nprint(f\"--- Step 1: Loading original model for pruning ---\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"\\nReading layer rankings from '{layer_stats_csv}'...\")\n# Load the CSV file which is already sorted by parameter count\ndf_stats = pd.read_csv(layer_stats_csv)\n\n# Get the names of the top 4 layers\ntop_4_layer_names = df_stats.head(4)['Layer Name'].tolist()\n\n# Get the actual module objects for these layers\nall_modules = dict(model.named_modules())\ntop_4_modules = [all_modules[name] for name in top_4_layer_names]\n\nprint(\"Identified layer groups for ranked pruning:\")\nprint(f\"  - Group 1 (25%): {top_4_layer_names[0]}\")\nprint(f\"  - Group 2 (35%): {top_4_layer_names[1]}\")\nprint(f\"  - Group 3 (45%): {top_4_layer_names[2]}\")\nprint(f\"  - Group 4 (50%): {top_4_layer_names[3]}\")\n\n# =============================================================================\n# STEP 2: Apply ranked-sparsity pruning\n# =============================================================================\nprint(\"\\n--- Step 2: Applying ranked-sparsity pruning ---\")\nsparsities = [0.25, 0.35, 0.45, 0.50]\n\nfor i, module in enumerate(top_4_modules):\n    sparsity = sparsities[i]\n    print(f\"Pruning '{top_4_layer_names[i]}' with {sparsity*100:.0f}% sparsity...\")\n    prune.l1_unstructured(module, name=\"weight\", amount=sparsity)\n\n# Make the pruning permanent for all pruned layers\nprint(\"\\nMaking pruning permanent...\")\nfor module in top_4_modules:\n    prune.remove(module, 'weight')\n\nprint(\"Ranked-sparsity pruning complete.\")\n\n\n# =============================================================================\n# STEP 3: Save the pruned model\n# =============================================================================\nprint(f\"\\n--- Step 3: Saving the pruned model to {pruned_model_save_dir} ---\")\nmodel.save_pretrained(pruned_model_save_dir)\ntokenizer.save_pretrained(pruned_model_save_dir)\nprint(\"Pruned model saved successfully.\")\n\ndel model\ntorch.cuda.empty_cache()\n\n\n# =============================================================================\n# STEP 4: Load the pruned model with 4-bit quantization\n# =============================================================================\nprint(f\"\\n--- Step 4: Loading the pruned model with 4-bit quantization ---\")\nquantization_config_4bit = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nfinal_model = AutoModelForCausalLM.from_pretrained(\n    pruned_model_save_dir,\n    quantization_config=quantization_config_4bit,\n    device_map=\"auto\",\n)\nprint(\"Final pruned and quantized model loaded successfully.\")\n\n\n# =============================================================================\n# STEP 5: Evaluate the performance of the final model\n# =============================================================================\ndef evaluate_perplexity(model, tokenizer, dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"test\"):\n    print(\"\\nEvaluating perplexity with sliding window...\")\n    test_dataset = load_dataset(dataset, subset, split=split)\n    encodings = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n    max_length = 2048\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Evaluating Chunks\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n        if input_ids.size(1) < 2: continue\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len: break\n    perplexity = torch.exp(torch.stack(nlls).mean())\n    return perplexity.item()\n\nprint(\"\\n--- Step 5: Evaluating performance of the final model ---\")\nfinal_perplexity = evaluate_perplexity(final_model, tokenizer)\n\nprint(\"\\n--- Final Performance Result ---\")\nprint(f\"Perplexity of the ranked-pruned and 4-bit quantized model: {final_perplexity:.4f}\")\nprint(f\"Baseline perplexity (original float16 model): 12.5985\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T11:03:45.730896Z","iopub.execute_input":"2025-06-21T11:03:45.731481Z","iopub.status.idle":"2025-06-21T11:05:44.389993Z","shell.execute_reply.started":"2025-06-21T11:03:45.731461Z","shell.execute_reply":"2025-06-21T11:05:44.389110Z"}},"outputs":[{"name":"stdout","text":"--- Step 1: Loading original model for pruning ---\n\nReading layer rankings from 'statistical_analysis_of_layers.csv'...\nIdentified layer groups for ranked pruning:\n  - Group 1 (25%): model.embed_tokens\n  - Group 2 (35%): lm_head\n  - Group 3 (45%): model.layers.11.mlp.up_proj\n  - Group 4 (50%): model.layers.11.mlp.gate_proj\n\n--- Step 2: Applying ranked-sparsity pruning ---\nPruning 'model.embed_tokens' with 25% sparsity...\nPruning 'lm_head' with 35% sparsity...\nPruning 'model.layers.11.mlp.up_proj' with 45% sparsity...\nPruning 'model.layers.11.mlp.gate_proj' with 50% sparsity...\n\nMaking pruning permanent...\nRanked-sparsity pruning complete.\n\n--- Step 3: Saving the pruned model to ./qwen2_ranked_pruned ---\nPruned model saved successfully.\n\n--- Step 4: Loading the pruned model with 4-bit quantization ---\nFinal pruned and quantized model loaded successfully.\n\n--- Step 5: Evaluating performance of the final model ---\n\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 32768). Running this sequence through the model will result in indexing errors\nEvaluating Chunks:  99%|█████████▉| 581/585 [01:49<00:00,  5.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Final Performance Result ---\nPerplexity of the ranked-pruned and 4-bit quantized model: 15.1293\nBaseline perplexity (original float16 model): 12.5985\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\npruned_model_save_dir = \"./qwen2_ranked_pruned_4bit\"\nlayer_stats_csv = \"statistical_analysis_of_layers.csv\" # Assumes this file exists from previous runs\noutput_stats_filename = \"ranked_pruned_4bit_layer_stats.csv\"\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# =============================================================================\n# STEP 1: Load the original model and identify layer groups from CSV\n# =============================================================================\nprint(f\"--- Step 1: Loading original model for pruning ---\")\n# Added trust_remote_code=True to prevent model loading errors\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# This part requires the \"statistical_analysis_of_layers.csv\" file to be present.\nprint(f\"\\nReading layer rankings from '{layer_stats_csv}'...\")\ndf_stats = pd.read_csv(layer_stats_csv)\n\n# Get the names of the top 4 layers\ntop_4_layer_names = df_stats.head(4)['Layer Name'].tolist()\n\n# Get the actual module objects for these layers\nall_modules = dict(model.named_modules())\ntop_4_modules = [all_modules[name] for name in top_4_layer_names]\n\nprint(\"Identified layer groups for ranked pruning:\")\nprint(f\"  - Group 1 (25%): {top_4_layer_names[0]}\")\nprint(f\"  - Group 2 (35%): {top_4_layer_names[1]}\")\nprint(f\"  - Group 3 (45%): {top_4_layer_names[2]}\")\nprint(f\"  - Group 4 (50%): {top_4_layer_names[3]}\")\n\n# =============================================================================\n# STEP 2: Apply ranked-sparsity pruning\n# =============================================================================\nprint(\"\\n--- Step 2: Applying ranked-sparsity pruning ---\")\nsparsities = [0.25, 0.35, 0.45, 0.50]\n\nfor i, module in enumerate(top_4_modules):\n    sparsity = sparsities[i]\n    print(f\"Pruning '{top_4_layer_names[i]}' with {sparsity*100:.0f}% sparsity...\")\n    prune.l1_unstructured(module, name=\"weight\", amount=sparsity)\n\nprint(\"\\nMaking pruning permanent...\")\nfor module in top_4_modules:\n    prune.remove(module, 'weight')\n\nprint(\"Ranked-sparsity pruning complete.\")\n\n\n# =============================================================================\n# STEP 3: Save the pruned model\n# =============================================================================\nprint(f\"\\n--- Step 3: Saving the pruned model to {pruned_model_save_dir} ---\")\nmodel.save_pretrained(pruned_model_save_dir)\ntokenizer.save_pretrained(pruned_model_save_dir)\nprint(\"Pruned model saved successfully.\")\n\ndel model\ntorch.cuda.empty_cache()\n\n\n# =============================================================================\n# STEP 4: Load the pruned model with 4-bit quantization\n# =============================================================================\nprint(f\"\\n--- Step 4: Loading the pruned model with 4-bit quantization ---\")\nquantization_config_4bit = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nfinal_model = AutoModelForCausalLM.from_pretrained(\n    pruned_model_save_dir,\n    quantization_config=quantization_config_4bit,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nprint(\"Final pruned and quantized model loaded successfully.\")\n\n\n# =============================================================================\n# STEP 5: (Optional) Evaluate the performance of the final model\n# =============================================================================\n# Skipping perplexity evaluation to focus on statistical analysis, but you can uncomment to run.\n# def evaluate_perplexity(model, tokenizer, dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"test\"):\n#     ... (full function code) ...\n#\n# print(\"\\n--- Step 5: Evaluating performance of the final model ---\")\n# final_perplexity = evaluate_perplexity(final_model, tokenizer)\n# print(f\"Perplexity: {final_perplexity:.4f}\")\n\n\n# =============================================================================\n# STEP 6: Perform and Save Layer-wise Statistical Analysis\n# =============================================================================\ndef generate_layer_wise_stats(model, model_description=\"model\"):\n    \"\"\"\n    Analyzes each layer in a model and returns a DataFrame with its stats.\n    \"\"\"\n    print(f\"\\n--- Generating Layer-wise Statistical Analysis for: {model_description} ---\")\n    layer_stats = []\n    for name, module in model.named_modules():\n        # Check for layers that have a weight parameter\n        if hasattr(module, \"weight\") and isinstance(module.weight, torch.nn.Parameter):\n            with torch.no_grad():\n                weights = module.weight.data\n                \n                # Calculate the actual sparsity of the layer\n                non_zero_elements = torch.count_nonzero(weights).item()\n                total_elements = weights.numel()\n                # Use standard Python division, which works correctly on integers\n                actual_sparsity = 1.0 - (non_zero_elements / total_elements)\n\n                layer_stats.append({\n                    'Layer Name': name,\n                    'Data Type': weights.dtype,\n                    'Number of Params': total_elements,\n                    'Actual Sparsity': f\"{actual_sparsity:.2%}\",\n                    'Weight Mean': weights.to(torch.float32).mean().item(),\n                    'Weight Std Dev': weights.to(torch.float32).std().item(),\n                    'Weight Min': weights.min().item(),\n                    'Weight Max': weights.max().item(),\n                })\n    \n    return pd.DataFrame(layer_stats)\n\n# Generate the stats for the final model\nfinal_results_df = generate_layer_wise_stats(final_model, \"Ranked-Pruned + 4-bit Model\")\n\n# Save the results to a CSV file\nfinal_results_df.to_csv(output_stats_filename, index=False)\nprint(f\"\\nLayer-wise statistical analysis saved to '{output_stats_filename}'\")\n\n# Display the full results table\nprint(\"\\n--- Layer-wise Results ---\")\nprint(final_results_df.to_string())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T11:53:47.938484Z","iopub.execute_input":"2025-06-21T11:53:47.938829Z","iopub.status.idle":"2025-06-21T11:53:53.010740Z","shell.execute_reply.started":"2025-06-21T11:53:47.938750Z","shell.execute_reply":"2025-06-21T11:53:53.009825Z"}},"outputs":[{"name":"stdout","text":"--- Step 1: Loading original model for pruning ---\n\nReading layer rankings from 'statistical_analysis_of_layers.csv'...\nIdentified layer groups for ranked pruning:\n  - Group 1 (25%): model.embed_tokens\n  - Group 2 (35%): lm_head\n  - Group 3 (45%): model.layers.11.mlp.up_proj\n  - Group 4 (50%): model.layers.11.mlp.gate_proj\n\n--- Step 2: Applying ranked-sparsity pruning ---\nPruning 'model.embed_tokens' with 25% sparsity...\nPruning 'lm_head' with 35% sparsity...\nPruning 'model.layers.11.mlp.up_proj' with 45% sparsity...\nPruning 'model.layers.11.mlp.gate_proj' with 50% sparsity...\n\nMaking pruning permanent...\nRanked-sparsity pruning complete.\n\n--- Step 3: Saving the pruned model to ./qwen2_ranked_pruned_4bit ---\nPruned model saved successfully.\n\n--- Step 4: Loading the pruned model with 4-bit quantization ---\nFinal pruned and quantized model loaded successfully.\n\n--- Generating Layer-wise Statistical Analysis for: Ranked-Pruned + 4-bit Model ---\n\nLayer-wise statistical analysis saved to 'ranked_pruned_4bit_layer_stats.csv'\n\n--- Layer-wise Results ---\n                                   Layer Name      Data Type  Number of Params Actual Sparsity  Weight Mean  Weight Std Dev    Weight Min  Weight Max\n0                          model.embed_tokens  torch.float16         136134656          35.00%     0.000135        0.015063 -1.962891e-01    0.127930\n1             model.layers.0.self_attn.q_proj    torch.uint8            401408           0.02%   122.449493       57.013077  0.000000e+00  255.000000\n2             model.layers.0.self_attn.k_proj    torch.uint8             57344           0.03%   122.912239       60.789528  0.000000e+00  255.000000\n3             model.layers.0.self_attn.v_proj    torch.uint8             57344           0.03%   122.284431       57.163578  0.000000e+00  255.000000\n4             model.layers.0.self_attn.o_proj    torch.uint8            401408           0.03%   122.530807       58.723915  0.000000e+00  255.000000\n5                model.layers.0.mlp.gate_proj    torch.uint8           2179072           0.03%   123.089676       60.116184  0.000000e+00  255.000000\n6                  model.layers.0.mlp.up_proj    torch.uint8           2179072           0.03%   122.503716       59.582504  0.000000e+00  255.000000\n7                model.layers.0.mlp.down_proj    torch.uint8           2179072           0.03%   122.434799       58.390778  0.000000e+00  255.000000\n8              model.layers.0.input_layernorm  torch.float16               896           0.00%     0.033067        0.125686 -6.250000e-01    0.960938\n9     model.layers.0.post_attention_layernorm  torch.float16               896           0.00%     0.722957        0.140165  4.257812e-01    1.820312\n10            model.layers.1.self_attn.q_proj    torch.uint8            401408           0.03%   122.119308       57.037323  0.000000e+00  255.000000\n11            model.layers.1.self_attn.k_proj    torch.uint8             57344           0.03%   122.468857       58.404640  0.000000e+00  255.000000\n12            model.layers.1.self_attn.v_proj    torch.uint8             57344           0.02%   122.019989       53.647526  0.000000e+00  255.000000\n13            model.layers.1.self_attn.o_proj    torch.uint8            401408           0.04%   122.435394       59.733967  0.000000e+00  255.000000\n14               model.layers.1.mlp.gate_proj    torch.uint8           2179072           0.03%   122.615517       59.848240  0.000000e+00  255.000000\n15                 model.layers.1.mlp.up_proj    torch.uint8           2179072           0.03%   122.587555       59.516453  0.000000e+00  255.000000\n16               model.layers.1.mlp.down_proj    torch.uint8           2179072           0.03%   122.521851       59.455154  0.000000e+00  255.000000\n17             model.layers.1.input_layernorm  torch.float16               896           0.00%     0.797259        0.444815 -5.859375e-02    3.843750\n18    model.layers.1.post_attention_layernorm  torch.float16               896           0.00%     1.125865        0.291850  1.585484e-05    3.203125\n19            model.layers.2.self_attn.q_proj    torch.uint8            401408           0.03%   122.440765       58.980537  0.000000e+00  255.000000\n20            model.layers.2.self_attn.k_proj    torch.uint8             57344           0.03%   122.334862       59.083477  0.000000e+00  255.000000\n21            model.layers.2.self_attn.v_proj    torch.uint8             57344           0.01%   122.437782       56.203239  0.000000e+00  255.000000\n22            model.layers.2.self_attn.o_proj    torch.uint8            401408           0.03%   122.740501       60.121815  0.000000e+00  255.000000\n23               model.layers.2.mlp.gate_proj    torch.uint8           2179072           0.03%   122.397972       59.655239  0.000000e+00  255.000000\n24                 model.layers.2.mlp.up_proj    torch.uint8           2179072           0.03%   122.582397       59.422638  0.000000e+00  255.000000\n25               model.layers.2.mlp.down_proj    torch.uint8           2179072           0.03%   122.475769       59.019272  0.000000e+00  255.000000\n26             model.layers.2.input_layernorm  torch.float16               896           0.00%     0.921201        0.415566 -1.515625e+00    3.828125\n27    model.layers.2.post_attention_layernorm  torch.float16               896           0.00%     1.238739        0.299352 -2.443790e-06    3.156250\n28            model.layers.3.self_attn.q_proj    torch.uint8            401408           0.03%   122.226318       58.690247  0.000000e+00  255.000000\n29            model.layers.3.self_attn.k_proj    torch.uint8             57344           0.03%   122.160583       58.665115  0.000000e+00  255.000000\n30            model.layers.3.self_attn.v_proj    torch.uint8             57344           0.03%   122.054642       58.241203  0.000000e+00  255.000000\n31            model.layers.3.self_attn.o_proj    torch.uint8            401408           0.03%   122.674500       60.211353  0.000000e+00  255.000000\n32               model.layers.3.mlp.gate_proj    torch.uint8           2179072           0.03%   122.436966       59.775272  0.000000e+00  255.000000\n33                 model.layers.3.mlp.up_proj    torch.uint8           2179072           0.03%   122.494865       59.320045  0.000000e+00  255.000000\n34               model.layers.3.mlp.down_proj    torch.uint8           2179072           0.03%   122.461182       58.789837  0.000000e+00  255.000000\n35             model.layers.3.input_layernorm  torch.float16               896           0.00%     1.091552        0.287205 -9.921875e-01    3.593750\n36    model.layers.3.post_attention_layernorm  torch.float16               896           0.00%     1.345123        0.313676  4.553795e-05    3.140625\n37            model.layers.4.self_attn.q_proj    torch.uint8            401408           0.03%   122.412567       58.395592  0.000000e+00  255.000000\n38            model.layers.4.self_attn.k_proj    torch.uint8             57344           0.02%   122.505638       57.437508  0.000000e+00  255.000000\n39            model.layers.4.self_attn.v_proj    torch.uint8             57344           0.03%   122.527206       57.753124  0.000000e+00  255.000000\n40            model.layers.4.self_attn.o_proj    torch.uint8            401408           0.03%   122.598671       60.049320  0.000000e+00  255.000000\n41               model.layers.4.mlp.gate_proj    torch.uint8           2179072           0.03%   122.308197       59.012550  0.000000e+00  255.000000\n42                 model.layers.4.mlp.up_proj    torch.uint8           2179072           0.03%   122.506302       58.776043  0.000000e+00  255.000000\n43               model.layers.4.mlp.down_proj    torch.uint8           2179072           0.03%   122.396080       57.773216  0.000000e+00  255.000000\n44             model.layers.4.input_layernorm  torch.float16               896           0.00%     0.959832        0.339886  5.004883e-02    3.296875\n45    model.layers.4.post_attention_layernorm  torch.float16               896           0.00%     1.266706        0.343240 -1.764297e-04    7.093750\n46            model.layers.5.self_attn.q_proj    torch.uint8            401408           0.03%   122.293861       57.509377  0.000000e+00  255.000000\n47            model.layers.5.self_attn.k_proj    torch.uint8             57344           0.02%   122.038734       56.686153  0.000000e+00  255.000000\n48            model.layers.5.self_attn.v_proj    torch.uint8             57344           0.02%   122.519554       58.281944  0.000000e+00  255.000000\n49            model.layers.5.self_attn.o_proj    torch.uint8            401408           0.04%   122.594696       59.359985  0.000000e+00  255.000000\n50               model.layers.5.mlp.gate_proj    torch.uint8           2179072           0.03%   122.635101       59.706650  0.000000e+00  255.000000\n51                 model.layers.5.mlp.up_proj    torch.uint8           2179072           0.03%   122.577965       59.258286  0.000000e+00  255.000000\n52               model.layers.5.mlp.down_proj    torch.uint8           2179072           0.03%   122.566971       58.356331  0.000000e+00  255.000000\n53             model.layers.5.input_layernorm  torch.float16               896           0.00%     1.123697        0.402148 -1.257812e+00    3.937500\n54    model.layers.5.post_attention_layernorm  torch.float16               896           0.00%     1.297313        0.362451 -5.297852e-02    2.953125\n55            model.layers.6.self_attn.q_proj    torch.uint8            401408           0.02%   122.348228       59.341290  0.000000e+00  255.000000\n56            model.layers.6.self_attn.k_proj    torch.uint8             57344           0.03%   122.607460       58.644550  0.000000e+00  255.000000\n57            model.layers.6.self_attn.v_proj    torch.uint8             57344           0.03%   122.672318       56.538612  0.000000e+00  255.000000\n58            model.layers.6.self_attn.o_proj    torch.uint8            401408           0.04%   122.621689       60.086044  0.000000e+00  255.000000\n59               model.layers.6.mlp.gate_proj    torch.uint8           2179072           0.03%   122.487679       59.610676  0.000000e+00  255.000000\n60                 model.layers.6.mlp.up_proj    torch.uint8           2179072           0.03%   122.515717       59.159874  0.000000e+00  255.000000\n61               model.layers.6.mlp.down_proj    torch.uint8           2179072           0.03%   122.427277       58.425549  0.000000e+00  255.000000\n62             model.layers.6.input_layernorm  torch.float16               896           0.00%     1.148009        0.431344 -8.281250e-01    3.968750\n63    model.layers.6.post_attention_layernorm  torch.float16               896           0.00%     1.323709        0.409137  1.068115e-04    3.781250\n64            model.layers.7.self_attn.q_proj    torch.uint8            401408           0.03%   122.507233       58.877815  0.000000e+00  255.000000\n65            model.layers.7.self_attn.k_proj    torch.uint8             57344           0.02%   122.809525       58.405144  0.000000e+00  255.000000\n66            model.layers.7.self_attn.v_proj    torch.uint8             57344           0.01%   122.398529       57.105778  0.000000e+00  255.000000\n67            model.layers.7.self_attn.o_proj    torch.uint8            401408           0.03%   122.427872       60.231308  0.000000e+00  255.000000\n68               model.layers.7.mlp.gate_proj    torch.uint8           2179072           0.03%   122.467621       59.449482  0.000000e+00  255.000000\n69                 model.layers.7.mlp.up_proj    torch.uint8           2179072           0.03%   122.576958       59.029331  0.000000e+00  255.000000\n70               model.layers.7.mlp.down_proj    torch.uint8           2179072           0.02%   122.448036       57.869713  0.000000e+00  255.000000\n71             model.layers.7.input_layernorm  torch.float16               896           0.00%     1.319497        0.504894 -1.132812e+00    3.890625\n72    model.layers.7.post_attention_layernorm  torch.float16               896           0.00%     1.421060        0.476248 -2.789497e-05    3.593750\n73            model.layers.8.self_attn.q_proj    torch.uint8            401408           0.03%   122.286270       58.512749  0.000000e+00  255.000000\n74            model.layers.8.self_attn.k_proj    torch.uint8             57344           0.03%   122.515854       58.737476  0.000000e+00  255.000000\n75            model.layers.8.self_attn.v_proj    torch.uint8             57344           0.02%   121.556747       53.919308  0.000000e+00  255.000000\n76            model.layers.8.self_attn.o_proj    torch.uint8            401408           0.03%   122.546211       59.318813  0.000000e+00  255.000000\n77               model.layers.8.mlp.gate_proj    torch.uint8           2179072           0.03%   122.291397       58.586197  0.000000e+00  255.000000\n78                 model.layers.8.mlp.up_proj    torch.uint8           2179072           0.03%   122.422333       58.211346  0.000000e+00  255.000000\n79               model.layers.8.mlp.down_proj    torch.uint8           2179072           0.02%   122.363037       57.372505  0.000000e+00  255.000000\n80             model.layers.8.input_layernorm  torch.float16               896           0.00%     1.069055        0.557704 -1.162109e-01    3.265625\n81    model.layers.8.post_attention_layernorm  torch.float16               896           0.00%     1.269379        0.485914  2.578125e-01    3.531250\n82            model.layers.9.self_attn.q_proj    torch.uint8            401408           0.03%   122.390465       56.256676  0.000000e+00  255.000000\n83            model.layers.9.self_attn.k_proj    torch.uint8             57344           0.03%   122.065613       54.682159  0.000000e+00  255.000000\n84            model.layers.9.self_attn.v_proj    torch.uint8             57344           0.02%   122.429710       56.929073  0.000000e+00  255.000000\n85            model.layers.9.self_attn.o_proj    torch.uint8            401408           0.03%   122.603188       59.099545  0.000000e+00  255.000000\n86               model.layers.9.mlp.gate_proj    torch.uint8           2179072           0.02%   122.301567       58.356701  0.000000e+00  255.000000\n87                 model.layers.9.mlp.up_proj    torch.uint8           2179072           0.03%   122.399559       57.938637  0.000000e+00  255.000000\n88               model.layers.9.mlp.down_proj    torch.uint8           2179072           0.02%   122.336487       56.905518  0.000000e+00  255.000000\n89             model.layers.9.input_layernorm  torch.float16               896           0.00%     1.568166        0.972377 -8.046875e-01   10.750000\n90    model.layers.9.post_attention_layernorm  torch.float16               896           0.00%     1.362994        0.577671  5.960464e-08    3.203125\n91           model.layers.10.self_attn.q_proj    torch.uint8            401408           0.03%   122.428535       57.853661  0.000000e+00  255.000000\n92           model.layers.10.self_attn.k_proj    torch.uint8             57344           0.03%   122.342796       57.592636  0.000000e+00  255.000000\n93           model.layers.10.self_attn.v_proj    torch.uint8             57344           0.02%   122.248924       55.023193  0.000000e+00  255.000000\n94           model.layers.10.self_attn.o_proj    torch.uint8            401408           0.03%   122.597015       59.999840  0.000000e+00  255.000000\n95              model.layers.10.mlp.gate_proj    torch.uint8           2179072           0.03%   122.242538       57.486557  0.000000e+00  255.000000\n96                model.layers.10.mlp.up_proj    torch.uint8           2179072           0.03%   122.382248       57.380318  0.000000e+00  255.000000\n97              model.layers.10.mlp.down_proj    torch.uint8           2179072           0.02%   122.256271       56.116333  0.000000e+00  255.000000\n98            model.layers.10.input_layernorm  torch.float16               896           0.00%     1.337198        0.957490 -1.117188e+00    4.531250\n99   model.layers.10.post_attention_layernorm  torch.float16               896           0.00%     1.315180        0.660076  2.412109e-01    3.515625\n100          model.layers.11.self_attn.q_proj    torch.uint8            401408           0.02%   122.357361       55.261597  0.000000e+00  255.000000\n101          model.layers.11.self_attn.k_proj    torch.uint8             57344           0.02%   122.336655       54.947315  0.000000e+00  255.000000\n102          model.layers.11.self_attn.v_proj    torch.uint8             57344           0.02%   121.792465       49.918896  0.000000e+00  255.000000\n103          model.layers.11.self_attn.o_proj    torch.uint8            401408           0.03%   122.729866       59.122852  0.000000e+00  255.000000\n104             model.layers.11.mlp.gate_proj    torch.uint8           2179072           0.02%   121.794060       54.552177  0.000000e+00  255.000000\n105               model.layers.11.mlp.up_proj    torch.uint8           2179072           0.02%   121.795113       55.621361  0.000000e+00  255.000000\n106             model.layers.11.mlp.down_proj    torch.uint8           2179072           0.02%   122.308258       56.518314  0.000000e+00  255.000000\n107           model.layers.11.input_layernorm  torch.float16               896           0.00%     1.796278        1.100070 -3.955078e-02    7.375000\n108  model.layers.11.post_attention_layernorm  torch.float16               896           0.00%     1.319809        0.492273 -4.386902e-05    2.656250\n109          model.layers.12.self_attn.q_proj    torch.uint8            401408           0.03%   122.387184       56.601173  0.000000e+00  255.000000\n110          model.layers.12.self_attn.k_proj    torch.uint8             57344           0.02%   122.084236       54.055668  0.000000e+00  255.000000\n111          model.layers.12.self_attn.v_proj    torch.uint8             57344           0.03%   122.553963       55.413692  0.000000e+00  255.000000\n112          model.layers.12.self_attn.o_proj    torch.uint8            401408           0.03%   122.456810       59.649113  0.000000e+00  255.000000\n113             model.layers.12.mlp.gate_proj    torch.uint8           2179072           0.03%   122.515480       58.496922  0.000000e+00  255.000000\n114               model.layers.12.mlp.up_proj    torch.uint8           2179072           0.03%   122.468887       58.263622  0.000000e+00  255.000000\n115             model.layers.12.mlp.down_proj    torch.uint8           2179072           0.03%   122.451996       57.696854  0.000000e+00  255.000000\n116           model.layers.12.input_layernorm  torch.float16               896           0.00%     1.513124        0.614301 -9.062500e-01    3.187500\n117  model.layers.12.post_attention_layernorm  torch.float16               896           0.00%     1.389511        0.493660 -1.554489e-04    2.531250\n118          model.layers.13.self_attn.q_proj    torch.uint8            401408           0.03%   122.594482       58.984116  0.000000e+00  255.000000\n119          model.layers.13.self_attn.k_proj    torch.uint8             57344           0.02%   122.257797       58.628372  0.000000e+00  255.000000\n120          model.layers.13.self_attn.v_proj    torch.uint8             57344           0.03%   123.069588       58.613197  0.000000e+00  255.000000\n121          model.layers.13.self_attn.o_proj    torch.uint8            401408           0.03%   122.540863       59.729221  0.000000e+00  255.000000\n122             model.layers.13.mlp.gate_proj    torch.uint8           2179072           0.03%   122.613060       58.714840  0.000000e+00  255.000000\n123               model.layers.13.mlp.up_proj    torch.uint8           2179072           0.03%   122.426796       58.654224  0.000000e+00  255.000000\n124             model.layers.13.mlp.down_proj    torch.uint8           2179072           0.03%   122.446121       57.806080  0.000000e+00  255.000000\n125           model.layers.13.input_layernorm  torch.float16               896           0.00%     1.486157        0.668099  9.130859e-02    3.359375\n126  model.layers.13.post_attention_layernorm  torch.float16               896           0.00%     1.372921        0.469418  2.460938e-01    2.390625\n127          model.layers.14.self_attn.q_proj    torch.uint8            401408           0.03%   122.417305       58.427380  0.000000e+00  255.000000\n128          model.layers.14.self_attn.k_proj    torch.uint8             57344           0.02%   122.586830       58.582542  0.000000e+00  255.000000\n129          model.layers.14.self_attn.v_proj    torch.uint8             57344           0.03%   122.514198       57.959877  0.000000e+00  255.000000\n130          model.layers.14.self_attn.o_proj    torch.uint8            401408           0.03%   122.514427       59.841614  0.000000e+00  255.000000\n131             model.layers.14.mlp.gate_proj    torch.uint8           2179072           0.03%   122.847336       58.512428  0.000000e+00  255.000000\n132               model.layers.14.mlp.up_proj    torch.uint8           2179072           0.03%   122.451302       58.408680  0.000000e+00  255.000000\n133             model.layers.14.mlp.down_proj    torch.uint8           2179072           0.03%   122.375107       57.857304  0.000000e+00  255.000000\n134           model.layers.14.input_layernorm  torch.float16               896           0.00%     1.626853        0.703336 -7.070312e-01    3.421875\n135  model.layers.14.post_attention_layernorm  torch.float16               896           0.00%     1.400580        0.434556 -2.746582e-02    2.343750\n136          model.layers.15.self_attn.q_proj    torch.uint8            401408           0.03%   122.333992       58.640057  0.000000e+00  255.000000\n137          model.layers.15.self_attn.k_proj    torch.uint8             57344           0.04%   122.613113       58.476673  0.000000e+00  255.000000\n138          model.layers.15.self_attn.v_proj    torch.uint8             57344           0.02%   122.612450       58.117527  0.000000e+00  255.000000\n139          model.layers.15.self_attn.o_proj    torch.uint8            401408           0.04%   122.644676       60.020535  0.000000e+00  255.000000\n140             model.layers.15.mlp.gate_proj    torch.uint8           2179072           0.03%   122.605316       57.647629  0.000000e+00  255.000000\n141               model.layers.15.mlp.up_proj    torch.uint8           2179072           0.03%   122.411713       58.096973  0.000000e+00  255.000000\n142             model.layers.15.mlp.down_proj    torch.uint8           2179072           0.03%   122.381088       56.934250  0.000000e+00  255.000000\n143           model.layers.15.input_layernorm  torch.float16               896           0.00%     1.425349        0.598486 -6.718750e-01    3.218750\n144  model.layers.15.post_attention_layernorm  torch.float16               896           0.00%     1.477057        0.390902  3.964844e-01    2.640625\n145          model.layers.16.self_attn.q_proj    torch.uint8            401408           0.02%   122.631737       57.242134  0.000000e+00  255.000000\n146          model.layers.16.self_attn.k_proj    torch.uint8             57344           0.02%   122.243149       57.233047  0.000000e+00  255.000000\n147          model.layers.16.self_attn.v_proj    torch.uint8             57344           0.02%   121.552582       47.267963  0.000000e+00  255.000000\n148          model.layers.16.self_attn.o_proj    torch.uint8            401408           0.04%   122.537239       60.315868  0.000000e+00  255.000000\n149             model.layers.16.mlp.gate_proj    torch.uint8           2179072           0.03%   122.145195       57.011639  0.000000e+00  255.000000\n150               model.layers.16.mlp.up_proj    torch.uint8           2179072           0.03%   122.381065       57.795898  0.000000e+00  255.000000\n151             model.layers.16.mlp.down_proj    torch.uint8           2179072           0.02%   122.409363       57.023834  0.000000e+00  255.000000\n152           model.layers.16.input_layernorm  torch.float16               896           0.00%     1.660565        0.565342 -9.101562e-01    3.562500\n153  model.layers.16.post_attention_layernorm  torch.float16               896           0.00%     1.578326        0.341396  4.335938e-01    3.515625\n154          model.layers.17.self_attn.q_proj    torch.uint8            401408           0.02%   122.359383       57.616341  0.000000e+00  255.000000\n155          model.layers.17.self_attn.k_proj    torch.uint8             57344           0.02%   122.574692       57.317005  0.000000e+00  255.000000\n156          model.layers.17.self_attn.v_proj    torch.uint8             57344           0.03%   122.082298       56.933636  0.000000e+00  255.000000\n157          model.layers.17.self_attn.o_proj    torch.uint8            401408           0.03%   122.542694       60.190842  0.000000e+00  255.000000\n158             model.layers.17.mlp.gate_proj    torch.uint8           2179072           0.03%   122.052681       58.566666  0.000000e+00  255.000000\n159               model.layers.17.mlp.up_proj    torch.uint8           2179072           0.03%   122.443932       58.896023  0.000000e+00  255.000000\n160             model.layers.17.mlp.down_proj    torch.uint8           2179072           0.03%   122.506355       58.598259  0.000000e+00  255.000000\n161           model.layers.17.input_layernorm  torch.float16               896           0.00%     1.571923        0.485495  8.544922e-02    3.390625\n162  model.layers.17.post_attention_layernorm  torch.float16               896           0.00%     1.730824        0.371812  3.398438e-01    3.234375\n163          model.layers.18.self_attn.q_proj    torch.uint8            401408           0.02%   122.483093       58.257999  0.000000e+00  255.000000\n164          model.layers.18.self_attn.k_proj    torch.uint8             57344           0.02%   122.424896       57.063023  0.000000e+00  255.000000\n165          model.layers.18.self_attn.v_proj    torch.uint8             57344           0.02%   122.242683       56.334293  0.000000e+00  255.000000\n166          model.layers.18.self_attn.o_proj    torch.uint8            401408           0.03%   122.591568       60.192047  0.000000e+00  255.000000\n167             model.layers.18.mlp.gate_proj    torch.uint8           2179072           0.03%   122.399132       59.713837  0.000000e+00  255.000000\n168               model.layers.18.mlp.up_proj    torch.uint8           2179072           0.03%   122.590309       59.696945  0.000000e+00  255.000000\n169             model.layers.18.mlp.down_proj    torch.uint8           2179072           0.03%   122.537140       59.688553  0.000000e+00  255.000000\n170           model.layers.18.input_layernorm  torch.float16               896           0.00%     1.589983        0.380777 -9.179688e-01    3.312500\n171  model.layers.18.post_attention_layernorm  torch.float16               896           0.00%     1.719568        0.396017  3.222656e-01    4.125000\n172          model.layers.19.self_attn.q_proj    torch.uint8            401408           0.03%   122.370789       58.756763  0.000000e+00  255.000000\n173          model.layers.19.self_attn.k_proj    torch.uint8             57344           0.04%   122.539192       57.950474  0.000000e+00  255.000000\n174          model.layers.19.self_attn.v_proj    torch.uint8             57344           0.03%   122.172958       57.781822  0.000000e+00  255.000000\n175          model.layers.19.self_attn.o_proj    torch.uint8            401408           0.03%   122.608917       59.815170  0.000000e+00  255.000000\n176             model.layers.19.mlp.gate_proj    torch.uint8           2179072           0.03%   122.362938       59.603672  0.000000e+00  255.000000\n177               model.layers.19.mlp.up_proj    torch.uint8           2179072           0.03%   122.608643       59.594120  0.000000e+00  255.000000\n178             model.layers.19.mlp.down_proj    torch.uint8           2179072           0.03%   122.512283       59.221603  0.000000e+00  255.000000\n179           model.layers.19.input_layernorm  torch.float16               896           0.00%     1.457768        0.332832 -3.789062e-01    3.609375\n180  model.layers.19.post_attention_layernorm  torch.float16               896           0.00%     1.908927        0.385458  5.273438e-01    4.625000\n181          model.layers.20.self_attn.q_proj    torch.uint8            401408           0.02%   122.334023       56.845272  0.000000e+00  255.000000\n182          model.layers.20.self_attn.k_proj    torch.uint8             57344           0.03%   122.532776       57.613400  0.000000e+00  255.000000\n183          model.layers.20.self_attn.v_proj    torch.uint8             57344           0.02%   122.215828       50.770954  0.000000e+00  255.000000\n184          model.layers.20.self_attn.o_proj    torch.uint8            401408           0.03%   122.557716       59.566563  0.000000e+00  255.000000\n185             model.layers.20.mlp.gate_proj    torch.uint8           2179072           0.03%   122.424606       58.830849  0.000000e+00  255.000000\n186               model.layers.20.mlp.up_proj    torch.uint8           2179072           0.03%   122.485687       58.847771  0.000000e+00  255.000000\n187             model.layers.20.mlp.down_proj    torch.uint8           2179072           0.03%   122.407227       58.461205  0.000000e+00  255.000000\n188           model.layers.20.input_layernorm  torch.float16               896           0.00%     1.516144        0.670899 -3.906250e-02    4.375000\n189  model.layers.20.post_attention_layernorm  torch.float16               896           0.00%     2.016449        0.307135  4.062500e-01    5.812500\n190          model.layers.21.self_attn.q_proj    torch.uint8            401408           0.03%   122.206932       55.284531  0.000000e+00  255.000000\n191          model.layers.21.self_attn.k_proj    torch.uint8             57344           0.02%   122.104080       54.976181  0.000000e+00  255.000000\n192          model.layers.21.self_attn.v_proj    torch.uint8             57344           0.01%   121.686287       47.764050  0.000000e+00  255.000000\n193          model.layers.21.self_attn.o_proj    torch.uint8            401408           0.03%   122.631546       60.510616  0.000000e+00  255.000000\n194             model.layers.21.mlp.gate_proj    torch.uint8           2179072           0.03%   122.603081       59.467609  0.000000e+00  255.000000\n195               model.layers.21.mlp.up_proj    torch.uint8           2179072           0.03%   122.608421       59.365440  0.000000e+00  255.000000\n196             model.layers.21.mlp.down_proj    torch.uint8           2179072           0.03%   122.543251       59.432903  0.000000e+00  255.000000\n197           model.layers.21.input_layernorm  torch.float16               896           0.00%     1.617219        1.366368  5.712891e-02    7.062500\n198  model.layers.21.post_attention_layernorm  torch.float16               896           0.00%     2.010075        0.280976 -2.803802e-04    5.187500\n199          model.layers.22.self_attn.q_proj    torch.uint8            401408           0.02%   122.145584       55.486202  0.000000e+00  255.000000\n200          model.layers.22.self_attn.k_proj    torch.uint8             57344           0.02%   121.674355       54.298115  0.000000e+00  255.000000\n201          model.layers.22.self_attn.v_proj    torch.uint8             57344           0.01%   121.643890       49.182926  0.000000e+00  255.000000\n202          model.layers.22.self_attn.o_proj    torch.uint8            401408           0.03%   122.668800       60.691677  0.000000e+00  255.000000\n203             model.layers.22.mlp.gate_proj    torch.uint8           2179072           0.03%   122.514572       59.364269  0.000000e+00  255.000000\n204               model.layers.22.mlp.up_proj    torch.uint8           2179072           0.03%   122.482727       59.349094  0.000000e+00  255.000000\n205             model.layers.22.mlp.down_proj    torch.uint8           2179072           0.03%   122.559990       59.139137  0.000000e+00  255.000000\n206           model.layers.22.input_layernorm  torch.float16               896           0.00%     1.654051        1.173776  1.582031e-01    6.468750\n207  model.layers.22.post_attention_layernorm  torch.float16               896           0.00%     2.134290        0.347131 -3.504753e-05    9.312500\n208          model.layers.23.self_attn.q_proj    torch.uint8            401408           0.02%   122.307945       56.587551  0.000000e+00  255.000000\n209          model.layers.23.self_attn.k_proj    torch.uint8             57344           0.03%   122.412567       55.729652  0.000000e+00  255.000000\n210          model.layers.23.self_attn.v_proj    torch.uint8             57344           0.02%   121.832542       50.135319  0.000000e+00  255.000000\n211          model.layers.23.self_attn.o_proj    torch.uint8            401408           0.03%   122.511978       58.919811  0.000000e+00  255.000000\n212             model.layers.23.mlp.gate_proj    torch.uint8           2179072           0.03%   123.077965       59.657902  0.000000e+00  255.000000\n213               model.layers.23.mlp.up_proj    torch.uint8           2179072           0.03%   122.473686       59.408039  0.000000e+00  255.000000\n214             model.layers.23.mlp.down_proj    torch.uint8           2179072           0.03%   122.383224       57.887184  0.000000e+00  255.000000\n215           model.layers.23.input_layernorm  torch.float16               896           0.00%     1.902257        1.037063  9.843750e-01    9.312500\n216  model.layers.23.post_attention_layernorm  torch.float16               896           0.00%     2.330924        0.357700 -1.707077e-04    7.718750\n217                                model.norm  torch.float16               896           0.00%     7.463624        0.841196 -2.281250e+00   17.375000\n218                                   lm_head  torch.float16         136134656          35.00%     0.000135        0.015063 -1.962891e-01    0.127930\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\npruned_model_save_dir = \"./qwen2_ranked_pruned_8bit\"\nlayer_stats_csv = \"statistical_analysis_of_layers.csv\"\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# =============================================================================\n# STEP 1: Load the original model and identify layer groups from CSV\n# =============================================================================\nprint(f\"--- Step 1: Loading original model for pruning ---\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"\\nReading layer rankings from '{layer_stats_csv}'...\")\ndf_stats = pd.read_csv(layer_stats_csv)\n\n# Get the names of the top 4 layers\ntop_4_layer_names = df_stats.head(4)['Layer Name'].tolist()\n\n# Get the actual module objects for these layers\nall_modules = dict(model.named_modules())\ntop_4_modules = [all_modules[name] for name in top_4_layer_names]\n\nprint(\"Identified layer groups for ranked pruning:\")\nprint(f\"  - Group 1 (25%): {top_4_layer_names[0]}\")\nprint(f\"  - Group 2 (35%): {top_4_layer_names[1]}\")\nprint(f\"  - Group 3 (45%): {top_4_layer_names[2]}\")\nprint(f\"  - Group 4 (50%): {top_4_layer_names[3]}\")\n\n# =============================================================================\n# STEP 2: Apply ranked-sparsity pruning\n# =============================================================================\nprint(\"\\n--- Step 2: Applying ranked-sparsity pruning ---\")\nsparsities = [0.25, 0.35, 0.45, 0.50]\n\nfor i, module in enumerate(top_4_modules):\n    sparsity = sparsities[i]\n    print(f\"Pruning '{top_4_layer_names[i]}' with {sparsity*100:.0f}% sparsity...\")\n    prune.l1_unstructured(module, name=\"weight\", amount=sparsity)\n\nprint(\"\\nMaking pruning permanent...\")\nfor module in top_4_modules:\n    prune.remove(module, 'weight')\n\nprint(\"Ranked-sparsity pruning complete.\")\n\n\n# =============================================================================\n# STEP 3: Save the pruned model\n# =============================================================================\nprint(f\"\\n--- Step 3: Saving the pruned model to {pruned_model_save_dir} ---\")\nmodel.save_pretrained(pruned_model_save_dir)\ntokenizer.save_pretrained(pruned_model_save_dir)\nprint(\"Pruned model saved successfully.\")\n\ndel model\ntorch.cuda.empty_cache()\n\n\n# =============================================================================\n# STEP 4: Load the pruned model with 8-bit quantization\n# =============================================================================\nprint(f\"\\n--- Step 4: Loading the pruned model with 8-bit quantization ---\")\nquantization_config_8bit = BitsAndBytesConfig(\n    load_in_8bit=True,\n)\n\nfinal_model = AutoModelForCausalLM.from_pretrained(\n    pruned_model_save_dir,\n    quantization_config=quantization_config_8bit,\n    device_map=\"auto\",\n)\nprint(\"Final pruned and 8-bit quantized model loaded successfully.\")\n\n\n# =============================================================================\n# STEP 5: Evaluate the performance of the final model\n# =============================================================================\ndef evaluate_perplexity(model, tokenizer, dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"test\"):\n    print(\"\\nEvaluating perplexity with sliding window...\")\n    test_dataset = load_dataset(dataset, subset, split=split)\n    encodings = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n    max_length = 2048\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n    nlls = []\n    prev_end_loc = 0\n    for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Evaluating Chunks\"):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n        if input_ids.size(1) < 2: continue\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len: break\n    perplexity = torch.exp(torch.stack(nlls).mean())\n    return perplexity.item()\n\nprint(\"\\n--- Step 5: Evaluating performance of the final model ---\")\nfinal_perplexity = evaluate_perplexity(final_model, tokenizer)\n\nprint(\"\\n--- Final Performance Result ---\")\nprint(f\"Perplexity of the ranked-pruned and 8-bit quantized model: {final_perplexity:.4f}\")\nprint(f\"Baseline perplexity (original float16 model): 12.5985\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T11:08:54.464204Z","iopub.execute_input":"2025-06-21T11:08:54.464551Z","iopub.status.idle":"2025-06-21T11:12:09.715383Z","shell.execute_reply.started":"2025-06-21T11:08:54.464528Z","shell.execute_reply":"2025-06-21T11:12:09.714531Z"}},"outputs":[{"name":"stdout","text":"--- Step 1: Loading original model for pruning ---\n\nReading layer rankings from 'statistical_analysis_of_layers.csv'...\nIdentified layer groups for ranked pruning:\n  - Group 1 (25%): model.embed_tokens\n  - Group 2 (35%): lm_head\n  - Group 3 (45%): model.layers.11.mlp.up_proj\n  - Group 4 (50%): model.layers.11.mlp.gate_proj\n\n--- Step 2: Applying ranked-sparsity pruning ---\nPruning 'model.embed_tokens' with 25% sparsity...\nPruning 'lm_head' with 35% sparsity...\nPruning 'model.layers.11.mlp.up_proj' with 45% sparsity...\nPruning 'model.layers.11.mlp.gate_proj' with 50% sparsity...\n\nMaking pruning permanent...\nRanked-sparsity pruning complete.\n\n--- Step 3: Saving the pruned model to ./qwen2_ranked_pruned_8bit ---\nPruned model saved successfully.\n\n--- Step 4: Loading the pruned model with 8-bit quantization ---\nFinal pruned and 8-bit quantized model loaded successfully.\n\n--- Step 5: Evaluating performance of the final model ---\n\nEvaluating perplexity with sliding window...\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 32768). Running this sequence through the model will result in indexing errors\nEvaluating Chunks:  99%|█████████▉| 581/585 [03:06<00:01,  3.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n--- Final Performance Result ---\nPerplexity of the ranked-pruned and 8-bit quantized model: 13.8522\nBaseline perplexity (original float16 model): 12.5985\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn.utils.prune as prune\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# --- Constants and Configuration ---\nseed = 252\ntorch.manual_seed(seed)\nmodel_path = \"Qwen/Qwen2-0.5B-Instruct\"\npruned_model_save_dir = \"./qwen2_ranked_pruned_8bit\"\nlayer_stats_csv = \"statistical_analysis_of_layers.csv\" # Assumes this file exists\noutput_stats_filename = \"ranked_pruned_8bit_layer_stats.csv\"\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# =============================================================================\n# STEP 1: Load the original model and identify layer groups from CSV\n# =============================================================================\nprint(f\"--- Step 1: Loading original model for pruning ---\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"\\nReading layer rankings from '{layer_stats_csv}'...\")\ndf_stats = pd.read_csv(layer_stats_csv)\ntop_4_layer_names = df_stats.head(4)['Layer Name'].tolist()\nall_modules = dict(model.named_modules())\ntop_4_modules = [all_modules[name] for name in top_4_layer_names]\n\nprint(\"Identified layer groups for ranked pruning:\")\nprint(f\"  - Group 1 (25%): {top_4_layer_names[0]}\")\nprint(f\"  - Group 2 (35%): {top_4_layer_names[1]}\")\nprint(f\"  - Group 3 (45%): {top_4_layer_names[2]}\")\nprint(f\"  - Group 4 (50%): {top_4_layer_names[3]}\")\n\n# =============================================================================\n# STEP 2: Apply ranked-sparsity pruning\n# =============================================================================\nprint(\"\\n--- Step 2: Applying ranked-sparsity pruning ---\")\nsparsities = [0.25, 0.35, 0.45, 0.50]\nfor i, module in enumerate(top_4_modules):\n    sparsity = sparsities[i]\n    print(f\"Pruning '{top_4_layer_names[i]}' with {sparsity*100:.0f}% sparsity...\")\n    prune.l1_unstructured(module, name=\"weight\", amount=sparsity)\n\nprint(\"\\nMaking pruning permanent...\")\nfor module in top_4_modules:\n    prune.remove(module, 'weight')\nprint(\"Ranked-sparsity pruning complete.\")\n\n# =============================================================================\n# STEP 3: Save the pruned model\n# =============================================================================\nprint(f\"\\n--- Step 3: Saving the pruned model to {pruned_model_save_dir} ---\")\nmodel.save_pretrained(pruned_model_save_dir)\ntokenizer.save_pretrained(pruned_model_save_dir)\nprint(\"Pruned model saved successfully.\")\ndel model\ntorch.cuda.empty_cache()\n\n# =============================================================================\n# STEP 4: Load the pruned model with 8-bit quantization\n# =============================================================================\nprint(f\"\\n--- Step 4: Loading the pruned model with 8-bit quantization ---\")\nquantization_config_8bit = BitsAndBytesConfig(load_in_8bit=True)\nfinal_model = AutoModelForCausalLM.from_pretrained(\n    pruned_model_save_dir,\n    quantization_config=quantization_config_8bit,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nprint(\"Final pruned and 8-bit quantized model loaded successfully.\")\n\n# =============================================================================\n# STEP 5: Perform and Save Layer-wise Statistical Analysis (Corrected Function)\n# =============================================================================\ndef generate_layer_wise_stats(model, model_description=\"model\"):\n    \"\"\"\n    Analyzes each layer in a model and returns a DataFrame with its stats.\n    \"\"\"\n    print(f\"\\n--- Generating Layer-wise Statistical Analysis for: {model_description} ---\")\n    layer_stats = []\n    for name, module in model.named_modules():\n        if hasattr(module, \"weight\") and isinstance(module.weight, torch.nn.Parameter):\n            with torch.no_grad():\n                weights = module.weight.data\n                non_zero_elements = torch.count_nonzero(weights).item()\n                total_elements = weights.numel()\n                \n                # --- THIS IS THE CORRECTED LINE ---\n                # Use standard Python division; no need for .float() on integers\n                actual_sparsity = 1.0 - (non_zero_elements / total_elements)\n\n                layer_stats.append({\n                    'Layer Name': name,\n                    'Data Type': weights.dtype,\n                    'Number of Params': total_elements,\n                    'Actual Sparsity': f\"{actual_sparsity:.2%}\",\n                    'Weight Mean': weights.to(torch.float32).mean().item(),\n                    'Weight Std Dev': weights.to(torch.float32).std().item(),\n                    'Weight Min': weights.min().item(),\n                    'Weight Max': weights.max().item(),\n                })\n    \n    return pd.DataFrame(layer_stats)\n\n# Generate the stats for the final model\nfinal_results_df = generate_layer_wise_stats(final_model, \"Ranked-Pruned + 8-bit Model\")\n\n# Save the results to a CSV file\nfinal_results_df.to_csv(output_stats_filename, index=False)\nprint(f\"\\nLayer-wise statistical analysis saved to '{output_stats_filename}'\")\n\n# Display the full results table\nprint(\"\\n--- Layer-wise Results ---\")\nprint(final_results_df.to_string())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T11:50:09.479613Z","iopub.execute_input":"2025-06-21T11:50:09.479959Z","iopub.status.idle":"2025-06-21T11:50:15.248370Z","shell.execute_reply.started":"2025-06-21T11:50:09.479937Z","shell.execute_reply":"2025-06-21T11:50:15.247714Z"}},"outputs":[{"name":"stdout","text":"--- Step 1: Loading original model for pruning ---\n\nReading layer rankings from 'statistical_analysis_of_layers.csv'...\nIdentified layer groups for ranked pruning:\n  - Group 1 (25%): model.embed_tokens\n  - Group 2 (35%): lm_head\n  - Group 3 (45%): model.layers.11.mlp.up_proj\n  - Group 4 (50%): model.layers.11.mlp.gate_proj\n\n--- Step 2: Applying ranked-sparsity pruning ---\nPruning 'model.embed_tokens' with 25% sparsity...\nPruning 'lm_head' with 35% sparsity...\nPruning 'model.layers.11.mlp.up_proj' with 45% sparsity...\nPruning 'model.layers.11.mlp.gate_proj' with 50% sparsity...\n\nMaking pruning permanent...\nRanked-sparsity pruning complete.\n\n--- Step 3: Saving the pruned model to ./qwen2_ranked_pruned_8bit ---\nPruned model saved successfully.\n\n--- Step 4: Loading the pruned model with 8-bit quantization ---\nFinal pruned and 8-bit quantized model loaded successfully.\n\n--- Generating Layer-wise Statistical Analysis for: Ranked-Pruned + 8-bit Model ---\n\nLayer-wise statistical analysis saved to 'ranked_pruned_8bit_layer_stats.csv'\n\n--- Layer-wise Results ---\n                                   Layer Name      Data Type  Number of Params Actual Sparsity  Weight Mean  Weight Std Dev    Weight Min  Weight Max\n0                          model.embed_tokens  torch.float16         136134656          35.00%     0.000135        0.015063 -1.962891e-01    0.127930\n1             model.layers.0.self_attn.q_proj     torch.int8            802816           1.46%    -0.028345       31.696020 -1.270000e+02  127.000000\n2             model.layers.0.self_attn.k_proj     torch.int8            114688           1.12%     0.043937       37.080254 -1.270000e+02  127.000000\n3             model.layers.0.self_attn.v_proj     torch.int8            114688           1.35%    -0.023141       33.155426 -1.270000e+02  127.000000\n4             model.layers.0.self_attn.o_proj     torch.int8            802816           1.65%    -0.015533       30.619982 -1.270000e+02  127.000000\n5                model.layers.0.mlp.gate_proj     torch.int8           4358144           1.20%     0.039224       35.988605 -1.270000e+02  127.000000\n6                  model.layers.0.mlp.up_proj     torch.int8           4358144           1.19%    -0.023843       36.183723 -1.270000e+02  127.000000\n7                model.layers.0.mlp.down_proj     torch.int8           4358144           1.60%     0.003208       29.946636 -1.270000e+02  127.000000\n8              model.layers.0.input_layernorm  torch.float16               896           0.00%     0.033067        0.125686 -6.250000e-01    0.960938\n9     model.layers.0.post_attention_layernorm  torch.float16               896           0.00%     0.722957        0.140165  4.257812e-01    1.820312\n10            model.layers.1.self_attn.q_proj     torch.int8            802816           1.47%    -0.072394       32.324303 -1.270000e+02  127.000000\n11            model.layers.1.self_attn.k_proj     torch.int8            114688           1.29%     0.003775       34.582489 -1.270000e+02  127.000000\n12            model.layers.1.self_attn.v_proj     torch.int8            114688           1.80%    -0.074289       28.332111 -1.270000e+02  127.000000\n13            model.layers.1.self_attn.o_proj     torch.int8            802816           1.67%    -0.037190       30.977995 -1.270000e+02  127.000000\n14               model.layers.1.mlp.gate_proj     torch.int8           4358144           1.19%     0.040727       36.305756 -1.270000e+02  127.000000\n15                 model.layers.1.mlp.up_proj     torch.int8           4358144           1.19%     0.013324       36.165897 -1.270000e+02  127.000000\n16               model.layers.1.mlp.down_proj     torch.int8           4358144           1.53%    -0.019145       30.467201 -1.270000e+02  127.000000\n17             model.layers.1.input_layernorm  torch.float16               896           0.00%     0.797259        0.444815 -5.859375e-02    3.843750\n18    model.layers.1.post_attention_layernorm  torch.float16               896           0.00%     1.125865        0.291850  1.585484e-05    3.203125\n19            model.layers.2.self_attn.q_proj     torch.int8            802816           1.29%     0.047350       33.769260 -1.270000e+02  127.000000\n20            model.layers.2.self_attn.k_proj     torch.int8            114688           1.33%     0.041704       33.458225 -1.270000e+02  127.000000\n21            model.layers.2.self_attn.v_proj     torch.int8            114688           1.49%    -0.002511       32.698853 -1.270000e+02  127.000000\n22            model.layers.2.self_attn.o_proj     torch.int8            802816           1.37%     0.046742       33.281624 -1.270000e+02  127.000000\n23               model.layers.2.mlp.gate_proj     torch.int8           4358144           1.18%    -0.015946       35.917931 -1.270000e+02  127.000000\n24                 model.layers.2.mlp.up_proj     torch.int8           4358144           1.19%     0.004369       36.066933 -1.270000e+02  127.000000\n25               model.layers.2.mlp.down_proj     torch.int8           4358144           1.56%     0.002674       30.263716 -1.270000e+02  127.000000\n26             model.layers.2.input_layernorm  torch.float16               896           0.00%     0.921201        0.415566 -1.515625e+00    3.828125\n27    model.layers.2.post_attention_layernorm  torch.float16               896           0.00%     1.238739        0.299352 -2.443790e-06    3.156250\n28            model.layers.3.self_attn.q_proj     torch.int8            802816           1.29%    -0.065042       33.833138 -1.270000e+02  127.000000\n29            model.layers.3.self_attn.k_proj     torch.int8            114688           1.24%    -0.090341       33.766918 -1.270000e+02  127.000000\n30            model.layers.3.self_attn.v_proj     torch.int8            114688           1.31%    -0.059413       34.724678 -1.270000e+02  127.000000\n31            model.layers.3.self_attn.o_proj     torch.int8            802816           1.40%    -0.000826       32.941647 -1.270000e+02  127.000000\n32               model.layers.3.mlp.gate_proj     torch.int8           4358144           1.20%     0.046223       36.091778 -1.270000e+02  127.000000\n33                 model.layers.3.mlp.up_proj     torch.int8           4358144           1.21%    -0.010581       36.086014 -1.270000e+02  127.000000\n34               model.layers.3.mlp.down_proj     torch.int8           4358144           1.62%    -0.002022       29.545382 -1.270000e+02  127.000000\n35             model.layers.3.input_layernorm  torch.float16               896           0.00%     1.091552        0.287205 -9.921875e-01    3.593750\n36    model.layers.3.post_attention_layernorm  torch.float16               896           0.00%     1.345123        0.313676  4.553795e-05    3.140625\n37            model.layers.4.self_attn.q_proj     torch.int8            802816           1.29%    -0.051054       34.074657 -1.270000e+02  127.000000\n38            model.layers.4.self_attn.k_proj     torch.int8            114688           1.34%     0.034642       33.222233 -1.270000e+02  127.000000\n39            model.layers.4.self_attn.v_proj     torch.int8            114688           1.33%     0.089521       34.326618 -1.270000e+02  127.000000\n40            model.layers.4.self_attn.o_proj     torch.int8            802816           1.33%     0.027751       33.041573 -1.270000e+02  127.000000\n41               model.layers.4.mlp.gate_proj     torch.int8           4358144           1.34%    -0.082911       35.385792 -1.270000e+02  127.000000\n42                 model.layers.4.mlp.up_proj     torch.int8           4358144           1.30%     0.011756       35.427063 -1.270000e+02  127.000000\n43               model.layers.4.mlp.down_proj     torch.int8           4358144           1.89%    -0.001429       28.359844 -1.270000e+02  127.000000\n44             model.layers.4.input_layernorm  torch.float16               896           0.00%     0.959832        0.339886  5.004883e-02    3.296875\n45    model.layers.4.post_attention_layernorm  torch.float16               896           0.00%     1.266706        0.343240 -1.764297e-04    7.093750\n46            model.layers.5.self_attn.q_proj     torch.int8            802816           1.45%     0.076907       33.415150 -1.270000e+02  127.000000\n47            model.layers.5.self_attn.k_proj     torch.int8            114688           1.45%    -0.143773       33.200447 -1.270000e+02  127.000000\n48            model.layers.5.self_attn.v_proj     torch.int8            114688           1.56%     0.023708       35.046749 -1.270000e+02  127.000000\n49            model.layers.5.self_attn.o_proj     torch.int8            802816           1.41%     0.010717       32.405251 -1.270000e+02  127.000000\n50               model.layers.5.mlp.gate_proj     torch.int8           4358144           1.20%     0.027773       35.981949 -1.270000e+02  127.000000\n51                 model.layers.5.mlp.up_proj     torch.int8           4358144           1.20%     0.031631       35.847771 -1.270000e+02  127.000000\n52               model.layers.5.mlp.down_proj     torch.int8           4358144           1.64%     0.030734       28.627981 -1.270000e+02  127.000000\n53             model.layers.5.input_layernorm  torch.float16               896           0.00%     1.123697        0.402148 -1.257812e+00    3.937500\n54    model.layers.5.post_attention_layernorm  torch.float16               896           0.00%     1.297313        0.362451 -5.297852e-02    2.953125\n55            model.layers.6.self_attn.q_proj     torch.int8            802816           1.21%    -0.129113       35.087555 -1.270000e+02  127.000000\n56            model.layers.6.self_attn.k_proj     torch.int8            114688           1.27%     0.056475       33.403091 -1.270000e+02  127.000000\n57            model.layers.6.self_attn.v_proj     torch.int8            114688           1.53%     0.117667       33.350693 -1.270000e+02  127.000000\n58            model.layers.6.self_attn.o_proj     torch.int8            802816           1.43%     0.010724       32.729847 -1.270000e+02  127.000000\n59               model.layers.6.mlp.gate_proj     torch.int8           4358144           1.20%     0.008075       35.958439 -1.270000e+02  127.000000\n60                 model.layers.6.mlp.up_proj     torch.int8           4358144           1.24%    -0.011143       35.734848 -1.270000e+02  127.000000\n61               model.layers.6.mlp.down_proj     torch.int8           4358144           1.77%    -0.009264       27.898127 -1.270000e+02  127.000000\n62             model.layers.6.input_layernorm  torch.float16               896           0.00%     1.148009        0.431344 -8.281250e-01    3.968750\n63    model.layers.6.post_attention_layernorm  torch.float16               896           0.00%     1.323709        0.409137  1.068115e-04    3.781250\n64            model.layers.7.self_attn.q_proj     torch.int8            802816           1.29%     0.025757       33.956814 -1.270000e+02  127.000000\n65            model.layers.7.self_attn.k_proj     torch.int8            114688           1.35%     0.094927       33.807186 -1.270000e+02  127.000000\n66            model.layers.7.self_attn.v_proj     torch.int8            114688           1.40%     0.002276       33.612511 -1.270000e+02  127.000000\n67            model.layers.7.self_attn.o_proj     torch.int8            802816           1.46%    -0.036062       31.577456 -1.270000e+02  127.000000\n68               model.layers.7.mlp.gate_proj     torch.int8           4358144           1.22%     0.064441       35.671238 -1.270000e+02  127.000000\n69                 model.layers.7.mlp.up_proj     torch.int8           4358144           1.22%     0.036197       35.720425 -1.270000e+02  127.000000\n70               model.layers.7.mlp.down_proj     torch.int8           4358144           1.84%     0.025007       27.192806 -1.270000e+02  127.000000\n71             model.layers.7.input_layernorm  torch.float16               896           0.00%     1.319497        0.504894 -1.132812e+00    3.890625\n72    model.layers.7.post_attention_layernorm  torch.float16               896           0.00%     1.421060        0.476248 -2.789497e-05    3.593750\n73            model.layers.8.self_attn.q_proj     torch.int8            802816           1.30%     0.004243       34.160797 -1.270000e+02  127.000000\n74            model.layers.8.self_attn.k_proj     torch.int8            114688           1.25%     0.214085       34.622337 -1.270000e+02  127.000000\n75            model.layers.8.self_attn.v_proj     torch.int8            114688           1.86%    -0.138890       28.512373 -1.270000e+02  127.000000\n76            model.layers.8.self_attn.o_proj     torch.int8            802816           1.56%    -0.031551       31.155781 -1.270000e+02  127.000000\n77               model.layers.8.mlp.gate_proj     torch.int8           4358144           1.34%    -0.067179       34.720978 -1.270000e+02  127.000000\n78                 model.layers.8.mlp.up_proj     torch.int8           4358144           1.35%    -0.000462       34.799004 -1.270000e+02  127.000000\n79               model.layers.8.mlp.down_proj     torch.int8           4358144           1.85%     0.002102       27.019598 -1.270000e+02  127.000000\n80             model.layers.8.input_layernorm  torch.float16               896           0.00%     1.069055        0.557704 -1.162109e-01    3.265625\n81    model.layers.8.post_attention_layernorm  torch.float16               896           0.00%     1.269379        0.485914  2.578125e-01    3.531250\n82            model.layers.9.self_attn.q_proj     torch.int8            802816           1.63%     0.038352       32.259338 -1.270000e+02  127.000000\n83            model.layers.9.self_attn.k_proj     torch.int8            114688           1.80%     0.011292       31.056698 -1.270000e+02  127.000000\n84            model.layers.9.self_attn.v_proj     torch.int8            114688           1.55%     0.047564       34.120419 -1.270000e+02  127.000000\n85            model.layers.9.self_attn.o_proj     torch.int8            802816           1.49%     0.028328       30.895153 -1.270000e+02  127.000000\n86               model.layers.9.mlp.gate_proj     torch.int8           4358144           1.31%     0.023698       34.555504 -1.270000e+02  127.000000\n87                 model.layers.9.mlp.up_proj     torch.int8           4358144           1.31%    -0.001349       34.491531 -1.270000e+02  127.000000\n88               model.layers.9.mlp.down_proj     torch.int8           4358144           1.97%     0.004062       26.035553 -1.270000e+02  127.000000\n89             model.layers.9.input_layernorm  torch.float16               896           0.00%     1.568166        0.972377 -8.046875e-01   10.750000\n90    model.layers.9.post_attention_layernorm  torch.float16               896           0.00%     1.362994        0.577671  5.960464e-08    3.203125\n91           model.layers.10.self_attn.q_proj     torch.int8            802816           1.35%    -0.030500       33.018311 -1.270000e+02  127.000000\n92           model.layers.10.self_attn.k_proj     torch.int8            114688           1.38%    -0.071123       33.199635 -1.270000e+02  127.000000\n93           model.layers.10.self_attn.v_proj     torch.int8            114688           1.54%     0.111067       31.644817 -1.270000e+02  127.000000\n94           model.layers.10.self_attn.o_proj     torch.int8            802816           1.61%     0.014262       30.328512 -1.270000e+02  127.000000\n95              model.layers.10.mlp.gate_proj     torch.int8           4358144           1.44%     0.051111       33.624126 -1.270000e+02  127.000000\n96                model.layers.10.mlp.up_proj     torch.int8           4358144           1.44%     0.005962       33.715721 -1.270000e+02  127.000000\n97              model.layers.10.mlp.down_proj     torch.int8           4358144           2.11%     0.004475       24.991734 -1.270000e+02  127.000000\n98            model.layers.10.input_layernorm  torch.float16               896           0.00%     1.337198        0.957490 -1.117188e+00    4.531250\n99   model.layers.10.post_attention_layernorm  torch.float16               896           0.00%     1.315180        0.660076  2.412109e-01    3.515625\n100          model.layers.11.self_attn.q_proj     torch.int8            802816           1.71%    -0.053872       31.063715 -1.270000e+02  127.000000\n101          model.layers.11.self_attn.k_proj     torch.int8            114688           1.87%     0.065901       30.885719 -1.270000e+02  127.000000\n102          model.layers.11.self_attn.v_proj     torch.int8            114688           2.46%    -0.060416       26.966867 -1.270000e+02  127.000000\n103          model.layers.11.self_attn.o_proj     torch.int8            802816           2.06%     0.012568       25.720003 -1.270000e+02  127.000000\n104             model.layers.11.mlp.gate_proj     torch.int8           4358144          50.00%     0.142029       32.709663 -1.270000e+02  127.000000\n105               model.layers.11.mlp.up_proj     torch.int8           4358144          45.00%    -0.002649       33.190983 -1.270000e+02  127.000000\n106             model.layers.11.mlp.down_proj     torch.int8           4358144           2.43%    -0.006158       25.935942 -1.270000e+02  127.000000\n107           model.layers.11.input_layernorm  torch.float16               896           0.00%     1.796278        1.100070 -3.955078e-02    7.375000\n108  model.layers.11.post_attention_layernorm  torch.float16               896           0.00%     1.319809        0.492273 -4.386902e-05    2.656250\n109          model.layers.12.self_attn.q_proj     torch.int8            802816           1.59%     0.003623       32.111561 -1.270000e+02  127.000000\n110          model.layers.12.self_attn.k_proj     torch.int8            114688           2.26%    -0.074262       29.786329 -1.270000e+02  127.000000\n111          model.layers.12.self_attn.v_proj     torch.int8            114688           1.68%     0.009225       32.539562 -1.270000e+02  127.000000\n112          model.layers.12.self_attn.o_proj     torch.int8            802816           1.57%    -0.030587       30.777227 -1.270000e+02  127.000000\n113             model.layers.12.mlp.gate_proj     torch.int8           4358144           1.28%     0.105272       34.669731 -1.270000e+02  127.000000\n114               model.layers.12.mlp.up_proj     torch.int8           4358144           1.28%     0.013942       34.692913 -1.270000e+02  127.000000\n115             model.layers.12.mlp.down_proj     torch.int8           4358144           1.83%     0.011350       26.911501 -1.270000e+02  127.000000\n116           model.layers.12.input_layernorm  torch.float16               896           0.00%     1.513124        0.614301 -9.062500e-01    3.187500\n117  model.layers.12.post_attention_layernorm  torch.float16               896           0.00%     1.389511        0.493660 -1.554489e-04    2.531250\n118          model.layers.13.self_attn.q_proj     torch.int8            802816           1.26%     0.030168       34.530415 -1.270000e+02  127.000000\n119          model.layers.13.self_attn.k_proj     torch.int8            114688           1.28%     0.040780       34.497120 -1.270000e+02  127.000000\n120          model.layers.13.self_attn.v_proj     torch.int8            114688           1.28%     0.242937       35.414688 -1.270000e+02  127.000000\n121          model.layers.13.self_attn.o_proj     torch.int8            802816           1.35%    -0.018616       33.120750 -1.270000e+02  127.000000\n122             model.layers.13.mlp.gate_proj     torch.int8           4358144           1.27%     0.124423       34.779495 -1.270000e+02  127.000000\n123               model.layers.13.mlp.up_proj     torch.int8           4358144           1.26%     0.005524       35.102947 -1.270000e+02  127.000000\n124             model.layers.13.mlp.down_proj     torch.int8           4358144           1.81%     0.001293       27.053612 -1.270000e+02  127.000000\n125           model.layers.13.input_layernorm  torch.float16               896           0.00%     1.486157        0.668099  9.130859e-02    3.359375\n126  model.layers.13.post_attention_layernorm  torch.float16               896           0.00%     1.372921        0.469418  2.460938e-01    2.390625\n127          model.layers.14.self_attn.q_proj     torch.int8            802816           1.31%    -0.028297       33.667137 -1.270000e+02  127.000000\n128          model.layers.14.self_attn.k_proj     torch.int8            114688           1.30%     0.108085       34.473591 -1.270000e+02  127.000000\n129          model.layers.14.self_attn.v_proj     torch.int8            114688           1.36%    -0.101353       35.459225 -1.270000e+02  127.000000\n130          model.layers.14.self_attn.o_proj     torch.int8            802816           1.38%     0.009950       32.955154 -1.270000e+02  127.000000\n131             model.layers.14.mlp.gate_proj     torch.int8           4358144           1.31%     0.213563       34.724194 -1.270000e+02  127.000000\n132               model.layers.14.mlp.up_proj     torch.int8           4358144           1.28%     0.008641       34.889950 -1.270000e+02  127.000000\n133             model.layers.14.mlp.down_proj     torch.int8           4358144           1.87%     0.005710       27.075420 -1.270000e+02  127.000000\n134           model.layers.14.input_layernorm  torch.float16               896           0.00%     1.626853        0.703336 -7.070312e-01    3.421875\n135  model.layers.14.post_attention_layernorm  torch.float16               896           0.00%     1.400580        0.434556 -2.746582e-02    2.343750\n136          model.layers.15.self_attn.q_proj     torch.int8            802816           1.27%    -0.041414       34.487492 -1.270000e+02  127.000000\n137          model.layers.15.self_attn.k_proj     torch.int8            114688           1.24%     0.165274       35.185085 -1.270000e+02  127.000000\n138          model.layers.15.self_attn.v_proj     torch.int8            114688           1.33%    -0.026603       35.102463 -1.270000e+02  127.000000\n139          model.layers.15.self_attn.o_proj     torch.int8            802816           1.37%    -0.004452       32.760239 -1.270000e+02  127.000000\n140             model.layers.15.mlp.gate_proj     torch.int8           4358144           1.43%     0.154241       33.787975 -1.270000e+02  127.000000\n141               model.layers.15.mlp.up_proj     torch.int8           4358144           1.32%     0.013370       34.465775 -1.270000e+02  127.000000\n142             model.layers.15.mlp.down_proj     torch.int8           4358144           1.97%     0.003728       26.587639 -1.270000e+02  127.000000\n143           model.layers.15.input_layernorm  torch.float16               896           0.00%     1.425349        0.598486 -6.718750e-01    3.218750\n144  model.layers.15.post_attention_layernorm  torch.float16               896           0.00%     1.477057        0.390902  3.964844e-01    2.640625\n145          model.layers.16.self_attn.q_proj     torch.int8            802816           1.44%     0.116468       33.497383 -1.270000e+02  127.000000\n146          model.layers.16.self_attn.k_proj     torch.int8            114688           1.55%    -0.006478       33.470615 -1.270000e+02  127.000000\n147          model.layers.16.self_attn.v_proj     torch.int8            114688           2.84%     0.015904       27.344837 -1.270000e+02  127.000000\n148          model.layers.16.self_attn.o_proj     torch.int8            802816           1.59%    -0.015079       30.408365 -1.270000e+02  127.000000\n149             model.layers.16.mlp.gate_proj     torch.int8           4358144           1.44%     0.015138       32.832165 -1.270000e+02  127.000000\n150               model.layers.16.mlp.up_proj     torch.int8           4358144           1.39%    -0.021593       33.942310 -1.270000e+02  127.000000\n151             model.layers.16.mlp.down_proj     torch.int8           4358144           1.99%     0.017722       27.139944 -1.270000e+02  127.000000\n152           model.layers.16.input_layernorm  torch.float16               896           0.00%     1.660565        0.565342 -9.101562e-01    3.562500\n153  model.layers.16.post_attention_layernorm  torch.float16               896           0.00%     1.578326        0.341396  4.335938e-01    3.515625\n154          model.layers.17.self_attn.q_proj     torch.int8            802816           1.39%    -0.021750       32.477989 -1.270000e+02  127.000000\n155          model.layers.17.self_attn.k_proj     torch.int8            114688           1.50%     0.046212       30.931232 -1.270000e+02  127.000000\n156          model.layers.17.self_attn.v_proj     torch.int8            114688           1.45%    -0.100473       33.844421 -1.270000e+02  127.000000\n157          model.layers.17.self_attn.o_proj     torch.int8            802816           1.39%    -0.002455       32.921291 -1.270000e+02  127.000000\n158             model.layers.17.mlp.gate_proj     torch.int8           4358144           1.28%    -0.163276       34.640003 -1.270000e+02  127.000000\n159               model.layers.17.mlp.up_proj     torch.int8           4358144           1.22%    -0.023160       35.339310 -1.270000e+02  127.000000\n160             model.layers.17.mlp.down_proj     torch.int8           4358144           1.70%     0.001589       28.766809 -1.270000e+02  127.000000\n161           model.layers.17.input_layernorm  torch.float16               896           0.00%     1.571923        0.485495  8.544922e-02    3.390625\n162  model.layers.17.post_attention_layernorm  torch.float16               896           0.00%     1.730824        0.371812  3.398438e-01    3.234375\n163          model.layers.18.self_attn.q_proj     torch.int8            802816           1.33%    -0.003743       33.171947 -1.270000e+02  127.000000\n164          model.layers.18.self_attn.k_proj     torch.int8            114688           1.38%    -0.094744       31.298363 -1.270000e+02  127.000000\n165          model.layers.18.self_attn.v_proj     torch.int8            114688           1.53%    -0.076224       33.298717 -1.270000e+02  127.000000\n166          model.layers.18.self_attn.o_proj     torch.int8            802816           1.36%    -0.019344       32.804619 -1.270000e+02  127.000000\n167             model.layers.18.mlp.gate_proj     torch.int8           4358144           1.17%    -0.052104       36.159760 -1.270000e+02  127.000000\n168               model.layers.18.mlp.up_proj     torch.int8           4358144           1.15%    -0.021970       36.369061 -1.270000e+02  127.000000\n169             model.layers.18.mlp.down_proj     torch.int8           4358144           1.55%     0.001103       29.539164 -1.270000e+02  127.000000\n170           model.layers.18.input_layernorm  torch.float16               896           0.00%     1.589983        0.380777 -9.179688e-01    3.312500\n171  model.layers.18.post_attention_layernorm  torch.float16               896           0.00%     1.719568        0.396017  3.222656e-01    4.125000\n172          model.layers.19.self_attn.q_proj     torch.int8            802816           1.28%    -0.023583       34.571548 -1.270000e+02  127.000000\n173          model.layers.19.self_attn.k_proj     torch.int8            114688           1.28%    -0.041312       33.380989 -1.270000e+02  127.000000\n174          model.layers.19.self_attn.v_proj     torch.int8            114688           1.37%    -0.096715       34.908176 -1.270000e+02  127.000000\n175          model.layers.19.self_attn.o_proj     torch.int8            802816           1.36%    -0.023158       32.948044 -1.270000e+02  127.000000\n176             model.layers.19.mlp.gate_proj     torch.int8           4358144           1.18%    -0.060299       36.158943 -1.270000e+02  127.000000\n177               model.layers.19.mlp.up_proj     torch.int8           4358144           1.19%     0.031694       36.276321 -1.270000e+02  127.000000\n178             model.layers.19.mlp.down_proj     torch.int8           4358144           1.61%    -0.001832       29.453075 -1.270000e+02  127.000000\n179           model.layers.19.input_layernorm  torch.float16               896           0.00%     1.457768        0.332832 -3.789062e-01    3.609375\n180  model.layers.19.post_attention_layernorm  torch.float16               896           0.00%     1.908927        0.385458  5.273438e-01    4.625000\n181          model.layers.20.self_attn.q_proj     torch.int8            802816           1.47%    -0.012289       33.207657 -1.270000e+02  127.000000\n182          model.layers.20.self_attn.k_proj     torch.int8            114688           1.40%     0.098179       33.559570 -1.270000e+02  127.000000\n183          model.layers.20.self_attn.v_proj     torch.int8            114688           2.18%     0.169399       25.633753 -1.270000e+02  127.000000\n184          model.layers.20.self_attn.o_proj     torch.int8            802816           1.65%     0.006376       31.089573 -1.270000e+02  127.000000\n185             model.layers.20.mlp.gate_proj     torch.int8           4358144           1.29%    -0.022825       35.392502 -1.270000e+02  127.000000\n186               model.layers.20.mlp.up_proj     torch.int8           4358144           1.28%    -0.006969       35.618717 -1.270000e+02  127.000000\n187             model.layers.20.mlp.down_proj     torch.int8           4358144           1.80%    -0.022586       28.687975 -1.270000e+02  127.000000\n188           model.layers.20.input_layernorm  torch.float16               896           0.00%     1.516144        0.670899 -3.906250e-02    4.375000\n189  model.layers.20.post_attention_layernorm  torch.float16               896           0.00%     2.016449        0.307135  4.062500e-01    5.812500\n190          model.layers.21.self_attn.q_proj     torch.int8            802816           1.91%    -0.021085       30.345940 -1.270000e+02  127.000000\n191          model.layers.21.self_attn.k_proj     torch.int8            114688           2.06%     0.054426       29.966473 -1.270000e+02  127.000000\n192          model.layers.21.self_attn.v_proj     torch.int8            114688           2.50%    -0.056492       24.468395 -1.270000e+02  127.000000\n193          model.layers.21.self_attn.o_proj     torch.int8            802816           1.19%    -0.016039       35.513878 -1.270000e+02  127.000000\n194             model.layers.21.mlp.gate_proj     torch.int8           4358144           1.18%     0.060513       35.963821 -1.270000e+02  127.000000\n195               model.layers.21.mlp.up_proj     torch.int8           4358144           1.20%     0.000806       36.153885 -1.270000e+02  127.000000\n196             model.layers.21.mlp.down_proj     torch.int8           4358144           1.57%    -0.010134       29.409071 -1.270000e+02  127.000000\n197           model.layers.21.input_layernorm  torch.float16               896           0.00%     1.617219        1.366368  5.712891e-02    7.062500\n198  model.layers.21.post_attention_layernorm  torch.float16               896           0.00%     2.010075        0.280976 -2.803802e-04    5.187500\n199          model.layers.22.self_attn.q_proj     torch.int8            802816           1.73%    -0.002694       30.770460 -1.270000e+02  127.000000\n200          model.layers.22.self_attn.k_proj     torch.int8            114688           1.84%    -0.134626       29.079275 -1.270000e+02  127.000000\n201          model.layers.22.self_attn.v_proj     torch.int8            114688           2.26%    -0.051270       25.361792 -1.270000e+02  127.000000\n202          model.layers.22.self_attn.o_proj     torch.int8            802816           1.39%     0.008919       33.252556 -1.270000e+02  127.000000\n203             model.layers.22.mlp.gate_proj     torch.int8           4358144           1.21%    -0.025303       35.583355 -1.270000e+02  127.000000\n204               model.layers.22.mlp.up_proj     torch.int8           4358144           1.23%    -0.030779       35.947906 -1.270000e+02  127.000000\n205             model.layers.22.mlp.down_proj     torch.int8           4358144           1.67%    -0.001224       28.999710 -1.270000e+02  127.000000\n206           model.layers.22.input_layernorm  torch.float16               896           0.00%     1.654051        1.173776  1.582031e-01    6.468750\n207  model.layers.22.post_attention_layernorm  torch.float16               896           0.00%     2.134290        0.347131 -3.504753e-05    9.312500\n208          model.layers.23.self_attn.q_proj     torch.int8            802816           1.43%     0.021945       32.454174 -1.270000e+02  127.000000\n209          model.layers.23.self_attn.k_proj     torch.int8            114688           1.65%     0.015616       30.673479 -1.270000e+02  127.000000\n210          model.layers.23.self_attn.v_proj     torch.int8            114688           2.08%     0.078413       28.576187 -1.270000e+02  127.000000\n211          model.layers.23.self_attn.o_proj     torch.int8            802816           1.43%    -0.014085       33.757397 -1.270000e+02  127.000000\n212             model.layers.23.mlp.gate_proj     torch.int8           4358144           1.18%     0.144522       36.000366 -1.270000e+02  127.000000\n213               model.layers.23.mlp.up_proj     torch.int8           4358144           1.20%    -0.026556       35.545517 -1.270000e+02  127.000000\n214             model.layers.23.mlp.down_proj     torch.int8           4358144           1.66%    -0.001260       28.845867 -1.270000e+02  127.000000\n215           model.layers.23.input_layernorm  torch.float16               896           0.00%     1.902257        1.037063  9.843750e-01    9.312500\n216  model.layers.23.post_attention_layernorm  torch.float16               896           0.00%     2.330924        0.357700 -1.707077e-04    7.718750\n217                                model.norm  torch.float16               896           0.00%     7.463624        0.841196 -2.281250e+00   17.375000\n218                                   lm_head  torch.float16         136134656          35.00%     0.000135        0.015063 -1.962891e-01    0.127930\n","output_type":"stream"}],"execution_count":13}]}